[
    {
        "session": "Rep_Fin_Groups",
        "title": "Representations of Finite Groups",
        "topics": [
            "Mathematics/Algebra"
        ],
        "authors": [
            "Jeremy Sylvestre"
        ],
        "date": "2015-08-12",
        "abstract": "We provide a formal framework for the theory of representations of finite groups, as modules over the group ring. Along the way, we develop the general theory of groups (relying on the group_add class for the basics), modules, and vector spaces, to the extent required for theory of group representations. We then provide formal proofs of several important introductory theorems in the subject, including Maschke's theorem, Schur's lemma, and Frobenius reciprocity. We also prove that every irreducible representation is isomorphic to a submodule of the group ring, leading to the fact that for a finite group there are only finitely many isomorphism classes of irreducible representations. In all of this, no restriction is made on the characteristic of the ring or field of scalars until the definition of a group representation, and then the only restriction made is that the characteristic must not divide the order of the group.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-08-12"
            }
        ],
        "theories": [
            "Rep_Fin_Groups"
        ]
    },
    {
        "session": "MonoidalCategory",
        "title": "Monoidal Categories",
        "authors": [
            "Eugene W. Stark"
        ],
        "topics": [
            "Mathematics/Category theory"
        ],
        "date": "2017-05-04",
        "abstract": "\n<p>\nBuilding on the formalization of basic category theory set out in the\nauthor's previous AFP article, the present article formalizes\nsome basic aspects of the theory of monoidal categories. Among the\nnotions defined here are monoidal category, monoidal functor, and\nequivalence of monoidal categories. The main theorems formalized are\nMacLane's coherence theorem and the constructions of the free\nmonoidal category and free strict monoidal category generated by a\ngiven category.  The coherence theorem is proved syntactically, using\na structurally recursive approach to reduction of terms that might\nhave some novel aspects. We also give proofs of some results given by\nEtingof et al, which may prove useful in a formal setting. In\nparticular, we show that the left and right unitors need not be taken\nas given data in the definition of monoidal category, nor does the\ndefinition of monoidal functor need to take as given a specific\nisomorphism expressing the preservation of the unit object. Our\ndefinitions of monoidal category and monoidal functor are stated so as\nto take advantage of the economy afforded by these facts.\n</p><p>\nRevisions made subsequent to the first version of this article added\nmaterial on cartesian monoidal categories; showing that the underlying\ncategory of a cartesian monoidal category is a cartesian category, and\nthat every cartesian category extends to a cartesian monoidal\ncategory.\n</p>",
        "extra": {
            "Change history": "[2017-05-18]\nIntegrated material from MonoidalCategory/Category3Adapter into Category3/ and deleted adapter.\n(revision 015543cdd069)<br>\n[2018-05-29]\nModifications required due to 'Category3' changes.  Introduced notation for \"in hom\".\n(revision 8318366d4575)<br>\n[2020-02-15]\nCosmetic improvements.\n(revision a51840d36867)<br>\n[2020-07-10]\nAdded new material on cartesian monoidal categories.\n(revision 06640f317a79)<br>"
        },
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2017-05-05"
            }
        ],
        "dependencies": [
            "Category3"
        ],
        "theories": [
            "MonoidalCategory",
            "CartesianMonoidalCategory",
            "FreeMonoidalCategory",
            "MonoidalFunctor"
        ]
    },
    {
        "session": "Shadow_SC_DOM",
        "title": "A Formal Model of the Safely Composable Document Object Model with Shadow Roots",
        "authors": [
            "Achim D. Brucker",
            "Michael Herzberg"
        ],
        "topics": [
            "Computer science/Data structures"
        ],
        "date": "2020-09-28",
        "abstract": "\nIn this AFP entry, we extend our formalization of the safely\ncomposable DOM with Shadow Roots. This is a proposal for Shadow Roots\nwith stricter safety guarantess than the standard compliant\nformalization (see \"Shadow DOM\"). Shadow Roots are a recent\nproposal of the web community to support a component-based development\napproach for client-side web applications.  Shadow roots are a\nsignificant extension to the DOM standard and, as web standards are\ncondemned to be backward compatible, such extensions often result in\ncomplex specification that may contain unwanted subtleties that can be\ndetected by a formalization.  Our Isabelle/HOL formalization is, in\nthe sense of object-orientation, an extension of our formalization of\nthe core DOM and enjoys the same basic properties, i.e., it is\nextensible, i.e., can be extended without the need of re-proving\nalready proven properties and executable, i.e., we can generate\nexecutable code from our specification. We exploit the executability\nto show that our formalization complies to the official standard of\nthe W3C, respectively, the WHATWG.",
        "licence": "BSD",
        "dependencies": [
            "Core_SC_DOM"
        ],
        "theories": [
            "ShadowRootClass",
            "ShadowRootMonad",
            "Shadow_DOM",
            "Shadow_DOM_BaseTest",
            "slots",
            "slots_fallback",
            "Shadow_DOM_Document_adoptNode",
            "Shadow_DOM_Document_getElementById",
            "Shadow_DOM_Node_insertBefore",
            "Shadow_DOM_Node_removeChild",
            "Shadow_DOM_Tests"
        ]
    },
    {
        "session": "Binding_Syntax_Theory",
        "title": "A General Theory of Syntax with Bindings",
        "authors": [
            "Lorenzo Gheri",
            "Andrei Popescu"
        ],
        "topics": [
            "Computer science/Programming languages/Lambda calculi",
            "Computer science/Functional programming",
            "Logic/General logic/Mechanization of proofs"
        ],
        "date": "2019-04-06",
        "abstract": "\nWe formalize a theory of syntax with bindings that has been developed\nand refined over the last decade to support several large\nformalization efforts. Terms are defined for an arbitrary number of\nconstructors of varying numbers of inputs, quotiented to\nalpha-equivalence and sorted according to a binding signature. The\ntheory includes many properties of the standard operators on terms:\nsubstitution, swapping and freshness. It also includes bindings-aware\ninduction and recursion principles and support for semantic\ninterpretation. This work has been presented in the ITP 2017 paper “A\nFormalized General Theory of Syntax with Bindings”.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2019-04-08"
            }
        ],
        "theories": [
            "Preliminaries",
            "QuasiTerms_Swap_Fresh",
            "QuasiTerms_PickFresh_Alpha",
            "QuasiTerms_Environments_Substitution",
            "Pick",
            "Equiv_Relation2",
            "Transition_QuasiTerms_Terms",
            "Terms",
            "Well_Sorted_Terms",
            "Iteration",
            "Semantic_Domains",
            "Recursion"
        ]
    },
    {
        "session": "Parity_Game",
        "title": "Positional Determinacy of Parity Games",
        "authors": [
            "Christoph Dittmann"
        ],
        "date": "2015-11-02",
        "topics": [
            "Mathematics/Games and economics",
            "Mathematics/Graph theory"
        ],
        "abstract": "\nWe present a formalization of parity games (a two-player game on\ndirected graphs) and a proof of their positional determinacy in\nIsabelle/HOL.  This proof works for both finite and infinite games.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-11-02"
            }
        ],
        "dependencies": [
            "Coinductive",
            "Graph_Theory"
        ],
        "theories": [
            "MoreCoinductiveList",
            "ParityGame",
            "Strategy",
            "AttractingStrategy",
            "Attractor",
            "WinningStrategy",
            "WellOrderedStrategy",
            "WinningRegion",
            "UniformStrategy",
            "AttractorStrategy",
            "PositionalDeterminacy",
            "AttractorInductive",
            "Graph_TheoryCompatibility"
        ]
    },
    {
        "session": "Weight_Balanced_Trees",
        "title": "Weight-Balanced Trees",
        "authors": [
            "Tobias Nipkow",
            "Stefan Dirix"
        ],
        "topics": [
            "Computer science/Data structures"
        ],
        "date": "2018-03-13",
        "abstract": "\nThis theory provides a verified implementation of weight-balanced\ntrees following the work of <a\nhref=\"https://doi.org/10.1017/S0956796811000104\">Hirai\nand Yamamoto</a> who proved that all parameters in a certain\nrange are valid, i.e. guarantee that insertion and deletion preserve\nweight-balance. Instead of a general theorem we provide parameterized\nproofs of preservation of the invariant that work for many (all?)\nvalid parameters.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2018-03-13"
            }
        ],
        "theories": [
            "Weight_Balanced_Trees_log",
            "Weight_Balanced_Trees"
        ]
    },
    {
        "session": "Regex_Equivalence",
        "title": "Unified Decision Procedures for Regular Expression Equivalence",
        "authors": [
            "Tobias Nipkow",
            "Dmitriy Traytel"
        ],
        "topics": [
            "Computer science/Automata and formal languages"
        ],
        "date": "2014-01-30",
        "abstract": "\nWe formalize a unified framework for verified decision procedures for regular\nexpression equivalence. Five recently published formalizations of such\ndecision procedures (three based on derivatives, two on marked regular\nexpressions) can be obtained as instances of the framework. We discover that\nthe two approaches based on marked regular expressions, which were previously\nthought to be the same, are different, and one seems to produce uniformly\nsmaller automata.  The common framework makes it possible to compare the\nperformance of the different decision procedures in a meaningful way.\n<a href=\"http://www21.in.tum.de/~nipkow/pubs/itp14.html\">\nThe formalization is described in a paper of the same name presented at\nInteractive Theorem Proving 2014</a>.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-11-30"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2014-01-30"
            }
        ],
        "dependencies": [
            "Regular-Sets",
            "Efficient-Mergesort",
            "Spec_Check"
        ],
        "theories": [
            "Automaton",
            "Derivatives_Finite",
            "Deriv_PDeriv",
            "Deriv_Autos",
            "Position_Autos",
            "After2",
            "Before2",
            "Regex_Equivalence",
            "Examples",
            "Benchmark"
        ]
    },
    {
        "session": "Subresultants",
        "title": "Subresultants",
        "authors": [
            "Sebastiaan J. C. Joosten",
            "René Thiemann",
            "Akihisa Yamada"
        ],
        "topics": [
            "Mathematics/Algebra"
        ],
        "date": "2017-04-06",
        "abstract": "\nWe formalize the theory of subresultants and the subresultant\npolynomial remainder sequence as described by Brown and Traub. As a\nresult, we obtain efficient certified algorithms for computing the\nresultant and the greatest common divisor of polynomials.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2017-04-07"
            }
        ],
        "dependencies": [
            "Jordan_Normal_Form",
            "Polynomial_Factorization"
        ],
        "theories": [
            "Resultant_Prelim",
            "Dichotomous_Lazard",
            "Binary_Exponentiation",
            "More_Homomorphisms",
            "Coeff_Int",
            "Subresultant",
            "Subresultant_Gcd"
        ]
    },
    {
        "session": "UpDown_Scheme",
        "title": "Verification of the UpDown Scheme",
        "authors": [
            "Johannes Hölzl"
        ],
        "date": "2015-01-28",
        "topics": [
            "Computer science/Algorithms/Mathematical"
        ],
        "abstract": "\nThe UpDown scheme is a recursive scheme used to compute the stiffness matrix\non a special form of sparse grids. Usually, when discretizing a Euclidean\nspace of dimension d we need O(n^d) points, for n points along each dimension.\nSparse grids are a hierarchical representation where the number of points is\nreduced to O(n * log(n)^d). One disadvantage of such sparse grids is that the\nalgorithm now operate recursively in the dimensions and levels of the sparse grid.\n<p>\nThe UpDown scheme allows us to compute the stiffness matrix on such a sparse\ngrid. The stiffness matrix represents the influence of each representation\nfunction on the L^2 scalar product. For a detailed description see\nDirk Pflüger's PhD thesis. This formalization was developed as an\ninterdisciplinary project (IDP) at the Technische Universität München.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2015-01-30"
            }
        ],
        "dependencies": [
            "Automatic_Refinement",
            "Separation_Logic_Imperative_HOL"
        ],
        "theories": [
            "Grid_Point",
            "Grid",
            "Triangular_Function",
            "UpDown_Scheme",
            "Up",
            "Down",
            "Up_Down",
            "Imperative"
        ]
    },
    {
        "session": "Delta_System_Lemma",
        "title": "Cofinality and the Delta System Lemma",
        "authors": [
            "Pedro Sánchez Terraf"
        ],
        "topics": [
            "Mathematics/Combinatorics",
            "Logic/Set theory"
        ],
        "date": "2020-12-27",
        "abstract": "\nWe formalize the basic results on cofinality of linearly ordered sets\nand ordinals and Šanin’s Lemma for uncountable families of finite\nsets. This last result is used to prove the countable chain condition\nfor Cohen posets. We work in the set theory framework of Isabelle/ZF,\nusing the Axiom of Choice as needed.",
        "licence": "BSD",
        "theories": [
            "ZF_Library",
            "Cofinality",
            "Cardinal_Library",
            "Konig",
            "Delta_System",
            "Cohen_Posets"
        ]
    },
    {
        "session": "Elliptic_Curves_Group_Law",
        "title": "The Group Law for Elliptic Curves",
        "authors": [
            "Stefan Berghofer"
        ],
        "topics": [
            "Computer science/Security/Cryptography"
        ],
        "date": "2017-02-28",
        "abstract": "\nWe prove the group law for elliptic curves in Weierstrass form over\nfields of characteristic greater than 2. In addition to affine\ncoordinates, we also formalize projective coordinates, which allow for\nmore efficient computations. By specializing the abstract\nformalization to prime fields, we can apply the curve operations to\nparameters used in standard security protocols.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2017-03-01"
            }
        ],
        "theories": [
            "Elliptic_Axclass",
            "Elliptic_Locale",
            "Elliptic_Test"
        ]
    },
    {
        "session": "Resolution_FOL",
        "title": "The Resolution Calculus for First-Order Logic",
        "authors": [
            "Anders Schlichtkrull"
        ],
        "date": "2016-06-30",
        "topics": [
            "Logic/General logic/Mechanization of proofs"
        ],
        "abstract": "\nThis theory is a formalization of the resolution calculus for\nfirst-order logic. It is proven sound and complete. The soundness\nproof uses the substitution lemma, which shows a correspondence\nbetween substitutions and updates to an environment. The completeness\nproof uses semantic trees, i.e. trees whose paths are partial Herbrand\ninterpretations. It employs Herbrand's theorem in a formulation which\nstates that an unsatisfiable set of clauses has a finite closed\nsemantic tree. It also uses the lifting lemma which lifts resolution\nderivation steps from the ground world up to the first-order world.\nThe theory is presented in a paper in the Journal of Automated Reasoning\n[Sch18] which extends a paper presented at the International Conference\non Interactive Theorem Proving [Sch16]. An earlier version was\npresented in an MSc thesis [Sch15]. The formalization mostly follows\ntextbooks by Ben-Ari [BA12], Chang and Lee [CL73], and Leitsch [Lei97].\nThe theory is part of the IsaFoL project [IsaFoL]. <p>\n<a name=\"Sch18\"></a>[Sch18] Anders Schlichtkrull. \"Formalization of the\nResolution Calculus for First-Order Logic\". Journal of Automated\nReasoning, 2018.<br> <a name=\"Sch16\"></a>[Sch16] Anders\nSchlichtkrull. \"Formalization of the Resolution Calculus for First-Order\nLogic\". In: ITP 2016. Vol. 9807. LNCS. Springer, 2016.<br>\n<a name=\"Sch15\"></a>[Sch15] Anders Schlichtkrull. <a href=\"https://people.compute.dtu.dk/andschl/Thesis.pdf\">\n\"Formalization of Resolution Calculus in Isabelle\"</a>.\n<a href=\"https://people.compute.dtu.dk/andschl/Thesis.pdf\">https://people.compute.dtu.dk/andschl/Thesis.pdf</a>.\nMSc thesis. Technical University of Denmark, 2015.<br>\n<a name=\"BA12\"></a>[BA12] Mordechai Ben-Ari. <i>Mathematical Logic for\nComputer Science</i>. 3rd. Springer, 2012.<br> <a\nname=\"CL73\"></a>[CL73] Chin-Liang Chang and Richard Char-Tung Lee.\n<i>Symbolic Logic and Mechanical Theorem Proving</i>. 1st. Academic\nPress, Inc., 1973.<br> <a name=\"Lei97\"></a>[Lei97] Alexander\nLeitsch. <i>The Resolution Calculus</i>. Texts in theoretical computer\nscience. Springer, 1997.<br> <a name=\"IsaFoL\"></a>[IsaFoL]\nIsaFoL authors. <a href=\"https://bitbucket.org/jasmin_blanchette/isafol\">\nIsaFoL: Isabelle Formalization of Logic</a>.\n<a href=\"https://bitbucket.org/jasmin_blanchette/isafol\">https://bitbucket.org/jasmin_blanchette/isafol</a>.",
        "extra": {
            "Change history": "[2018-01-24] added several new versions of the soundness and completeness theorems as described in the paper [Sch18]. <br>\n[2018-03-20] added a concrete instance of the unification and completeness theorems using the First-Order Terms AFP-entry from IsaFoR as described in the papers [Sch16] and [Sch18]."
        },
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-06-30"
            }
        ],
        "dependencies": [
            "First_Order_Terms"
        ],
        "theories": [
            "TermsAndLiterals",
            "Tree",
            "Resolution",
            "Completeness",
            "Examples",
            "Unification_Theorem",
            "Completeness_Instance"
        ]
    },
    {
        "session": "Circus",
        "title": "Isabelle/Circus",
        "authors": [
            "Abderrahmane Feliachi",
            "Burkhart Wolff",
            "Marie-Claude Gaudel"
        ],
        "contributors": [
            "Makarius Wenzel"
        ],
        "date": "2012-05-27",
        "topics": [
            "Computer science/Concurrency/Process calculi",
            "Computer science/System description languages"
        ],
        "abstract": "The Circus specification language combines elements for complex data and behavior specifications, using an integration of Z and CSP with a refinement calculus. Its semantics is based on Hoare and He's Unifying Theories of Programming (UTP). Isabelle/Circus is a formalization of the UTP and the Circus language in Isabelle/HOL. It contains proof rules and tactic support that allows for proofs of refinement for Circus processes (involving both data and behavioral aspects).\n<p>\nThe Isabelle/Circus environment supports a syntax for the semantic definitions which is close to textbook presentations of Circus. This article contains an extended version of corresponding VSTTE Paper together with the complete formal development of its underlying commented theories.",
        "extra": {
            "Change history": "[2014-06-05] More polishing, shorter proofs, added Circus syntax, added Makarius Wenzel as contributor."
        },
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-29"
            }
        ],
        "theories": [
            "Var",
            "Relations",
            "Designs",
            "Reactive_Processes",
            "CSP_Processes",
            "Circus_Actions",
            "Var_list",
            "Denotational_Semantics",
            "Circus_Syntax",
            "Refinement",
            "Refinement_Example"
        ]
    },
    {
        "session": "Treaps",
        "title": "Treaps",
        "authors": [
            "Maximilian P. L. Haslbeck",
            "Manuel Eberl",
            "Tobias Nipkow"
        ],
        "topics": [
            "Computer science/Data structures"
        ],
        "date": "2018-02-06",
        "abstract": "\n<p> A Treap is a binary tree whose nodes contain pairs\nconsisting of some payload and an associated priority. It must have\nthe search-tree property w.r.t. the payloads and the heap property\nw.r.t. the priorities. Treaps are an interesting data structure that\nis related to binary search trees (BSTs) in the following way: if one\nforgets all the priorities of a treap, the resulting BST is exactly\nthe same as if one had inserted the elements into an empty BST in\norder of ascending priority. This means that a treap behaves like a\nBST where we can pretend the elements were inserted in a different\norder from the one in which they were actually inserted. </p>\n<p> In particular, by choosing these priorities at random upon\ninsertion of an element, we can pretend that we inserted the elements\nin <em>random order</em>, so that the shape of the\nresulting tree is that of a random BST no matter in what order we\ninsert the elements. This is the main result of this\nformalisation.</p>",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2018-02-07"
            },
            {
                "2017": "2018-02-06"
            }
        ],
        "dependencies": [
            "Random_BSTs",
            "Comparison_Sort_Lower_Bound"
        ],
        "theories": [
            "Probability_Misc",
            "Treap",
            "Random_List_Permutation",
            "Treap_Sort_and_BSTs",
            "Random_Treap"
        ]
    },
    {
        "session": "FunWithTilings",
        "title": "Fun With Tilings",
        "authors": [
            "Tobias Nipkow",
            "Lawrence C. Paulson"
        ],
        "date": "2008-11-07",
        "topics": [
            "Mathematics/Misc"
        ],
        "abstract": "Tilings are defined inductively. It is shown that one form of mutilated chess board cannot be tiled with dominoes, while another one can be tiled with L-shaped tiles. Please add further fun examples of this kind!",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-24"
            },
            {
                "2011-1": "2011-10-11"
            },
            {
                "2011": "2011-02-11"
            },
            {
                "2009-2": "2010-07-01"
            },
            {
                "2009-1": "2009-12-12"
            },
            {
                "2009": "2009-04-29"
            }
        ],
        "theories": [
            "Tilings"
        ]
    },
    {
        "session": "Quick_Sort_Cost",
        "title": "The number of comparisons in QuickSort",
        "authors": [
            "Manuel Eberl"
        ],
        "topics": [
            "Computer science/Algorithms"
        ],
        "date": "2017-03-15",
        "abstract": "\n<p>We give a formal proof of the well-known results about the\nnumber of comparisons performed by two variants of QuickSort: first,\nthe expected number of comparisons of randomised QuickSort\n(i.&thinsp;e.&nbsp;QuickSort with random pivot choice) is\n<em>2&thinsp;(n+1)&thinsp;H<sub>n</sub> -\n4&thinsp;n</em>, which is asymptotically equivalent to\n<em>2&thinsp;n ln n</em>; second, the number of\ncomparisons performed by the classic non-randomised QuickSort has the\nsame distribution in the average case as the randomised one.</p>",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2017-03-16"
            }
        ],
        "dependencies": [
            "List-Index",
            "Regular-Sets",
            "Comparison_Sort_Lower_Bound",
            "Landau_Symbols"
        ],
        "theories": [
            "Randomised_Quick_Sort",
            "Quick_Sort_Average_Case"
        ]
    },
    {
        "session": "AVL-Trees",
        "title": "AVL Trees",
        "authors": [
            "Tobias Nipkow",
            "Cornelia Pusch"
        ],
        "date": "2004-03-19",
        "topics": [
            "Computer science/Data structures"
        ],
        "abstract": "Two formalizations of AVL trees with room for extensions. The first formalization is monolithic and shorter, the second one in two stages, longer and a bit simpler. The final implementation is the same. If you are interested in developing this further, please contact <tt>gerwin.klein@nicta.com.au</tt>.",
        "extra": {
            "Change history": "[2011-04-11] Ondrej Kuncar added delete function"
        },
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-24"
            },
            {
                "2011-1": "2011-10-11"
            },
            {
                "2011": "2011-02-11"
            },
            {
                "2009-2": "2010-06-30"
            },
            {
                "2009-1": "2009-12-12"
            },
            {
                "2009": "2009-04-29"
            },
            {
                "2008": "2008-06-10"
            },
            {
                "2007": "2007-11-27"
            },
            {
                "2005": "2005-10-14"
            },
            {
                "2004": "2004-05-21"
            },
            {
                "2004": "2004-04-20"
            },
            {
                "2003": "2004-03-19"
            }
        ],
        "theories": [
            "AVL",
            "AVL2"
        ]
    },
    {
        "session": "MonoBoolTranAlgebra",
        "title": "Algebra of Monotonic Boolean Transformers",
        "authors": [
            "Viorel Preoteasa"
        ],
        "topics": [
            "Computer science/Programming languages/Logics"
        ],
        "date": "2011-09-22",
        "abstract": "Algebras of imperative programming languages have been successful in reasoning about programs. In general an algebra of programs is an algebraic structure with programs as elements and with program compositions (sequential composition, choice, skip) as algebra operations. Various versions of these algebras were introduced to model partial correctness, total correctness, refinement, demonic choice, and other aspects. We formalize here an algebra which can be used to model total correctness, refinement, demonic and angelic choice. The basic model of this algebra are monotonic Boolean transformers (monotonic functions from a Boolean algebra to itself).",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-24"
            },
            {
                "2011-1": "2011-10-11"
            },
            {
                "2011": "2011-09-27"
            }
        ],
        "dependencies": [
            "LatticeProperties"
        ],
        "theories": [
            "Mono_Bool_Tran",
            "Mono_Bool_Tran_Algebra",
            "Assertion_Algebra",
            "Statements"
        ]
    },
    {
        "session": "CoreC++",
        "title": "CoreC++",
        "authors": [
            "Daniel Wasserrab"
        ],
        "date": "2006-05-15",
        "topics": [
            "Computer science/Programming languages/Language definitions"
        ],
        "abstract": "We present an operational semantics and type safety proof for multiple inheritance in C++. The semantics models the behavior of method calls, field accesses, and two forms of casts in C++ class hierarchies. For explanations see the OOPSLA 2006 paper by Wasserrab, Nipkow, Snelting and Tip.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-03-02"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-24"
            },
            {
                "2011-1": "2011-10-11"
            },
            {
                "2011": "2011-02-11"
            },
            {
                "2009-2": "2010-06-30"
            },
            {
                "2009-1": "2009-12-12"
            },
            {
                "2009": "2009-04-29"
            },
            {
                "2008": "2008-06-10"
            },
            {
                "2007": "2007-11-27"
            },
            {
                "2005": "2006-05-16"
            }
        ],
        "theories": [
            "Auxiliary",
            "Type",
            "Value",
            "Expr",
            "Decl",
            "ClassRel",
            "SubObj",
            "Objects",
            "Exceptions",
            "Syntax",
            "State",
            "BigStep",
            "SmallStep",
            "SystemClasses",
            "TypeRel",
            "WellType",
            "WellForm",
            "WWellForm",
            "Equivalence",
            "DefAss",
            "WellTypeRT",
            "Conform",
            "Progress",
            "HeapExtension",
            "CWellForm",
            "TypeSafe",
            "Determinism",
            "Annotate",
            "Execute",
            "CoreC++"
        ]
    },
    {
        "session": "InformationFlowSlicing_Inter",
        "title": "Inter-Procedural Information Flow Noninterference via Slicing",
        "authors": [
            "Daniel Wasserrab"
        ],
        "date": "2010-03-23",
        "topics": [
            "Computer science/Security"
        ],
        "abstract": "\n<p>\nIn this contribution, we show how correctness proofs for <a\nhref=\"Slicing.html\">intra-</a> and <a\nhref=\"HRB-Slicing.html\">interprocedural slicing</a> can be used to prove\nthat slicing is able to guarantee information flow noninterference.\nMoreover, we also illustrate how to lift the control flow graphs of the\nrespective frameworks such that they fulfil the additional assumptions\nneeded in the noninterference proofs. A detailed description of the\nintraprocedural proof and its interplay with the slicing framework can be\nfound in the PLAS'09 paper by Wasserrab et al.\n</p>\n<p>\nThis entry contains the part for inter-procedural slicing. See entry\n<a href=\"InformationFlowSlicing.html\">InformationFlowSlicing</a>\nfor the intra-procedural part.\n</p>",
        "extra": {
            "Change history": "[2016-06-10] The original entry <a\nhref=\"InformationFlowSlicing.html\">InformationFlowSlicing</a> contained both\nthe <a href=\"InformationFlowSlicing_Inter.html\">inter-</a> and <a\nhref=\"InformationFlowSlicing.html\">intra-procedural</a> case was split into\ntwo for easier maintenance."
        },
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            }
        ],
        "dependencies": [
            "HRB-Slicing"
        ],
        "theories": [
            "NonInterferenceInter",
            "LiftingInter"
        ]
    },
    {
        "session": "Algebraic_Numbers",
        "title": "Algebraic Numbers in Isabelle/HOL",
        "topics": [
            "Mathematics/Algebra"
        ],
        "authors": [
            "René Thiemann",
            "Akihisa Yamada",
            "Sebastiaan J. C. Joosten"
        ],
        "contributors": [
            "Manuel Eberl"
        ],
        "date": "2015-12-22",
        "abstract": "Based on existing libraries for matrices, factorization of rational polynomials, and Sturm's theorem, we formalized algebraic numbers in Isabelle/HOL. Our development serves as an implementation for real and complex numbers, and it admits to compute roots and completely factorize real and complex polynomials, provided that all coefficients are rational numbers. Moreover, we provide two implementations to display algebraic numbers, an injective and expensive one, or a faster but approximative version.\n</p><p>\nTo this end, we mechanized several results on resultants, which also required us to prove that polynomials over a unique factorization domain form again a unique factorization domain.\n</p>",
        "extra": {
            "Change history": "[2016-01-29] Split off Polynomial Interpolation and Polynomial Factorization<br>\n[2017-04-16] Use certified Berlekamp-Zassenhaus factorization, use subresultant algorithm for computing resultants, improved bisection algorithm"
        },
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-12-22"
            }
        ],
        "dependencies": [
            "Berlekamp_Zassenhaus",
            "Sturm_Sequences"
        ],
        "theories": [
            "Algebraic_Numbers_Prelim",
            "Bivariate_Polynomials",
            "Resultant",
            "Algebraic_Numbers",
            "Sturm_Rat",
            "Factors_of_Int_Poly",
            "Real_Algebraic_Numbers",
            "Real_Roots",
            "Complex_Roots_Real_Poly",
            "Compare_Complex",
            "Interval_Arithmetic",
            "Complex_Algebraic_Numbers",
            "Real_Factorization",
            "Show_Real_Alg",
            "Show_Real_Approx",
            "Show_Real_Precise",
            "Algebraic_Number_Tests",
            "Algebraic_Numbers_External_Code"
        ]
    },
    {
        "session": "Count_Complex_Roots",
        "title": "Count the Number of Complex Roots",
        "authors": [
            "Wenda Li"
        ],
        "topics": [
            "Mathematics/Analysis"
        ],
        "date": "2017-10-17",
        "abstract": "\nBased on evaluating Cauchy indices through remainder sequences, this\nentry provides an effective procedure to count the number of complex\nroots (with multiplicity) of a polynomial within a rectangle box or a\nhalf-plane. Potential applications of this entry include certified\ncomplex root isolation (of a polynomial) and testing the Routh-Hurwitz\nstability criterion (i.e., to check whether all the roots of some\ncharacteristic polynomial have negative real parts).",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-18"
            }
        ],
        "dependencies": [
            "Sturm_Tarski",
            "Winding_Number_Eval"
        ],
        "theories": [
            "Extended_Sturm",
            "More_Polynomials",
            "Count_Complex_Roots",
            "Count_Complex_Roots_Examples"
        ]
    },
    {
        "session": "CAVA_Automata",
        "title": "The CAVA Automata Library",
        "authors": [
            "Peter Lammich"
        ],
        "date": "2014-05-28",
        "topics": [
            "Computer science/Automata and formal languages"
        ],
        "abstract": "\nWe report on the graph and automata library that is used in the fully\nverified LTL model checker CAVA.\nAs most components of CAVA use some type of graphs or automata, a common\nautomata library simplifies assembly of the components and reduces\nredundancy.\n<p>\nThe CAVA Automata Library provides a hierarchy of graph and automata\nclasses, together with some standard algorithms.\nIts object oriented design allows for sharing of algorithms, theorems,\nand implementations between its classes, and also simplifies extensions\nof the library.\nMoreover, it is integrated into the Automatic Refinement Framework,\nsupporting automatic refinement of the abstract automata types to\nefficient data structures.\n<p>\nNote that the CAVA Automata Library is work in progress. Currently, it\nis very specifically tailored towards the requirements of the CAVA model\nchecker.\nNevertheless, the formalization techniques presented here allow an\nextension of the library to a wider scope. Moreover, they are not\nlimited to graph libraries, but apply to class hierarchies in general.\n<p>\nThe CAVA Automata Library is described in the paper: Peter Lammich, The\nCAVA Automata Library, Isabelle Workshop 2014.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2014-05-29"
            }
        ],
        "dependencies": [
            "CAVA_Base"
        ],
        "theories": [
            "Digraph_Basic",
            "Digraph",
            "Automata",
            "Lasso",
            "Simulation",
            "Step_Conv",
            "Stuttering_Extension",
            "Digraph_Impl",
            "Automata_Impl",
            "All_Of_CAVA_Automata"
        ]
    },
    {
        "session": "Game_Based_Crypto",
        "title": "Game-based cryptography in HOL",
        "authors": [
            "Andreas Lochbihler",
            "S. Reza Sefidgar",
            "Bhargav Bhatt"
        ],
        "topics": [
            "Computer science/Security/Cryptography"
        ],
        "date": "2017-05-05",
        "abstract": "\n<p>In this AFP entry, we show how to specify game-based cryptographic\nsecurity notions and formally prove secure several cryptographic\nconstructions from the literature using the CryptHOL framework. Among\nothers, we formalise the notions of a random oracle, a pseudo-random\nfunction, an unpredictable function, and of encryption schemes that are\nindistinguishable under chosen plaintext and/or ciphertext attacks. We\nprove the random-permutation/random-function switching lemma, security\nof the Elgamal and hashed Elgamal public-key encryption scheme and\ncorrectness and security of several constructions with pseudo-random\nfunctions.\n</p><p>Our proofs follow the game-hopping style advocated by\nShoup and Bellare and Rogaway, from which most of the examples have\nbeen taken. We generalise some of their results such that they can be\nreused in other proofs. Thanks to CryptHOL's integration with\nIsabelle's parametricity infrastructure, many simple hops are easily\njustified using the theory of representation independence.</p>",
        "extra": {
            "Change history": "[2018-09-28]\nadded the CryptHOL tutorial for game-based cryptography\n(revision 489a395764ae)"
        },
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2017-05-11"
            }
        ],
        "dependencies": [
            "CryptHOL"
        ],
        "theories": [
            "Diffie_Hellman",
            "IND_CCA2",
            "IND_CCA2_sym",
            "IND_CPA",
            "IND_CPA_PK",
            "IND_CPA_PK_Single",
            "SUF_CMA",
            "Pseudo_Random_Function",
            "Pseudo_Random_Permutation",
            "Guessing_Many_One",
            "Unpredictable_Function",
            "Security_Spec",
            "Elgamal",
            "Hashed_Elgamal",
            "RP_RF",
            "PRF_UHF",
            "PRF_IND_CPA",
            "PRF_UPF_IND_CCA",
            "Cryptographic_Constructions",
            "Game_Based_Crypto",
            "CryptHOL_Tutorial"
        ]
    },
    {
        "session": "Huffman",
        "title": "The Textbook Proof of Huffman's Algorithm",
        "authors": [
            "Jasmin Christian Blanchette"
        ],
        "date": "2008-10-15",
        "topics": [
            "Computer science/Data structures"
        ],
        "abstract": "Huffman's algorithm is a procedure for constructing a binary tree with minimum weighted path length. This report presents a formal proof of the correctness of Huffman's algorithm written using Isabelle/HOL. Our proof closely follows the sketches found in standard algorithms textbooks, uncovering a few snags in the process. Another distinguishing feature of our formalization is the use of custom induction rules to help Isabelle's automatic tactics, leading to very short proofs for most of the lemmas.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-03-02"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-24"
            },
            {
                "2011-1": "2011-10-11"
            },
            {
                "2011": "2011-02-11"
            },
            {
                "2009-2": "2010-07-01"
            },
            {
                "2009-1": "2009-12-12"
            },
            {
                "2009": "2009-04-29"
            },
            {
                "2008": "2008-10-21"
            },
            {
                "2008": "2008-10-15"
            }
        ],
        "theories": [
            "Huffman"
        ]
    },
    {
        "session": "SDS_Impossibility",
        "title": "The Incompatibility of SD-Efficiency and SD-Strategy-Proofness",
        "authors": [
            "Manuel Eberl"
        ],
        "date": "2016-05-04",
        "topics": [
            "Mathematics/Games and economics"
        ],
        "abstract": "\nThis formalisation contains the proof that there is no anonymous and\nneutral Social Decision Scheme for at least four voters and\nalternatives that fulfils both SD-Efficiency and SD-Strategy-\nProofness. The proof is a fully structured and quasi-human-redable\none. It was derived from the (unstructured) SMT proof of the case for\nexactly four voters and alternatives by Brandl et al.  Their proof\nrelies on an unverified translation of the original problem to SMT,\nand the proof that lifts the argument for exactly four voters and\nalternatives to the general case is also not machine-checked.  In this\nIsabelle proof, on the other hand, all of these steps are  fully\nproven and machine-checked. This is particularly important seeing as a\npreviously published informal proof of a weaker statement contained a\nmistake in precisely this lifting step.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-05-05"
            }
        ],
        "dependencies": [
            "Randomised_Social_Choice"
        ],
        "theories": [
            "SDS_Impossibility"
        ]
    },
    {
        "session": "Zeta_Function",
        "title": "The Hurwitz and Riemann ζ Functions",
        "authors": [
            "Manuel Eberl"
        ],
        "topics": [
            "Mathematics/Number theory",
            "Mathematics/Analysis"
        ],
        "date": "2017-10-12",
        "abstract": "\n<p>This entry builds upon the results about formal and analytic Dirichlet\nseries to define the Hurwitz &zeta; function &zeta;(<em>a</em>,<em>s</em>) and,\nbased on that, the Riemann &zeta; function &zeta;(<em>s</em>).\nThis is done by first defining them for &real;(<em>z</em>) > 1\nand then successively extending the domain to the left using the\nEuler&ndash;MacLaurin formula.</p>\n<p>Apart from the most basic facts such as analyticity, the following\nresults are provided:</p>\n<ul>\n<li>the Stieltjes constants and the Laurent expansion of\n&zeta;(<em>s</em>) at <em>s</em> = 1</li>\n<li>the non-vanishing of &zeta;(<em>s</em>)\nfor &real;(<em>z</em>) &ge; 1</li>\n<li>the relationship between &zeta;(<em>a</em>,<em>s</em>) and &Gamma;</li>\n<li>the special values at negative integers and positive even integers</li>\n<li>Hurwitz's formula and the reflection formula for &zeta;(<em>s</em>)</li>\n<li>the <a href=\"https://arxiv.org/abs/math/0405478\">\nHadjicostas&ndash;Chapman formula</a></li>\n</ul>\n<p>The entry also contains Euler's analytic proof of the infinitude of primes,\nbased on the fact that &zeta;(<i>s</i>) has a pole at <i>s</i> = 1.</p>",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-16"
            }
        ],
        "dependencies": [
            "Dirichlet_Series",
            "Euler_MacLaurin",
            "Bernoulli",
            "Winding_Number_Eval"
        ],
        "theories": [
            "Zeta_Library",
            "Zeta_Function",
            "Zeta_Laurent_Expansion",
            "Hadjicostas_Chapman"
        ]
    },
    {
        "session": "Robinson_Arithmetic",
        "title": "Robinson Arithmetic",
        "authors": [
            "Andrei Popescu",
            "Dmitriy Traytel"
        ],
        "topics": [
            "Logic/Proof theory"
        ],
        "date": "2020-09-16",
        "abstract": "\nWe instantiate our syntax-independent logic infrastructure developed\nin <a\nhref=\"https://www.isa-afp.org/entries/Syntax_Independent_Logic.html\">a\nseparate AFP entry</a> to the FOL theory of Robinson arithmetic\n(also known as Q). The latter was formalised using Nominal Isabelle by\nadapting <a\nhref=\"https://www.isa-afp.org/entries/Incompleteness.html\">Larry\nPaulson’s formalization of the Hereditarily Finite Set\ntheory</a>.",
        "licence": "BSD",
        "dependencies": [
            "Syntax_Independent_Logic",
            "Nominal2"
        ],
        "theories": [
            "Robinson_Arithmetic",
            "Instance"
        ]
    },
    {
        "session": "PSemigroupsConvolution",
        "title": "Partial Semigroups and Convolution Algebras",
        "authors": [
            "Brijesh Dongol",
            "Victor B. F. Gomes",
            "Ian J. Hayes",
            "Georg Struth"
        ],
        "topics": [
            "Mathematics/Algebra"
        ],
        "date": "2017-06-13",
        "abstract": "\nPartial Semigroups are relevant to the foundations of quantum\nmechanics and combinatorics as well as to interval and separation\nlogics. Convolution algebras can be understood either as algebras of\ngeneralised binary modalities over ternary Kripke frames, in\nparticular over partial semigroups, or as algebras of quantale-valued\nfunctions which are equipped with a convolution-style operation of\nmultiplication that is parametrised by a ternary relation. Convolution\nalgebras provide algebraic semantics for various substructural logics,\nincluding categorial, relevance and linear logics, for separation\nlogic and for interval logics; they cover quantitative and qualitative\napplications. These mathematical components for partial semigroups and\nconvolution algebras provide uniform foundations from which models of\ncomputation based on relations, program traces or pomsets, and\nverification components for separation or interval temporal logics can\nbe built with little effort.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2017-06-13"
            }
        ],
        "theories": [
            "Partial_Semigroups",
            "Partial_Semigroup_Models",
            "Quantales",
            "Binary_Modalities",
            "Unary_Modalities",
            "Partial_Semigroup_Lifting"
        ]
    },
    {
        "session": "Stone_Algebras",
        "title": "Stone Algebras",
        "authors": [
            "Walter Guttmann"
        ],
        "date": "2016-09-06",
        "topics": [
            "Mathematics/Order"
        ],
        "abstract": "\nA range of algebras between lattices and Boolean algebras generalise\nthe notion of a complement. We develop a hierarchy of these\npseudo-complemented algebras that includes Stone algebras.\nIndependently of this theory we study filters based on partial orders.\nBoth theories are combined to prove Chen and Grätzer's construction\ntheorem for Stone algebras. The latter involves extensive reasoning\nabout algebraic structures in addition to reasoning in algebraic\nstructures.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-28"
            },
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-09-06"
            }
        ],
        "theories": [
            "Lattice_Basics",
            "P_Algebras",
            "Filters",
            "Stone_Construction"
        ]
    },
    {
        "session": "Interpreter_Optimizations",
        "title": "Inline Caching and Unboxing Optimization for Interpreters",
        "authors": [
            "Martin Desharnais"
        ],
        "topics": [
            "Computer science/Programming languages/Misc"
        ],
        "date": "2020-12-07",
        "abstract": "\nThis Isabelle/HOL formalization builds on the\n<em>VeriComp</em> entry of the <em>Archive of Formal\nProofs</em> to provide the following contributions:  <ul>\n<li>an operational semantics for a realistic virtual machine\n(Std) for dynamically typed programming languages;</li>\n<li>the formalization of an inline caching optimization (Inca),\na proof of bisimulation with (Std), and a compilation\nfunction;</li> <li>the formalization of an unboxing\noptimization (Ubx), a proof of bisimulation with (Inca), and a simple\ncompilation function.</li> </ul>  This formalization was\ndescribed in the CPP 2021 paper <em>Towards Efficient and\nVerified Virtual Machines for Dynamic Languages</em>",
        "licence": "BSD",
        "dependencies": [
            "VeriComp"
        ],
        "theories": [
            "Env",
            "Env_list",
            "List_util",
            "Option_applicative",
            "Result",
            "Global",
            "Op",
            "OpInl",
            "Dynamic",
            "Inca",
            "Unboxed",
            "OpUbx",
            "Ubx",
            "Ubx_type_inference",
            "Unboxed_lemmas",
            "Inca_to_Ubx_simulation",
            "Inca_to_Ubx_compiler",
            "Op_example",
            "Std",
            "Std_to_Inca_simulation",
            "Std_to_Inca_compiler"
        ]
    },
    {
        "session": "MiniML",
        "title": "Mini ML",
        "authors": [
            "Wolfgang Naraschewski",
            "Tobias Nipkow"
        ],
        "date": "2004-03-19",
        "topics": [
            "Computer science/Programming languages/Type systems"
        ],
        "abstract": "This theory defines the type inference rules and the type inference algorithm <i>W</i> for MiniML (simply-typed lambda terms with <tt>let</tt>) due to Milner. It proves the soundness and completeness of <i>W</i> w.r.t. the rules.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2019-04-18"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-03-02"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-24"
            },
            {
                "2011-1": "2011-10-11"
            },
            {
                "2011": "2011-02-11"
            },
            {
                "2009-2": "2010-07-01"
            },
            {
                "2009-1": "2009-12-12"
            },
            {
                "2009": "2009-04-29"
            },
            {
                "2008": "2008-06-10"
            },
            {
                "2007": "2007-11-27"
            },
            {
                "2005": "2005-10-14"
            },
            {
                "2004": "2004-05-21"
            },
            {
                "2004": "2004-04-20"
            },
            {
                "2003": "2004-03-23"
            }
        ],
        "theories": [
            "Maybe",
            "Type",
            "Instance",
            "Generalize",
            "MiniML",
            "W"
        ]
    },
    {
        "session": "Catalan_Numbers",
        "title": "Catalan Numbers",
        "authors": [
            "Manuel Eberl"
        ],
        "date": "2016-06-21",
        "topics": [
            "Mathematics/Combinatorics"
        ],
        "abstract": "\n<p>In this work, we define the Catalan numbers <em>C<sub>n</sub></em>\nand prove several equivalent definitions (including some closed-form\nformulae). We also show one of their applications (counting the number\nof binary trees of size <em>n</em>), prove the asymptotic growth\napproximation <em>C<sub>n</sub> &sim; 4<sup>n</sup> / (&radic;<span\nstyle=\"text-decoration: overline\">&pi;</span> &middot;\nn<sup>1.5</sup>)</em>, and provide reasonably efficient executable\ncode to compute them.</p>  <p>The derivation of the closed-form\nformulae uses algebraic manipulations of the ordinary generating\nfunction of the Catalan numbers, and the asymptotic approximation is\nthen done using generalised binomial coefficients and the Gamma\nfunction. Thanks to these highly non-elementary mathematical tools,\nthe proofs are very short and simple.</p>",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            }
        ],
        "dependencies": [
            "Landau_Symbols"
        ],
        "theories": [
            "Catalan_Auxiliary_Integral",
            "Catalan_Numbers"
        ]
    },
    {
        "session": "InfPathElimination",
        "title": "Infeasible Paths Elimination by Symbolic Execution Techniques: Proof of Correctness and Preservation of Paths",
        "authors": [
            "Romain Aissat",
            "Frederic Voisin",
            "Burkhart Wolff"
        ],
        "date": "2016-08-18",
        "topics": [
            "Computer science/Programming languages/Static analysis"
        ],
        "abstract": "\nTRACER is a tool for verifying safety properties of sequential C\nprograms. TRACER attempts at building a finite symbolic execution\ngraph which over-approximates the set of all concrete reachable states\nand the set of feasible paths.  We present an abstract framework for\nTRACER and similar CEGAR-like systems. The framework provides 1) a\ngraph- transformation based method for reducing the feasible paths in\ncontrol-flow graphs, 2) a model for symbolic execution, subsumption,\npredicate abstraction and invariant generation. In this framework we\nformally prove two key properties: correct construction of the\nsymbolic states and preservation of feasible paths. The framework\nfocuses on core operations, leaving to concrete prototypes to “fit in”\nheuristics for combining them.  The accompanying paper (published in\nITP 2016) can be found at\nhttps://www.lri.fr/∼wolff/papers/conf/2016-itp-InfPathsNSE.pdf.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-08-18"
            }
        ],
        "theories": [
            "Graph",
            "Aexp",
            "Bexp",
            "Labels",
            "Store",
            "Conf",
            "SymExec",
            "LTS",
            "SubRel",
            "ArcExt",
            "SubExt",
            "RB"
        ]
    },
    {
        "session": "Prime_Harmonic_Series",
        "title": "The Divergence of the Prime Harmonic Series",
        "authors": [
            "Manuel Eberl"
        ],
        "date": "2015-12-28",
        "topics": [
            "Mathematics/Number theory"
        ],
        "abstract": "\n<p>\nIn this work, we prove the lower bound <span class=\"nobr\">ln(H_n) -\nln(5/3)</span> for the\npartial sum of the Prime Harmonic series and, based on this, the divergence of\nthe Prime Harmonic Series\n<span class=\"nobr\">∑[p&thinsp;prime]&thinsp;·&thinsp;1/p.</span>\n</p><p>\nThe proof relies on the unique squarefree decomposition of natural numbers. This\nis similar to Euler's original proof (which was highly informal and morally\nquestionable). Its advantage over proofs by contradiction, like the famous one\nby Paul Erdős, is that it provides a relatively good lower bound for the partial\nsums.\n</p>",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2016-01-05"
            }
        ],
        "theories": [
            "Prime_Harmonic_Misc",
            "Squarefree_Nat",
            "Prime_Harmonic"
        ]
    },
    {
        "session": "Regular-Sets",
        "title": "Regular Sets and Expressions",
        "authors": [
            "Alexander Krauss",
            "Tobias Nipkow"
        ],
        "contributors": [
            "Manuel Eberl"
        ],
        "topics": [
            "Computer science/Automata and formal languages"
        ],
        "date": "2010-05-12",
        "abstract": "This is a library of constructions on regular expressions and languages. It provides the operations of concatenation, Kleene star and derivative on languages. Regular expressions and their meaning are defined. An executable equivalence checker for regular expressions is verified; it does not need automata but works directly on regular expressions. <i>By mapping regular expressions to binary relations, an automatic and complete proof method for (in)equalities of binary relations over union, concatenation and (reflexive) transitive closure is obtained.</i> <P> Extended regular expressions with complement and intersection are also defined and an equivalence checker is provided.",
        "extra": {
            "Change history": "[2011-08-26] Christian Urban added a theory about derivatives and partial derivatives of regular expressions<br>\n[2012-05-10] Tobias Nipkow added extended regular expressions<br>\n[2012-05-10] Tobias Nipkow added equivalence checking with partial derivatives"
        },
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-03-02"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-24"
            },
            {
                "2011-1": "2011-10-11"
            },
            {
                "2011": "2011-02-11"
            },
            {
                "2009-2": "2010-07-01"
            }
        ],
        "theories": [
            "Regular_Set",
            "Regular_Exp",
            "NDerivative",
            "Equivalence_Checking",
            "Relation_Interpretation",
            "Regexp_Method",
            "Regexp_Constructions",
            "Derivatives",
            "pEquivalence_Checking",
            "Regular_Exp2",
            "Equivalence_Checking2"
        ]
    },
    {
        "session": "Landau_Symbols",
        "title": "Landau Symbols",
        "authors": [
            "Manuel Eberl"
        ],
        "date": "2015-07-14",
        "topics": [
            "Mathematics/Analysis"
        ],
        "abstract": "This entry provides Landau symbols to describe and reason about the asymptotic growth of functions for sufficiently large inputs. A number of simplification procedures are provided for additional convenience: cancelling of dominated terms in sums under a Landau symbol, cancelling of common factors in products, and a decision procedure for Landau expressions containing products of powers of functions like x, ln(x), ln(ln(x)) etc.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-07-15"
            }
        ],
        "theories": [
            "Group_Sort",
            "Landau_Real_Products",
            "Landau_Simprocs",
            "files/landau_simprocs.ML",
            "Landau_More"
        ]
    },
    {
        "session": "CakeML_Codegen",
        "title": "A Verified Code Generator from Isabelle/HOL to CakeML",
        "authors": [
            "Lars Hupel"
        ],
        "topics": [
            "Computer science/Programming languages/Compiling",
            "Logic/Rewriting"
        ],
        "date": "2019-07-08",
        "abstract": "\nThis entry contains the formalization that accompanies my PhD thesis\n(see https://lars.hupel.info/research/codegen/). I develop a verified\ncompilation toolchain from executable specifications in Isabelle/HOL\nto CakeML abstract syntax trees. This improves over the\nstate-of-the-art in Isabelle by providing a trustworthy procedure for\ncode generation.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-07-11"
            }
        ],
        "dependencies": [
            "CakeML",
            "Constructor_Funs",
            "Dict_Construction",
            "Higher_Order_Terms",
            "Huffman",
            "Pairing_Heap",
            "Root_Balanced_Tree",
            "Show"
        ],
        "theories": [
            "ML_Utils",
            "files/utils.ML",
            "Compiler_Utils",
            "Code_Utils",
            "files/pattern_compatibility.ML",
            "files/dynamic_unfold.ML",
            "CakeML_Utils",
            "Test_Utils",
            "Doc_Terms",
            "Terms_Extras",
            "files/hol_term.ML",
            "HOL_Datatype",
            "files/hol_datatype.ML",
            "Constructors",
            "Consts",
            "Strong_Term",
            "Sterm",
            "Pterm",
            "Term_as_Value",
            "Value",
            "Doc_CupCake",
            "CupCake_Env",
            "CupCake_Semantics",
            "Doc_Rewriting",
            "General_Rewriting",
            "Rewriting_Term",
            "Rewriting_Nterm",
            "Rewriting_Pterm_Elim",
            "Rewriting_Pterm",
            "Rewriting_Sterm",
            "Big_Step_Sterm",
            "Big_Step_Value",
            "Big_Step_Value_ML",
            "Doc_Preproc",
            "Eval_Class",
            "files/tactics.ML",
            "Embed",
            "files/embed.ML",
            "Eval_Instances",
            "files/eval_instances.ML",
            "Doc_Backend",
            "CakeML_Setup",
            "CakeML_Backend",
            "CakeML_Correctness",
            "CakeML_Byte",
            "Doc_Compiler",
            "Composition",
            "Compiler",
            "Test_Composition",
            "Test_Print",
            "Test_Embed_Data",
            "Test_Embed_Data2",
            "Test_Embed_Tree",
            "Test_Datatypes"
        ]
    },
    {
        "session": "Power_Sum_Polynomials",
        "title": "Power Sum Polynomials",
        "authors": [
            "Manuel Eberl"
        ],
        "topics": [
            "Mathematics/Algebra"
        ],
        "date": "2020-04-24",
        "abstract": "\n<p>This article provides a formalisation of the symmetric\nmultivariate polynomials known as <em>power sum\npolynomials</em>. These are of the form\np<sub>n</sub>(<em>X</em><sub>1</sub>,&hellip;,\n<em>X</em><sub><em>k</em></sub>) =\n<em>X</em><sub>1</sub><sup>n</sup>\n+ &hellip; +\nX<sub><em>k</em></sub><sup>n</sup>.\nA formal proof of the Girard–Newton Theorem is also given. This\ntheorem relates the power sum polynomials to the elementary symmetric\npolynomials s<sub><em>k</em></sub> in the form\nof a recurrence relation\n(-1)<sup><em>k</em></sup>\n<em>k</em> s<sub><em>k</em></sub>\n=\n&sum;<sub>i&isinv;[0,<em>k</em>)</sub>\n(-1)<sup>i</sup> s<sub>i</sub>\np<sub><em>k</em>-<em>i</em></sub>&thinsp;.</p>\n<p>As an application, this is then used to solve a generalised\nform of a puzzle given as an exercise in Dummit and Foote's\n<em>Abstract Algebra</em>: For <em>k</em>\ncomplex unknowns <em>x</em><sub>1</sub>,\n&hellip;,\n<em>x</em><sub><em>k</em></sub>,\ndefine p<sub><em>j</em></sub> :=\n<em>x</em><sub>1</sub><sup><em>j</em></sup>\n+ &hellip; +\n<em>x</em><sub><em>k</em></sub><sup><em>j</em></sup>.\nThen for each vector <em>a</em> &isinv;\n&#x2102;<sup><em>k</em></sup>, show that\nthere is exactly one solution to the system p<sub>1</sub>\n= a<sub>1</sub>, &hellip;,\np<sub><em>k</em></sub> =\na<sub><em>k</em></sub> up to permutation of\nthe\n<em>x</em><sub><em>i</em></sub>\nand determine the value of\np<sub><em>i</em></sub> for\ni&gt;k.</p>",
        "licence": "BSD",
        "dependencies": [
            "Symmetric_Polynomials",
            "Polynomial_Factorization"
        ],
        "theories": [
            "Power_Sum_Polynomials_Library",
            "Power_Sum_Polynomials",
            "Power_Sum_Puzzle"
        ]
    },
    {
        "session": "Possibilistic_Noninterference",
        "title": "Possibilistic Noninterference",
        "authors": [
            "Andrei Popescu",
            "Johannes Hölzl"
        ],
        "date": "2012-09-10",
        "topics": [
            "Computer science/Security",
            "Computer science/Programming languages/Type systems"
        ],
        "abstract": "We formalize a wide variety of Volpano/Smith-style  noninterference\nnotions for a while language with parallel composition.\nWe systematize and classify these notions according to\ncompositionality w.r.t. the language constructs. Compositionality\nyields sound syntactic criteria (a.k.a. type systems) in a uniform way.\n<p>\nAn <a href=\"http://www21.in.tum.de/~nipkow/pubs/cpp12.html\">article</a>\nabout these proofs is published in the proceedings\nof the conference Certified Programs and Proofs 2012.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-09-10"
            }
        ],
        "theories": [
            "MyTactics",
            "Interface",
            "Bisim",
            "Language_Semantics",
            "During_Execution",
            "Compositionality",
            "Syntactic_Criteria",
            "After_Execution",
            "Concrete"
        ]
    },
    {
        "session": "Relational_Disjoint_Set_Forests",
        "title": "Relational Disjoint-Set Forests",
        "authors": [
            "Walter Guttmann"
        ],
        "topics": [
            "Computer science/Data structures"
        ],
        "date": "2020-08-26",
        "abstract": "\nWe give a simple relation-algebraic semantics of read and write\noperations on associative arrays. The array operations seamlessly\nintegrate with assignments in the Hoare-logic library. Using relation\nalgebras and Kleene algebras we verify the correctness of an\narray-based implementation of disjoint-set forests with a naive union\noperation and a find operation with path compression.",
        "licence": "BSD",
        "dependencies": [
            "Stone_Kleene_Relation_Algebras"
        ],
        "theories": [
            "Disjoint_Set_Forests"
        ]
    },
    {
        "session": "Graph_Theory",
        "title": "Graph Theory",
        "authors": [
            "Lars Noschinski"
        ],
        "date": "2013-04-28",
        "topics": [
            "Mathematics/Graph theory"
        ],
        "abstract": "This development provides a formalization of directed graphs, supporting (labelled) multi-edges and infinite graphs. A polymorphic edge type allows edges to be treated as pairs of vertices, if multi-edges are not required. Formalized properties are i.a. walks (and related concepts), connectedness and subgraphs and basic properties of isomorphisms.\n<p>\nThis formalization is used to prove characterizations of Euler Trails, Shortest Paths and Kuratowski subgraphs.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-05-02"
            }
        ],
        "theories": [
            "Rtrancl_On",
            "Stuff",
            "Digraph",
            "Bidirected_Digraph",
            "Arc_Walk",
            "Pair_Digraph",
            "Digraph_Component",
            "Vertex_Walk",
            "Digraph_Component_Vwalk",
            "Digraph_Isomorphism",
            "Auxiliary",
            "Subdivision",
            "Euler",
            "Kuratowski",
            "Weighted_Graph",
            "Shortest_Path",
            "Graph_Theory"
        ]
    },
    {
        "session": "Abortable_Linearizable_Modules",
        "title": "Abortable Linearizable Modules",
        "authors": [
            "Rachid Guerraoui",
            "Viktor Kuncak",
            "Giuliano Losa"
        ],
        "date": "2012-03-01",
        "topics": [
            "Computer science/Algorithms/Distributed"
        ],
        "abstract": "\nWe define the Abortable Linearizable Module automaton (ALM for short)\nand prove its key composition property using the IOA theory of\nHOLCF. The ALM is at the heart of the Speculative Linearizability\nframework. This framework simplifies devising correct speculative\nalgorithms by enabling their decomposition into independent modules\nthat can be analyzed and proved correct in isolation. It is\nparticularly useful when working in a distributed environment, where\nthe need to tolerate faults and asynchrony has made current\nmonolithic protocols so intricate that it is no longer tractable to\ncheck their correctness. Our theory contains a typical example of a\nrefinement proof in the I/O-automata framework of Lynch and Tuttle.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-24"
            },
            {
                "2011-1": "2012-03-02"
            }
        ],
        "theories": [
            "Sequences",
            "IOA",
            "RDR",
            "SLin",
            "Simulations",
            "Idempotence",
            "Consensus"
        ]
    },
    {
        "session": "Quaternions",
        "title": "Quaternions",
        "authors": [
            "Lawrence C. Paulson"
        ],
        "topics": [
            "Mathematics/Algebra",
            "Mathematics/Geometry"
        ],
        "date": "2018-09-05",
        "abstract": "\nThis theory is inspired by the HOL Light development of quaternions,\nbut follows its own route. Quaternions are developed coinductively, as\nin the existing formalisation of the complex numbers. Quaternions are\nquickly shown to belong to the type classes of real normed division\nalgebras and real inner product spaces. And therefore they inherit a\ngreat body of facts involving algebraic  laws, limits, continuity,\netc., which must be proved explicitly in the HOL Light version.  The\ndevelopment concludes with the geometric interpretation of the product\nof imaginary quaternions.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-09-07"
            }
        ],
        "theories": [
            "Quaternions"
        ]
    },
    {
        "session": "LightweightJava",
        "title": "Lightweight Java",
        "authors": [
            "Rok Strniša",
            "Matthew Parkinson"
        ],
        "topics": [
            "Computer science/Programming languages/Language definitions"
        ],
        "date": "2011-02-07",
        "abstract": "A fully-formalized and extensible minimal imperative fragment of Java.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-24"
            },
            {
                "2011-1": "2011-10-11"
            },
            {
                "2011": "2011-03-02"
            }
        ],
        "theories": [
            "Lightweight_Java_Definition",
            "Lightweight_Java_Equivalence",
            "Lightweight_Java_Proof"
        ]
    },
    {
        "session": "Ordinal_Partitions",
        "title": "Ordinal Partitions",
        "authors": [
            "Lawrence C. Paulson"
        ],
        "topics": [
            "Mathematics/Combinatorics",
            "Logic/Set theory"
        ],
        "date": "2020-08-03",
        "abstract": "\nThe theory of partition relations concerns generalisations of\nRamsey's theorem. For any ordinal $\\alpha$, write $\\alpha \\to\n(\\alpha, m)^2$ if for each function $f$ from unordered pairs of\nelements of $\\alpha$ into $\\{0,1\\}$, either there is a subset\n$X\\subseteq \\alpha$ order-isomorphic to $\\alpha$ such that\n$f\\{x,y\\}=0$ for all $\\{x,y\\}\\subseteq X$, or there is an $m$ element\nset $Y\\subseteq \\alpha$ such that $f\\{x,y\\}=1$ for all\n$\\{x,y\\}\\subseteq Y$. (In both cases, with $\\{x,y\\}$ we require\n$x\\not=y$.) In particular, the infinite Ramsey theorem can be written\nin this notation as $\\omega \\to (\\omega, \\omega)^2$, or if we\nrestrict $m$ to the positive integers as above, then $\\omega \\to\n(\\omega, m)^2$ for all $m$.  This entry formalises Larson's proof\nof $\\omega^\\omega \\to (\\omega^\\omega, m)^2$ along with a similar proof\nof a result due to Specker: $\\omega^2 \\to (\\omega^2, m)^2$. Also\nproved is a necessary result by Erdős and Milner:\n$\\omega^{1+\\alpha\\cdot n} \\to (\\omega^{1+\\alpha}, 2^n)^2$.",
        "licence": "BSD",
        "theories": [
            "Library_Additions",
            "Partitions",
            "Erdos_Milner",
            "Omega_Omega"
        ]
    },
    {
        "session": "PCF",
        "title": "Logical Relations for PCF",
        "authors": [
            "Peter Gammie"
        ],
        "date": "2012-07-01",
        "topics": [
            "Computer science/Programming languages/Lambda calculi"
        ],
        "abstract": "We apply Andy Pitts's methods of defining relations over domains to\nseveral classical results in the literature. We show that the Y\ncombinator coincides with the domain-theoretic fixpoint operator,\nthat parallel-or and the Plotkin existential are not definable in\nPCF, that the continuation semantics for PCF coincides with the\ndirect semantics, and that our domain-theoretic semantics for PCF is\nadequate for reasoning about contextual equivalence in an\noperational semantics. Our version of PCF is untyped and has both\nstrict and non-strict function abstractions. The development is\ncarried out in HOLCF.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-07-03"
            }
        ],
        "theories": [
            "Basis",
            "Logical_Relations",
            "PCF",
            "OpSem",
            "Continuations",
            "SmallStep"
        ]
    },
    {
        "session": "Recursion-Theory-I",
        "title": "Recursion Theory I",
        "authors": [
            "Michael Nedzelsky"
        ],
        "date": "2008-04-05",
        "topics": [
            "Logic/Computability"
        ],
        "abstract": "This document presents the formalization of introductory material from  recursion theory --- definitions and basic properties of primitive recursive  functions, Cantor pairing function and computably enumerable sets  (including a proof of existence of a one-complete computably enumerable set  and a proof of the Rice's theorem).",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-24"
            },
            {
                "2011-1": "2011-10-11"
            },
            {
                "2011": "2011-02-11"
            },
            {
                "2009-2": "2010-07-01"
            },
            {
                "2009-1": "2009-12-12"
            },
            {
                "2009": "2009-04-29"
            },
            {
                "2008": "2008-06-10"
            },
            {
                "2007": "2008-04-11"
            }
        ],
        "theories": [
            "CPair",
            "PRecFun",
            "files/Utils.ML",
            "PRecList",
            "PRecFun2",
            "PRecFinSet",
            "PRecUnGr",
            "RecEnSet"
        ]
    },
    {
        "session": "Poincare_Bendixson",
        "title": "The Poincaré-Bendixson Theorem",
        "authors": [
            "Fabian Immler",
            "Yong Kiam Tan"
        ],
        "topics": [
            "Mathematics/Analysis"
        ],
        "date": "2019-12-18",
        "abstract": "\nThe Poincaré-Bendixson theorem is a classical result in the study of\n(continuous) dynamical systems. Colloquially, it restricts the\npossible behaviors of planar dynamical systems: such systems cannot be\nchaotic. In practice, it is a useful tool for proving the existence of\n(limiting) periodic behavior in planar systems. The theorem is an\ninteresting and challenging benchmark for formalized mathematics\nbecause proofs in the literature rely on geometric sketches and only\nhint at symmetric cases. It also requires a substantial background of\nmathematical theories, e.g., the Jordan curve theorem, real analysis,\nordinary differential equations, and limiting (long-term) behavior of\ndynamical systems.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-12-18"
            }
        ],
        "theories": [
            "Analysis_Misc",
            "ODE_Misc",
            "Invariance",
            "Limit_Set",
            "Periodic_Orbit",
            "Poincare_Bendixson",
            "Affine_Arithmetic_Misc",
            "Examples"
        ]
    },
    {
        "session": "Recursion-Addition",
        "title": "Recursion Theorem in ZF",
        "authors": [
            "Georgy Dunaev"
        ],
        "topics": [
            "Logic/Set theory"
        ],
        "date": "2020-05-11",
        "abstract": "\nThis document contains a proof of the recursion theorem. This is a\nmechanization of the proof of the recursion theorem from the text <i>Introduction to\nSet Theory</i>, by Karel Hrbacek and Thomas Jech. This\nimplementation may be used as the basis for a model of Peano arithmetic in\nZF. While recursion and the natural numbers are already available in Isabelle/ZF, this clean development\nis much easier to follow.",
        "licence": "BSD",
        "theories": [
            "recursion"
        ]
    },
    {
        "session": "FLP",
        "title": "A Constructive Proof for FLP",
        "authors": [
            "Benjamin Bisping",
            "Paul-David Brodmann",
            "Tim Jungnickel",
            "Christina Rickmann",
            "Henning Seidler",
            "Anke Stüber",
            "Arno Wilhelm-Weidner",
            "Kirstin Peters",
            "Uwe Nestmann"
        ],
        "date": "2016-05-18",
        "topics": [
            "Computer science/Concurrency"
        ],
        "abstract": "\nThe impossibility of distributed consensus with one faulty process is\na result with important consequences for real world distributed\nsystems e.g., commits in replicated databases. Since proofs are not\nimmune to faults and even plausible proofs with a profound formalism\ncan conclude wrong results, we validate the fundamental result named\nFLP after Fischer, Lynch and Paterson.\nWe present a formalization of distributed systems\nand the aforementioned consensus problem. Our proof is based on Hagen\nVölzer's paper \"A constructive proof for FLP\". In addition to the\nenhanced confidence in the validity of Völzer's proof, we contribute\nthe missing gaps to show the correctness in Isabelle/HOL. We clarify\nthe proof details and even prove fairness of the infinite execution\nthat contradicts consensus. Our Isabelle formalization can also be\nreused for further proofs of properties of distributed systems.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-05-18"
            }
        ],
        "theories": [
            "Multiset",
            "AsynchronousSystem",
            "ListUtilities",
            "Execution",
            "FLPSystem",
            "FLPTheorem",
            "FLPExistingSystem"
        ]
    },
    {
        "session": "Isabelle_Meta_Model",
        "title": "A Meta-Model for the Isabelle API",
        "authors": [
            "Frédéric Tuong",
            "Burkhart Wolff"
        ],
        "date": "2015-09-16",
        "topics": [
            "Computer science/Programming languages/Language definitions"
        ],
        "abstract": "\nWe represent a theory <i>of</i> (a fragment of) Isabelle/HOL <i>in</i>\nIsabelle/HOL. The purpose of this exercise is to write packages for\ndomain-specific specifications such as class models, B-machines, ...,\nand generally speaking, any domain-specific languages whose\nabstract syntax can be defined by a HOL \"datatype\". On this basis, the\nIsabelle code-generator can then be used to generate code for global\ncontext transformations as well as tactic code.\n<p>\nConsequently the package is geared towards\nparsing, printing and code-generation to the Isabelle API.\nIt is at the moment not sufficiently rich for doing meta theory on\nIsabelle itself. Extensions in this direction are possible though.\n<p>\nMoreover, the chosen fragment is fairly rudimentary. However it should be\neasily adapted to one's needs if a package is written on top of it.\nThe supported API contains types, terms, transformation of\nglobal context like definitions and data-type declarations as well\nas infrastructure for Isar-setups.\n<p>\nThis theory is drawn from the\n<a href=\"http://isa-afp.org/entries/Featherweight_OCL.html\">Featherweight OCL</a>\nproject where\nit is used to construct a package for object-oriented data-type theories\ngenerated from UML class diagrams. The Featherweight OCL, for example, allows for\nboth the direct execution of compiled tactic code by the Isabelle API\nas well as the generation of \".thy\"-files for debugging purposes.\n<p>\nGained experience from this project shows that the compiled code is sufficiently\nefficient for practical purposes while being based on a formal <i>model</i>\non which properties of the package can be proven such as termination of certain\ntransformations, correctness, etc.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-09-28"
            },
            {
                "2015": "2015-09-25"
            }
        ],
        "theories": [
            "Antiquote_Setup",
            "files/ISABELLE_HOME/src/Doc/antiquote_setup.ML",
            "Isabelle_Cartouche_Examples",
            "Isabelle_Main0",
            "Isabelle_code_target",
            "Isabelle_code_runtime",
            "Isabelle_Main1",
            "Isabelle_typedecl",
            "Isabelle_Main2",
            "Init",
            "Meta_Pure",
            "Parser_init",
            "Parser_Pure",
            "Meta_SML",
            "Meta_Isabelle",
            "Printer_init",
            "Printer_Pure",
            "Printer_SML",
            "Printer_Isabelle",
            "Toy_Library_Static",
            "Init_rbt",
            "Meta_Toy",
            "Meta_Toy_extended",
            "Meta_META",
            "Core_init",
            "Floor1_infra",
            "Floor1_access",
            "Floor1_examp",
            "Floor2_examp",
            "Floor1_ctxt",
            "Core",
            "Parser_Toy",
            "Parser_Toy_extended",
            "Parser_META",
            "Printer_Toy",
            "Printer_Toy_extended",
            "Printer_META",
            "Printer",
            "Generator_static",
            "Generator_dynamic_sequential",
            "Design_deep",
            "Toy_Library",
            "Design_shallow",
            "Rail",
            "Design_generated",
            "Design_generated_generated"
        ]
    },
    {
        "session": "Triangle",
        "title": "Basic Geometric Properties of Triangles",
        "authors": [
            "Manuel Eberl"
        ],
        "date": "2015-12-28",
        "topics": [
            "Mathematics/Geometry"
        ],
        "abstract": "\n<p>\nThis entry contains a definition of angles between vectors and between three\npoints. Building on this, we prove basic geometric properties of triangles, such\nas the Isosceles Triangle Theorem, the Law of Sines and the Law of Cosines, that\nthe sum of the angles of a triangle is π, and the congruence theorems for\ntriangles.\n</p><p>\nThe definitions and proofs were developed following those by John Harrison in\nHOL Light. However, due to Isabelle's type class system, all definitions and\ntheorems in the Isabelle formalisation hold for all real inner product spaces.\n</p>",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2016-01-05"
            }
        ],
        "theories": [
            "Angles",
            "Triangle"
        ]
    },
    {
        "session": "First_Order_Terms",
        "title": "First-Order Terms",
        "authors": [
            "Christian Sternagel",
            "René Thiemann"
        ],
        "topics": [
            "Logic/Rewriting",
            "Computer science/Algorithms"
        ],
        "license": "LGPL",
        "date": "2018-02-06",
        "abstract": "\nWe formalize basic results on first-order terms, including matching and a\nfirst-order unification algorithm, as well as well-foundedness of the\nsubsumption order. This entry is part of the <i>Isabelle\nFormalization of Rewriting</i> <a\nhref=\"http://cl-informatik.uibk.ac.at/isafor\">IsaFoR</a>,\nwhere first-order terms are omni-present: the unification algorithm is\nused to certify several confluence and termination techniques, like\ncritical-pair computation and dependency graph approximations; and the\nsubsumption order is a crucial ingredient for completion.",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2018-02-07"
            },
            {
                "2017": "2018-02-06"
            }
        ],
        "dependencies": [
            "Abstract-Rewriting"
        ],
        "theories": [
            "Transitive_Closure_More",
            "Seq_More",
            "Fun_More",
            "Option_Monad",
            "Term",
            "Term_Pair_Multiset",
            "Abstract_Matching",
            "Unifiers",
            "Abstract_Unification",
            "Unification",
            "Matching",
            "Subsumption"
        ]
    },
    {
        "session": "Noninterference_Concurrent_Composition",
        "title": "Conservation of CSP Noninterference Security under Concurrent Composition",
        "authors": [
            "Pasquale Noce"
        ],
        "date": "2016-06-13",
        "topics": [
            "Computer science/Security",
            "Computer science/Concurrency/Process calculi"
        ],
        "abstract": "\n<p>In his outstanding work on Communicating Sequential Processes,\nHoare has defined two fundamental binary operations allowing to\ncompose the input processes into another, typically more complex,\nprocess: sequential composition and concurrent composition.\nParticularly, the output of the latter operation is a process in which\nany event not shared by both operands can occur whenever the operand\nthat admits the event can engage in it, whereas any event shared by\nboth operands can occur just in case both can engage in it.</p>\n<p>This paper formalizes Hoare's definition of concurrent composition\nand proves, in the general case of a possibly intransitive policy,\nthat CSP noninterference security is conserved under this operation.\nThis result, along with the previous analogous one concerning\nsequential composition, enables the construction of more and more\ncomplex processes enforcing noninterference security by composing,\nsequentially or concurrently, simpler secure processes, whose security\ncan in turn be proven using either the definition of security, or\nunwinding theorems.</p>",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-06-13"
            }
        ],
        "dependencies": [
            "Noninterference_Sequential_Composition"
        ],
        "theories": [
            "ConcurrentComposition"
        ]
    },
    {
        "session": "Simpl",
        "title": "A Sequential Imperative Programming Language Syntax, Semantics, Hoare Logics and Verification Environment",
        "authors": [
            "Norbert Schirmer"
        ],
        "date": "2008-02-29",
        "topics": [
            "Computer science/Programming languages/Language definitions",
            "Computer science/Programming languages/Logics"
        ],
        "license": "LGPL",
        "abstract": "We present the theory of Simpl, a sequential imperative programming language. We introduce its syntax, its semantics (big and small-step operational semantics) and Hoare logics for both partial as well as total correctness. We prove soundness and completeness of the Hoare logic. We integrate and automate the Hoare logic in Isabelle/HOL to obtain a practically usable verification environment for imperative programs. Simpl is independent of a concrete programming language but expressive enough to cover all common language features: mutually recursive procedures, abrupt termination and exceptions, runtime faults, local and global variables, pointers and heap, expressions with side effects, pointers to procedures, partial application and closures, dynamic method invocation and also unbounded nondeterminism.",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-24"
            },
            {
                "2011-1": "2011-10-11"
            },
            {
                "2011": "2011-02-11"
            },
            {
                "2009-2": "2010-07-01"
            },
            {
                "2009-1": "2009-12-12"
            },
            {
                "2009": "2009-09-12"
            },
            {
                "2009": "2009-04-29"
            },
            {
                "2008": "2008-06-10"
            },
            {
                "2007": "2008-03-07"
            }
        ],
        "theories": [
            "Language",
            "Semantic",
            "HoarePartialDef",
            "HoarePartialProps",
            "HoarePartial",
            "Termination",
            "SmallStep",
            "HoareTotalDef",
            "HoareTotalProps",
            "HoareTotal",
            "Hoare",
            "StateSpace",
            "AlternativeSmallStep",
            "Simpl_Heap",
            "HeapList",
            "Generalise",
            "files/generalise_state.ML",
            "Vcg",
            "files/hoare.ML",
            "files/hoare_syntax.ML",
            "SyntaxTest",
            "VcgEx",
            "VcgExSP",
            "VcgExTotal",
            "Quicksort",
            "XVcg",
            "XVcgEx",
            "ProcParEx",
            "ProcParExSP",
            "Closure",
            "ClosureEx",
            "Compose",
            "ComposeEx",
            "UserGuide",
            "Simpl"
        ]
    },
    {
        "session": "Random_Graph_Subgraph_Threshold",
        "title": "Properties of Random Graphs -- Subgraph Containment",
        "authors": [
            "Lars Hupel"
        ],
        "date": "2014-02-13",
        "topics": [
            "Mathematics/Graph theory",
            "Mathematics/Probability theory"
        ],
        "abstract": "Random graphs are graphs with a fixed number of vertices, where each edge is present with a fixed probability. We are interested in the probability that a random graph contains a certain pattern, for example a cycle or a clique. A very high edge probability gives rise to perhaps too many edges (which degrades performance for many algorithms), whereas a low edge probability might result in a disconnected graph. We prove a theorem about a threshold probability such that a higher edge probability will asymptotically almost surely produce a random graph with the desired subgraph.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2014-02-14"
            }
        ],
        "dependencies": [
            "Girth_Chromatic"
        ],
        "theories": [
            "Ugraph_Misc",
            "Prob_Lemmas",
            "Ugraph_Lemmas",
            "Ugraph_Properties",
            "Subgraph_Threshold"
        ]
    },
    {
        "session": "Heard_Of",
        "title": "Verifying Fault-Tolerant Distributed Algorithms in the Heard-Of Model",
        "date": "2012-07-27",
        "authors": [
            "Henri Debrat",
            "Stephan Merz"
        ],
        "topics": [
            "Computer science/Algorithms/Distributed"
        ],
        "abstract": "\nDistributed computing is inherently based on replication, promising\nincreased tolerance to failures of individual computing nodes or\ncommunication channels. Realizing this promise, however, involves\nquite subtle algorithmic mechanisms, and requires precise statements\nabout the kinds and numbers of faults that an algorithm tolerates (such\nas process crashes, communication faults or corrupted values).  The\nlandmark theorem due to Fischer, Lynch, and Paterson shows that it is\nimpossible to achieve Consensus among N asynchronously communicating\nnodes in the presence of even a single permanent failure. Existing\nsolutions must rely on assumptions of \"partial synchrony\".\n<p>\nIndeed, there have been numerous misunderstandings on what exactly a given\nalgorithm is supposed to realize in what kinds of environments. Moreover, the\nabundance of subtly different computational models complicates comparisons\nbetween different algorithms. Charron-Bost and Schiper introduced the Heard-Of\nmodel for representing algorithms and failure assumptions in a uniform\nframework, simplifying comparisons between algorithms.\n<p>\nIn this contribution, we represent the Heard-Of model in Isabelle/HOL. We define\ntwo semantics of runs of algorithms with different unit of atomicity and relate\nthese through a reduction theorem that allows us to verify algorithms in the\ncoarse-grained semantics (where proofs are easier) and infer their correctness\nfor the fine-grained one (which corresponds to actual executions). We\ninstantiate the framework by verifying six Consensus algorithms that differ in\nthe underlying algorithmic mechanisms and the kinds of faults they tolerate.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-03-02"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-07-30"
            }
        ],
        "dependencies": [
            "Stuttering_Equivalence"
        ],
        "theories": [
            "HOModel",
            "Reduction",
            "Majorities",
            "OneThirdRuleDefs",
            "OneThirdRuleProof",
            "UvDefs",
            "UvProof",
            "LastVotingDefs",
            "LastVotingProof",
            "UteDefs",
            "UteProof",
            "AteDefs",
            "AteProof",
            "EigbyzDefs",
            "EigbyzProof"
        ]
    },
    {
        "session": "Adaptive_State_Counting",
        "title": "Formalisation of an Adaptive State Counting Algorithm",
        "authors": [
            "Robert Sachtleben"
        ],
        "topics": [
            "Computer science/Automata and formal languages",
            "Computer science/Algorithms"
        ],
        "date": "2019-08-16",
        "abstract": "\nThis entry provides a formalisation of a refinement of an adaptive\nstate counting algorithm, used to test for reduction between finite\nstate machines. The algorithm has been originally presented by Hierons\nin the paper <a\nhref=\"https://doi.org/10.1109/TC.2004.85\">Testing from a\nNon-Deterministic Finite State Machine Using Adaptive State\nCounting</a>.  Definitions for finite state machines and\nadaptive test cases are given and many useful theorems are derived\nfrom these. The algorithm is formalised using mutually recursive\nfunctions, for which it is proven that the generated test suite is\nsufficient to test for reduction against finite state machines of a\ncertain fault domain. Additionally, the algorithm is specified in a\nsimple WHILE-language and its correctness is shown using Hoare-logic.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-08-19"
            }
        ],
        "dependencies": [
            "Transition_Systems_and_Automata"
        ],
        "theories": [
            "FSM",
            "FSM_Product",
            "ATC",
            "ASC_LB",
            "ASC_Suite",
            "ASC_Sufficiency",
            "ASC_Hoare",
            "ASC_Example"
        ]
    },
    {
        "session": "Word_Lib",
        "title": "Finite Machine Word Library",
        "authors": [
            "Joel Beeren",
            "Matthew Fernandez",
            "Xin Gao",
            "Gerwin Klein",
            "Rafal Kolanski",
            "Japheth Lim",
            "Corey Lewis",
            "Daniel Matichuk",
            "Thomas Sewell"
        ],
        "date": "2016-06-09",
        "topics": [
            "Computer science/Data structures"
        ],
        "abstract": "\nThis entry contains an extension to the Isabelle library for\nfixed-width machine words. In particular, the entry adds quickcheck setup\nfor words, printing as hexadecimals, additional operations, reasoning\nabout alignment, signed words, enumerations of words, normalisation of\nword numerals, and an extensive library of properties about generic\nfixed-width words, as well as an instantiation of many of these to the\ncommonly used 32 and 64-bit bases.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-06-09"
            }
        ],
        "theories": [
            "More_Arithmetic",
            "More_Divides",
            "More_Word",
            "Signed_Words",
            "Traditional_Infix_Syntax",
            "Word_EqI",
            "Bit_Comprehension",
            "Bits_Int",
            "Typedef_Morphisms",
            "Aligned",
            "Least_significant_bit",
            "Most_significant_bit",
            "Even_More_List",
            "Reversed_Bit_Lists",
            "Ancient_Numeral",
            "Bitwise",
            "Bitwise_Signed",
            "Enumeration",
            "Enumeration_Word",
            "Generic_set_bit",
            "Hex_Words",
            "More_Sublist",
            "More_Misc",
            "Strict_part_mono",
            "Legacy_Aliases",
            "Next_and_Prev",
            "Norm_Words",
            "Rsplit",
            "Type_Syntax",
            "Signed_Division_Word",
            "Word_Lemmas",
            "Word_8",
            "Word_16",
            "Word_Syntax",
            "Word_Names",
            "More_Word_Operations",
            "Word_32",
            "Many_More",
            "Word_Lib_Sumo",
            "Word_64",
            "Guide",
            "Examples"
        ]
    },
    {
        "session": "Dirichlet_Series",
        "title": "Dirichlet Series",
        "authors": [
            "Manuel Eberl"
        ],
        "topics": [
            "Mathematics/Number theory"
        ],
        "date": "2017-10-12",
        "abstract": "\nThis entry is a formalisation of much of Chapters 2, 3, and 11 of\nApostol's &ldquo;Introduction to Analytic Number\nTheory&rdquo;. This includes: <ul> <li>Definitions and\nbasic properties for several number-theoretic functions (Euler's\n&phi;, M&ouml;bius &mu;, Liouville's &lambda;,\nthe divisor function &sigma;, von Mangoldt's\n&Lambda;)</li> <li>Executable code for most of these\nfunctions, the most efficient implementations using the factoring\nalgorithm by Thiemann <i>et al.</i></li>\n<li>Dirichlet products and formal Dirichlet series</li>\n<li>Analytic results connecting convergent formal Dirichlet\nseries to complex functions</li> <li>Euler product\nexpansions</li> <li>Asymptotic estimates of\nnumber-theoretic functions including the density of squarefree\nintegers and the average number of divisors of a natural\nnumber</li> </ul> These results are useful as a basis for\ndeveloping more number-theoretic results, such as the Prime Number\nTheorem.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-16"
            }
        ],
        "dependencies": [
            "Polynomial_Factorization",
            "Landau_Symbols",
            "Euler_MacLaurin"
        ],
        "theories": [
            "Dirichlet_Misc",
            "Multiplicative_Function",
            "Dirichlet_Product",
            "Dirichlet_Series",
            "Moebius_Mu",
            "More_Totient",
            "Liouville_Lambda",
            "Divisor_Count",
            "Arithmetic_Summatory",
            "Partial_Summation",
            "Euler_Products",
            "Dirichlet_Series_Analysis",
            "Arithmetic_Summatory_Asymptotics",
            "Dirichlet_Efficient_Code"
        ]
    },
    {
        "session": "Free-Boolean-Algebra",
        "topics": [
            "Logic/General logic/Classical propositional logic"
        ],
        "title": "Free Boolean Algebra",
        "authors": [
            "Brian Huffman"
        ],
        "date": "2010-03-29",
        "abstract": "This theory defines a type constructor representing the free Boolean algebra over a set of generators. Values of type (α)<i>formula</i> represent propositional formulas with uninterpreted variables from type α, ordered by implication. In addition to all the standard Boolean algebra operations, the library also provides a function for building homomorphisms to any other Boolean algebra type.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-03-02"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-24"
            },
            {
                "2011-1": "2011-10-11"
            },
            {
                "2011": "2011-02-11"
            },
            {
                "2009-2": "2010-06-30"
            },
            {
                "2009-1": "2010-03-29"
            }
        ],
        "theories": [
            "Free_Boolean_Algebra"
        ]
    },
    {
        "session": "Call_Arity",
        "title": "The Safety of Call Arity",
        "authors": [
            "Joachim Breitner"
        ],
        "date": "2015-02-20",
        "topics": [
            "Computer science/Programming languages/Transformations"
        ],
        "abstract": "\nWe formalize the Call Arity analysis, as implemented in GHC, and prove\nboth functional correctness and, more interestingly, safety (i.e. the\ntransformation does not increase allocation).\n<p>\nWe use syntax and the denotational semantics from the entry\n\"Launchbury\", where we formalized Launchbury's natural semantics for\nlazy evaluation.\n<p>\nThe functional correctness of Call Arity is proved with regard to that\ndenotational semantics. The operational properties are shown with\nregard to a small-step semantics akin to Sestoft's mark 1 machine,\nwhich we prove to be equivalent to Launchbury's semantics.\n<p>\nWe use Christian Urban's Nominal2 package to define our terms and make\nuse of Brian Huffman's HOLCF package for the domain-theoretical\naspects of the development.",
        "extra": {
            "Change history": "[2015-03-16] This entry now builds on top of the Launchbury entry,\nand the equivalency proof of the natural and the small-step semantics\nwas added."
        },
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2015-05-11"
            },
            {
                "2014": "2015-02-21"
            },
            {
                "2014": "2015-02-20"
            }
        ],
        "dependencies": [
            "Launchbury"
        ],
        "theories": [
            "BalancedTraces",
            "SestoftConf",
            "Sestoft",
            "SestoftCorrect",
            "Arity",
            "AEnv",
            "Arity-Nominal",
            "ArityAnalysisSig",
            "ArityAnalysisAbinds",
            "ArityAnalysisSpec",
            "TrivialArityAnal",
            "Cardinality-Domain",
            "CardinalityAnalysisSig",
            "ConstOn",
            "CardinalityAnalysisSpec",
            "ArityAnalysisStack",
            "NoCardinalityAnalysis",
            "TransformTools",
            "AbstractTransform",
            "EtaExpansion",
            "EtaExpansionSafe",
            "ArityStack",
            "ArityEtaExpansion",
            "ArityEtaExpansionSafe",
            "ArityTransform",
            "ArityConsistent",
            "ArityTransformSafe",
            "Set-Cpo",
            "Env-Set-Cpo",
            "CoCallGraph",
            "CoCallAnalysisSig",
            "AList-Utils-HOLCF",
            "CoCallGraph-Nominal",
            "CoCallAnalysisBinds",
            "ArityAnalysisFix",
            "CoCallFix",
            "CoCallAnalysisImpl",
            "CallArityEnd2End",
            "SestoftGC",
            "CardArityTransformSafe",
            "CoCallAritySig",
            "CoCallAnalysisSpec",
            "ArityAnalysisFixProps",
            "CoCallImplSafe",
            "List-Interleavings",
            "TTree",
            "TTree-HOLCF",
            "AnalBinds",
            "TTreeAnalysisSig",
            "CoCallGraph-TTree",
            "CoCallImplTTree",
            "Cardinality-Domain-Lists",
            "TTreeAnalysisSpec",
            "CoCallImplTTreeSafe",
            "TTreeImplCardinality",
            "TTreeImplCardinalitySafe",
            "CallArityEnd2EndSafe",
            "ArityAnalysisCorrDenotational"
        ]
    },
    {
        "session": "Security_Protocol_Refinement",
        "title": "Developing Security Protocols by Refinement",
        "authors": [
            "Christoph Sprenger",
            "Ivano Somaini"
        ],
        "topics": [
            "Computer science/Security"
        ],
        "license": "LGPL",
        "date": "2017-05-24",
        "abstract": "\nWe propose a development method for security protocols based on\nstepwise refinement. Our refinement strategy transforms abstract\nsecurity goals into protocols that are secure when operating over an\ninsecure channel controlled by a Dolev-Yao-style intruder. As\nintermediate levels of abstraction, we employ messageless guard\nprotocols and channel protocols communicating over channels with\nsecurity properties. These abstractions provide insights on why\nprotocols are secure and foster the development of families of\nprotocols sharing common structure and properties. We have implemented\nour method in Isabelle/HOL and used it to develop different entity\nauthentication and key establishment protocols, including realistic\nfeatures such as key confirmation, replay caches, and encrypted\ntickets. Our development highlights that guard protocols and channel\nprotocols provide fundamental abstractions for bridging the gap\nbetween security properties and standard protocol descriptions based\non cryptographic messages. It also shows that our refinement approach\nscales to protocols of nontrivial size and complexity.",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2017-05-25"
            }
        ],
        "theories": [
            "Infra",
            "Refinement",
            "Agents",
            "Keys",
            "Atoms",
            "Runs",
            "Channels",
            "Message",
            "s0g_secrecy",
            "a0n_agree",
            "a0i_agree",
            "m1_auth",
            "m2_auth_chan",
            "m2_confid_chan",
            "m3_sig",
            "m3_enc",
            "m1_keydist",
            "m1_keydist_iirn",
            "m1_keydist_inrn",
            "m1_kerberos",
            "m2_kerberos",
            "m3_kerberos_par",
            "m3_kerberos5",
            "m3_kerberos4",
            "m1_nssk",
            "m2_nssk",
            "m3_nssk_par",
            "m3_nssk",
            "m1_ds",
            "m2_ds",
            "m3_ds_par",
            "m3_ds"
        ]
    },
    {
        "session": "Optimal_BST",
        "title": "Optimal Binary Search Trees",
        "authors": [
            "Tobias Nipkow",
            "Dániel Somogyi"
        ],
        "topics": [
            "Computer science/Algorithms",
            "Computer science/Data structures"
        ],
        "date": "2018-05-27",
        "abstract": "\nThis article formalizes recursive algorithms for the construction\nof optimal binary search trees given fixed access frequencies.\nWe follow Knuth (1971), Yao (1980) and Mehlhorn (1984).\nThe algorithms are memoized with the help of the AFP article\n<a href=\"Monad_Memo_DP.html\">Monadification, Memoization and Dynamic Programming</a>,\nthus yielding dynamic programming algorithms.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2018-05-29"
            }
        ],
        "dependencies": [
            "Monad_Memo_DP"
        ],
        "theories": [
            "Weighted_Path_Length",
            "Optimal_BST",
            "Quadrilateral_Inequality",
            "Optimal_BST2",
            "Optimal_BST_Examples",
            "Optimal_BST_Code",
            "Optimal_BST_Memo"
        ]
    },
    {
        "session": "Card_Number_Partitions",
        "title": "Cardinality of Number Partitions",
        "authors": [
            "Lukas Bulwahn"
        ],
        "date": "2016-01-14",
        "topics": [
            "Mathematics/Combinatorics"
        ],
        "abstract": "\nThis entry provides a basic library for number partitions, defines the\ntwo-argument partition function through its recurrence relation and relates\nthis partition function to the cardinality of number partitions. The main\nproof shows that the recursively-defined partition function with arguments\nn and k equals the cardinality of number partitions of n with exactly k parts.\nThe combinatorial proof follows the proof sketch of Theorem 2.4.1 in\nMazur's textbook `Combinatorics: A Guided Tour`. This entry can serve as\nstarting point for various more intrinsic properties about number partitions,\nthe partition function and related recurrence relations.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2016-01-14"
            }
        ],
        "theories": [
            "Additions_to_Main",
            "Number_Partition",
            "Card_Number_Partitions"
        ]
    },
    {
        "session": "Isabelle_C",
        "title": "Isabelle/C",
        "authors": [
            "Frédéric Tuong",
            "Burkhart Wolff"
        ],
        "topics": [
            "Computer science/Programming languages/Language definitions",
            "Computer science/Semantics",
            "Tools"
        ],
        "date": "2019-10-22",
        "abstract": "\nWe present a framework for C code in C11 syntax deeply integrated into\nthe Isabelle/PIDE development environment. Our framework provides an\nabstract interface for verification back-ends to be plugged-in\nindependently. Thus, various techniques such as deductive program\nverification or white-box testing can be applied to the same source,\nwhich is part of an integrated PIDE document model. Semantic back-ends\nare free to choose the supported C fragment and its semantics. In\nparticular, they can differ on the chosen memory model or the\nspecification mechanism for framing conditions. Our framework supports\nsemantic annotations of C sources in the form of comments. Annotations\nserve to locally control back-end settings, and can express the term\nfocus to which an annotation refers. Both the logical and the\nsyntactic context are available when semantic annotations are\nevaluated. As a consequence, a formula in an annotation can refer both\nto HOL or C variables. Our approach demonstrates the degree of\nmaturity and expressive power the Isabelle/PIDE sub-system has\nachieved in recent years. Our integration technique employs Lex and\nYacc style grammars to ensure efficient deterministic parsing.  This\nis the core-module of Isabelle/C; the AFP package for Clean and\nClean_wrapper as well as AutoCorres and AutoCorres_wrapper (available\nvia git) are applications of this front-end.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-12-19"
            }
        ],
        "dependencies": [
            "Isar_Ref"
        ],
        "theories": [
            "C_Lexer_Language",
            "C_Ast",
            "files/AFP/Isabelle_C/C11-FrontEnd/generated/c_ast.ML",
            "C_Environment",
            "C_Parser_Language",
            "files/AFP/Isabelle_C/src_ext/mlton/lib/mlyacc-lib/base.sig",
            "files/AFP/Isabelle_C/src_ext/mlton/lib/mlyacc-lib/join.sml",
            "files/AFP/Isabelle_C/src_ext/mlton/lib/mlyacc-lib/lrtable.sml",
            "files/AFP/Isabelle_C/src_ext/mlton/lib/mlyacc-lib/stream.sml",
            "files/AFP/Isabelle_C/src_ext/mlton/lib/mlyacc-lib/parser1.sml",
            "files/AFP/Isabelle_C/C11-FrontEnd/generated/c_grammar_fun.grm.sig",
            "files/AFP/Isabelle_C/C11-FrontEnd/generated/c_grammar_fun.grm.sml",
            "C_Lexer_Annotation",
            "C_Parser_Annotation",
            "C_Eval",
            "C_Command",
            "C_Document",
            "C_Main",
            "C0",
            "files/AFP/Isabelle_C/src_ext/parser_menhir/tests/argument_scope.c",
            "files/AFP/Isabelle_C/src_ext/parser_menhir/tests/atomic_parenthesis.c",
            "files/AFP/Isabelle_C/src_ext/parser_menhir/tests/bitfield_declaration_ambiguity.c",
            "files/AFP/Isabelle_C/src_ext/parser_menhir/tests/bitfield_declaration_ambiguity.ok.c",
            "files/AFP/Isabelle_C/src_ext/parser_menhir/tests/block_scope.c",
            "files/AFP/Isabelle_C/src_ext/parser_menhir/tests/char-literal-printing.c",
            "files/AFP/Isabelle_C/src_ext/parser_menhir/tests/c-namespace.c",
            "files/AFP/Isabelle_C/src_ext/parser_menhir/tests/control-scope.c",
            "files/AFP/Isabelle_C/src_ext/parser_menhir/tests/dangling_else.c",
            "files/AFP/Isabelle_C/src_ext/parser_menhir/tests/declarators.c",
            "files/AFP/Isabelle_C/src_ext/parser_menhir/tests/designator.c",
            "files/AFP/Isabelle_C/src_ext/parser_menhir/tests/enum.c",
            "files/AFP/Isabelle_C/src_ext/parser_menhir/tests/enum_constant_visibility.c",
            "files/AFP/Isabelle_C/src_ext/parser_menhir/tests/enum_shadows_typedef.c",
            "files/AFP/Isabelle_C/src_ext/parser_menhir/tests/enum-trick.c",
            "files/AFP/Isabelle_C/src_ext/parser_menhir/tests/expressions.c",
            "files/AFP/Isabelle_C/src_ext/parser_menhir/tests/function-decls.c",
            "files/AFP/Isabelle_C/src_ext/parser_menhir/tests/local_scope.c",
            "files/AFP/Isabelle_C/src_ext/parser_menhir/tests/local_typedef.c",
            "files/AFP/Isabelle_C/src_ext/parser_menhir/tests/long-long-struct.c",
            "files/AFP/Isabelle_C/src_ext/parser_menhir/tests/namespaces.c",
            "files/AFP/Isabelle_C/src_ext/parser_menhir/tests/no_local_scope.c",
            "files/AFP/Isabelle_C/src_ext/parser_menhir/tests/parameter_declaration_ambiguity.c",
            "files/AFP/Isabelle_C/src_ext/parser_menhir/tests/parameter_declaration_ambiguity.test.c",
            "files/AFP/Isabelle_C/src_ext/parser_menhir/tests/statements.c",
            "files/AFP/Isabelle_C/src_ext/parser_menhir/tests/struct-recursion.c",
            "files/AFP/Isabelle_C/src_ext/parser_menhir/tests/typedef_star.c",
            "files/AFP/Isabelle_C/src_ext/parser_menhir/tests/types.c",
            "files/AFP/Isabelle_C/src_ext/parser_menhir/tests/variable_star.c",
            "files/AFP/Isabelle_C/src_ext/parser_menhir/tests/bitfield_declaration_ambiguity.fail.c",
            "C1",
            "C2",
            "C_paper",
            "C_Appendices",
            "README"
        ]
    },
    {
        "session": "Fermat3_4",
        "title": "Fermat's Last Theorem for Exponents 3 and 4 and the Parametrisation of Pythagorean Triples",
        "authors": [
            "Roelof Oosterhuis"
        ],
        "date": "2007-08-12",
        "topics": [
            "Mathematics/Number theory"
        ],
        "abstract": "This document presents the mechanised proofs of<ul><li>Fermat's Last Theorem for exponents 3 and 4 and</li><li>the parametrisation of Pythagorean Triples.</li></ul>",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-24"
            },
            {
                "2011-1": "2011-10-11"
            },
            {
                "2011": "2011-02-11"
            },
            {
                "2009-2": "2010-06-30"
            },
            {
                "2009-1": "2009-12-12"
            },
            {
                "2009": "2009-04-29"
            },
            {
                "2008": "2008-06-10"
            },
            {
                "2007": "2007-11-27"
            }
        ],
        "theories": [
            "Fermat4",
            "Quad_Form",
            "Fermat3"
        ]
    },
    {
        "session": "Formula_Derivatives",
        "title": "Derivatives of Logical Formulas",
        "authors": [
            "Dmitriy Traytel"
        ],
        "topics": [
            "Computer science/Automata and formal languages",
            "Logic/General logic/Decidability of theories"
        ],
        "date": "2015-05-28",
        "abstract": "\nWe formalize new decision procedures for WS1S, M2L(Str), and Presburger\nArithmetics. Formulas of these logics denote regular languages. Unlike\ntraditional decision procedures, we do <em>not</em> translate formulas into automata\n(nor into regular expressions), at least not explicitly. Instead we devise\nnotions of derivatives (inspired by Brzozowski derivatives for regular\nexpressions) that operate on formulas directly and compute a syntactic\nbisimulation using these derivatives. The treatment of Boolean connectives and\nquantifiers is uniform for all mentioned logics and is abstracted into a\nlocale. This locale is then instantiated by different atomic formulas and their\nderivatives (which may differ even for the same logic under different encodings\nof interpretations as formal words).\n<p>\nThe WS1S instance is described in the draft paper <a\nhref=\"https://people.inf.ethz.ch/trayteld/papers/csl15-ws1s_derivatives/index.html\">A\nCoalgebraic Decision Procedure for WS1S</a> by the author.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-28"
            }
        ],
        "dependencies": [
            "List-Index",
            "Coinductive_Languages",
            "Deriving"
        ],
        "theories": [
            "While_Default",
            "FSet_More",
            "Automaton",
            "Abstract_Formula",
            "WS1S_Prelim",
            "WS1S_Formula",
            "WS1S_Alt_Formula",
            "Presburger_Formula",
            "WS1S_Presburger_Equivalence",
            "WS1S_Nameful"
        ]
    },
    {
        "session": "ROBDD",
        "title": "Algorithms for Reduced Ordered Binary Decision Diagrams",
        "authors": [
            "Julius Michaelis",
            "Maximilian P. L. Haslbeck",
            "Peter Lammich",
            "Lars Hupel"
        ],
        "date": "2016-04-27",
        "topics": [
            "Computer science/Algorithms",
            "Computer science/Data structures"
        ],
        "abstract": "\nWe present a verified and executable implementation of ROBDDs in\nIsabelle/HOL. Our implementation relates pointer-based computation in\nthe Heap monad to operations on an abstract definition of boolean\nfunctions. Internally, we implemented the if-then-else combinator in a\nrecursive fashion, following the Shannon decomposition of the argument\nfunctions. The implementation mixes and adapts known techniques and is\nbuilt with efficiency in mind.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-04-27"
            }
        ],
        "dependencies": [
            "Sepref_Prereq",
            "Automatic_Refinement",
            "Collections",
            "Native_Word"
        ],
        "theories": [
            "Bool_Func",
            "BDT",
            "Option_Helpers",
            "Abstract_Impl",
            "Pointer_Map",
            "Middle_Impl",
            "Array_List",
            "Pointer_Map_Impl",
            "Conc_Impl",
            "Level_Collapse",
            "BDD_Examples",
            "BDD_Code"
        ]
    },
    {
        "session": "Progress_Tracking",
        "title": "Formalization of Timely Dataflow's Progress Tracking Protocol",
        "authors": [
            "Matthias Brun",
            "Sára Decova",
            "Andrea Lattuada",
            "Dmitriy Traytel"
        ],
        "topics": [
            "Computer science/Algorithms/Distributed"
        ],
        "date": "2021-04-13",
        "abstract": "\nLarge-scale stream processing systems often follow the dataflow\nparadigm, which enforces a program structure that exposes a high\ndegree of parallelism. The Timely Dataflow distributed system supports\nexpressive cyclic dataflows for which it offers low-latency data- and\npipeline-parallel stream processing. To achieve high expressiveness\nand performance, Timely Dataflow uses an intricate distributed\nprotocol for tracking the computation’s progress. We formalize this\nprogress tracking protocol and verify its safety. Our formalization is\ndescribed in detail in our forthcoming <a\nhref=\"https://traytel.bitbucket.io/papers/itp21-progress_tracking/safe.pdf\">ITP'21\npaper</a>.",
        "licence": "BSD",
        "dependencies": [
            "Nested_Multisets_Ordinals"
        ],
        "theories": [
            "Auxiliary",
            "Exchange_Abadi",
            "Exchange",
            "Antichain",
            "Graph",
            "Propagate",
            "Combined"
        ]
    },
    {
        "session": "Coinductive_Languages",
        "title": "A Codatatype of Formal Languages",
        "authors": [
            "Dmitriy Traytel"
        ],
        "topics": [
            "Computer science/Automata and formal languages"
        ],
        "date": "2013-11-15",
        "abstract": "<p>We define formal languages as a codataype of infinite trees\nbranching over the alphabet. Each node in such a tree indicates whether the\npath to this node constitutes a word inside or outside of the language. This\ncodatatype is isormorphic to the set of lists representation of languages,\nbut caters for definitions by corecursion and proofs by coinduction.</p>\n<p>Regular operations on languages are then defined by primitive corecursion.\nA difficulty arises here, since the standard definitions of concatenation and\niteration from the coalgebraic literature are not primitively\ncorecursive-they require guardedness up-to union/concatenation.\nWithout support for up-to corecursion, these operation must be defined as a\ncomposition of primitive ones (and proved being equal to the standard\ndefinitions). As an exercise in coinduction we also prove the axioms of\nKleene algebra for the defined regular operations.</p>\n<p>Furthermore, a language for context-free grammars given by productions in\nGreibach normal form and an initial nonterminal is constructed by primitive\ncorecursion, yielding an executable decision procedure for the word problem\nwithout further ado.</p>",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            }
        ],
        "dependencies": [
            "Regular-Sets"
        ],
        "theories": [
            "Coinductive_Language",
            "Coinductive_Regular_Set",
            "Context_Free_Grammar"
        ]
    },
    {
        "session": "Kuratowski_Closure_Complement",
        "title": "The Kuratowski Closure-Complement Theorem",
        "authors": [
            "Peter Gammie",
            "Gianpaolo Gioiosa"
        ],
        "topics": [
            "Mathematics/Topology"
        ],
        "date": "2017-10-26",
        "abstract": "\nWe discuss a topological curiosity discovered by Kuratowski (1922):\nthe fact that the number of distinct operators on a topological space\ngenerated by compositions of closure and complement never exceeds 14,\nand is exactly 14 in the case of R. In addition, we prove a theorem\ndue to Chagrov (1982) that classifies topological spaces according to\nthe number of such operators they support.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-27"
            }
        ],
        "theories": [
            "KuratowskiClosureComplementTheorem"
        ]
    },
    {
        "session": "Ptolemys_Theorem",
        "title": "Ptolemy's Theorem",
        "authors": [
            "Lukas Bulwahn"
        ],
        "date": "2016-08-07",
        "topics": [
            "Mathematics/Geometry"
        ],
        "abstract": "\nThis entry provides an analytic proof to Ptolemy's Theorem using\npolar form transformation and trigonometric identities.\nIn this formalization, we use ideas from John Harrison's HOL Light\nformalization and the proof sketch on the Wikipedia entry of Ptolemy's Theorem.\nThis theorem is the 95th theorem of the Top 100 Theorems list.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-08-08"
            }
        ],
        "theories": [
            "Ptolemys_Theorem"
        ]
    },
    {
        "session": "Furstenberg_Topology",
        "title": "Furstenberg's topology and his proof of the infinitude of primes",
        "authors": [
            "Manuel Eberl"
        ],
        "topics": [
            "Mathematics/Number theory"
        ],
        "date": "2020-03-22",
        "abstract": "\n<p>This article gives a formal version of Furstenberg's\ntopological proof of the infinitude of primes. He defines a topology\non the integers based on arithmetic progressions (or, equivalently,\nresidue classes). Using some fairly obvious properties of this\ntopology, the infinitude of primes is then easily obtained.</p>\n<p>Apart from this, this topology is also fairly ‘nice’ in\ngeneral: it is second countable, metrizable, and perfect. All of these\n(well-known) facts are formally proven, including an explicit metric\nfor the topology given by Zulfeqarr.</p>",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2020-03-27"
            }
        ],
        "theories": [
            "Furstenberg_Topology"
        ]
    },
    {
        "session": "Separation_Algebra",
        "title": "Separation Algebra",
        "authors": [
            "Gerwin Klein",
            "Rafal Kolanski",
            "Andrew Boyton"
        ],
        "date": "2012-05-11",
        "topics": [
            "Computer science/Programming languages/Logics"
        ],
        "license": "BSD",
        "abstract": "We present a generic type class implementation of separation algebra for Isabelle/HOL as well as lemmas and generic tactics which can be used directly for any instantiation of the type class. <P> The ex directory contains example instantiations that include structures such as a heap or virtual memory. <P> The abstract separation algebra is based upon \"Abstract Separation Logic\" by Calcagno et al. These theories are also the basis of the ITP 2012 rough diamond \"Mechanised Separation Algebra\" by the authors. <P> The aim of this work is to support and significantly reduce the effort for future separation logic developments in Isabelle/HOL by factoring out the part of separation logic that can be treated abstractly once and for all. This includes developing typical default rule sets for reasoning as well as automated tactic support for separation logic.",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-24"
            },
            {
                "2011-1": "2012-05-11"
            }
        ],
        "theories": [
            "Separation_Algebra",
            "Sep_Heap_Instance",
            "Sep_Tactics",
            "files/sep_tactics.ML",
            "Simple_Separation_Example",
            "Sep_Tactics_Test",
            "Map_Extra",
            "VM_Example",
            "Separation_Algebra_Alt",
            "Sep_Eq",
            "Types_D",
            "Abstract_Separation_D",
            "Separation_D"
        ]
    },
    {
        "session": "Locally-Nameless-Sigma",
        "title": "Locally Nameless Sigma Calculus",
        "authors": [
            "Ludovic Henrio",
            "Florian Kammüller",
            "Bianca Lutz",
            "Henry Sudhof"
        ],
        "date": "2010-04-30",
        "topics": [
            "Computer science/Programming languages/Language definitions"
        ],
        "abstract": "We present a Theory of Objects based on the original functional sigma-calculus by Abadi and Cardelli but with an additional parameter to methods. We prove confluence of the operational semantics following the outline of Nipkow's proof of confluence for the lambda-calculus reusing his theory Commutation, a generic diamond lemma reduction. We furthermore formalize a simple type system for our sigma-calculus including a proof of type safety. The entire development uses the concept of Locally Nameless representation for binders. We reuse an earlier proof of confluence for a simpler sigma-calculus based on de Bruijn indices and lists to represent objects.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-24"
            },
            {
                "2011-1": "2011-10-11"
            },
            {
                "2011": "2011-02-11"
            },
            {
                "2009-2": "2010-07-01"
            },
            {
                "2009-1": "2010-05-03"
            }
        ],
        "dependencies": [
            "Applicative_Lifting"
        ],
        "theories": [
            "ListPre",
            "FMap",
            "Sigma",
            "ParRed",
            "Environments",
            "TypedSigma",
            "Locally_Nameless_Sigma"
        ]
    },
    {
        "session": "Decreasing-Diagrams-II",
        "title": "Decreasing Diagrams II",
        "authors": [
            "Bertram Felgenhauer"
        ],
        "license": "LGPL",
        "date": "2015-08-20",
        "topics": [
            "Logic/Rewriting"
        ],
        "abstract": "This theory formalizes the commutation version of decreasing diagrams for Church-Rosser modulo. The proof follows Felgenhauer and van Oostrom (RTA 2013). The theory also provides important specializations, in particular van Oostrom’s conversion version (TCS 2008) of decreasing diagrams.",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-08-21"
            }
        ],
        "dependencies": [
            "Abstract-Rewriting",
            "Open_Induction",
            "Well_Quasi_Orders"
        ],
        "theories": [
            "Decreasing_Diagrams_II_Aux",
            "Decreasing_Diagrams_II"
        ]
    },
    {
        "session": "Prime_Distribution_Elementary",
        "title": "Elementary Facts About the Distribution of Primes",
        "authors": [
            "Manuel Eberl"
        ],
        "topics": [
            "Mathematics/Number theory"
        ],
        "date": "2019-02-21",
        "abstract": "\n<p>This entry is a formalisation of Chapter 4 (and parts of\nChapter 3) of Apostol's <a\nhref=\"https://www.springer.com/de/book/9780387901633\"><em>Introduction\nto Analytic Number Theory</em></a>. The main topics that\nare addressed are properties of the distribution of prime numbers that\ncan be shown in an elementary way (i.&thinsp;e. without the Prime\nNumber Theorem), the various equivalent forms of the PNT (which imply\neach other in elementary ways), and consequences that follow from the\nPNT in elementary ways. The latter include, most notably, asymptotic\nbounds for the number of distinct prime factors of\n<em>n</em>, the divisor function\n<em>d(n)</em>, Euler's totient function\n<em>&phi;(n)</em>, and\nlcm(1,&hellip;,<em>n</em>).</p>",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2019-02-22"
            }
        ],
        "dependencies": [
            "Zeta_Function",
            "Prime_Number_Theorem"
        ],
        "theories": [
            "Prime_Distribution_Elementary_Library",
            "More_Dirichlet_Misc",
            "Primes_Omega",
            "Primorial",
            "Lcm_Nat_Upto",
            "Shapiro_Tauberian",
            "Partial_Zeta_Bounds",
            "Moebius_Mu_Sum",
            "Elementary_Prime_Bounds",
            "Summatory_Divisor_Sigma_Bounds",
            "Selberg_Asymptotic_Formula",
            "PNT_Consequences"
        ]
    },
    {
        "session": "Error_Function",
        "title": "The Error Function",
        "authors": [
            "Manuel Eberl"
        ],
        "topics": [
            "Mathematics/Analysis"
        ],
        "date": "2018-02-06",
        "abstract": "\n<p> This entry provides the definitions and basic properties of\nthe complex and real error function erf and the complementary error\nfunction erfc. Additionally, it gives their full asymptotic\nexpansions. </p>",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2018-02-07"
            }
        ],
        "dependencies": [
            "Landau_Symbols"
        ],
        "theories": [
            "Error_Function",
            "Error_Function_Asymptotics"
        ]
    },
    {
        "session": "Statecharts",
        "title": "Formalizing Statecharts using Hierarchical Automata",
        "authors": [
            "Steffen Helke",
            "Florian Kammüller"
        ],
        "topics": [
            "Computer science/Automata and formal languages"
        ],
        "date": "2010-08-08",
        "abstract": "We formalize in Isabelle/HOL the abtract syntax and a synchronous\nstep semantics for the specification language Statecharts. The formalization\nis based on Hierarchical Automata which allow a structural decomposition of\nStatecharts into Sequential Automata. To support the composition of\nStatecharts, we introduce calculating operators to construct a Hierarchical\nAutomaton in a stepwise manner. Furthermore, we present a complete semantics\nof Statecharts including a theory of data spaces, which enables the modelling\nof racing effects. We also adapt CTL for\nStatecharts to build a bridge for future combinations with model\nchecking. However the main motivation of this work is to provide a sound and\ncomplete basis for reasoning on Statecharts. As a central meta theorem we\nprove that the well-formedness of a Statechart is preserved by the semantics.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-24"
            },
            {
                "2011-1": "2011-10-11"
            },
            {
                "2011": "2011-02-11"
            },
            {
                "2009-2": "2010-08-18"
            }
        ],
        "theories": [
            "Contrib",
            "DataSpace",
            "Data",
            "Update",
            "Expr",
            "SA",
            "HA",
            "HASem",
            "Kripke",
            "HAKripke",
            "HAOps",
            "CarAudioSystem"
        ]
    },
    {
        "session": "Padic_Ints",
        "title": "Hensel's Lemma for the p-adic Integers",
        "authors": [
            "Aaron Crighton"
        ],
        "topics": [
            "Mathematics/Number theory"
        ],
        "date": "2021-03-23",
        "abstract": "\nWe formalize the ring of <em>p</em>-adic integers within the framework of the\nHOL-Algebra library. The carrier of the ring is formalized as the\ninverse limit of quotients of the integers by powers of a fixed prime\n<em>p</em>. We define an integer-valued valuation, as well as an\nextended-integer valued valuation which sends 0 to the infinite\nelement. Basic topological facts about the <em>p</em>-adic integers are\nformalized, including completeness and sequential compactness. Taylor\nexpansions of polynomials over a commutative ring are defined,\nculminating in the formalization of Hensel's Lemma based on a\nproof due to Keith Conrad.",
        "licence": "BSD",
        "theories": [
            "Function_Ring",
            "Cring_Poly",
            "Supplementary_Ring_Facts",
            "Extended_Int",
            "Padic_Construction",
            "Padic_Integers",
            "Padic_Int_Topology",
            "Padic_Int_Polynomials",
            "Hensels_Lemma",
            "Zp_Compact"
        ]
    },
    {
        "session": "Stable_Matching",
        "title": "Stable Matching",
        "authors": [
            "Peter Gammie"
        ],
        "date": "2016-10-24",
        "topics": [
            "Mathematics/Games and economics"
        ],
        "abstract": "\nWe mechanize proofs of several results from the matching with\ncontracts literature, which generalize those of the classical\ntwo-sided matching scenarios that go by the name of stable marriage.\nOur focus is on game theoretic issues. Along the way we develop\nexecutable algorithms for computing optimal stable matches.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-10-24"
            }
        ],
        "theories": [
            "Sotomayor",
            "Basis",
            "Choice_Functions",
            "Contracts",
            "COP",
            "Bossiness",
            "Strategic"
        ]
    },
    {
        "session": "Types_Tableaus_and_Goedels_God",
        "title": "Types, Tableaus and Gödel’s God in Isabelle/HOL",
        "authors": [
            "David Fuenmayor",
            "Christoph Benzmüller"
        ],
        "topics": [
            "Logic/Philosophical aspects"
        ],
        "date": "2017-05-01",
        "abstract": "\nA computer-formalisation of the essential parts of Fitting's\ntextbook \"Types, Tableaus and Gödel's God\" in\nIsabelle/HOL is presented. In particular, Fitting's (and\nAnderson's) variant of the ontological argument is verified and\nconfirmed. This variant avoids the modal collapse, which has been\ncriticised as an undesirable side-effect of Kurt Gödel's (and\nDana Scott's) versions of the ontological argument.\nFitting's work is employing an intensional higher-order modal\nlogic, which we shallowly embed here in classical higher-order logic.\nWe then utilize the embedded logic for the formalisation of\nFitting's argument. (See also the earlier AFP entry ``Gödel's God in Isabelle/HOL''.)",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2017-05-02"
            }
        ],
        "theories": [
            "Relations",
            "IHOML",
            "IHOML_Examples",
            "GoedelProof_P1",
            "GoedelProof_P2",
            "FittingProof",
            "AndersonProof"
        ]
    },
    {
        "session": "Laplace_Transform",
        "title": "Laplace Transform",
        "authors": [
            "Fabian Immler"
        ],
        "topics": [
            "Mathematics/Analysis"
        ],
        "date": "2019-08-14",
        "abstract": "\nThis entry formalizes the Laplace transform and concrete Laplace\ntransforms for arithmetic functions, frequency shift, integration and\n(higher) differentiation in the time domain. It proves Lerch's\nlemma and uniqueness of the Laplace transform for continuous\nfunctions. In order to formalize the foundational assumptions, this\nentry contains a formalization of piecewise continuous functions and\nfunctions of exponential order.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-08-16"
            }
        ],
        "theories": [
            "Laplace_Transform_Library",
            "Piecewise_Continuous",
            "Existence",
            "Lerch_Lemma",
            "Uniqueness",
            "Laplace_Transform"
        ]
    },
    {
        "session": "Lambda_Free_RPOs",
        "title": "Formalization of Recursive Path Orders for Lambda-Free Higher-Order Terms",
        "authors": [
            "Jasmin Christian Blanchette",
            "Uwe Waldmann",
            "Daniel Wand"
        ],
        "date": "2016-09-23",
        "topics": [
            "Logic/Rewriting"
        ],
        "abstract": "This Isabelle/HOL formalization defines recursive path orders (RPOs) for higher-order terms without lambda-abstraction and proves many useful properties about them. The main order fully coincides with the standard RPO on first-order terms also in the presence of currying, distinguishing it from previous work. An optimized variant is formalized as well. It appears promising as the basis of a higher-order superposition calculus.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            }
        ],
        "dependencies": [
            "Nested_Multisets_Ordinals"
        ],
        "theories": [
            "Lambda_Free_Util",
            "Lambda_Free_Term",
            "Infinite_Chain",
            "Extension_Orders",
            "Lambda_Free_RPO_App",
            "Lambda_Free_RPO_Std",
            "Lambda_Free_RPO_Optim",
            "Lambda_Encoding",
            "Lambda_Free_RPOs"
        ]
    },
    {
        "session": "HRB-Slicing",
        "title": "Backing up Slicing: Verifying the Interprocedural Two-Phase Horwitz-Reps-Binkley Slicer",
        "authors": [
            "Daniel Wasserrab"
        ],
        "date": "2009-11-13",
        "topics": [
            "Computer science/Programming languages/Static analysis"
        ],
        "abstract": "After verifying <a href=\"Slicing.html\">dynamic and static interprocedural slicing</a>, we present a modular framework for static interprocedural slicing. To this end, we formalized the standard two-phase slicer from Horwitz, Reps and Binkley (see their TOPLAS 12(1) 1990 paper) together with summary edges as presented by Reps et al. (see FSE 1994). The framework is again modular in the programming language by using an abstract CFG, defined via structural and well-formedness properties. Using a weak simulation between the original and sliced graph, we were able to prove the correctness of static interprocedural slicing. We also instantiate our framework with a simple While language with procedures. This shows that the chosen abstractions are indeed valid.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-24"
            },
            {
                "2011-1": "2011-10-11"
            },
            {
                "2011": "2011-02-11"
            },
            {
                "2009-2": "2010-07-01"
            },
            {
                "2009-1": "2009-12-12"
            },
            {
                "2009": "2009-11-19"
            }
        ],
        "dependencies": [
            "Jinja"
        ],
        "theories": [
            "AuxLemmas",
            "BasicDefs",
            "CFG",
            "CFGExit",
            "CFG_wf",
            "CFGExit_wf",
            "SemanticsCFG",
            "ReturnAndCallNodes",
            "Observable",
            "Postdomination",
            "SDG",
            "HRBSlice",
            "SCDObservable",
            "Distance",
            "Slice",
            "WeakSimulation",
            "FundamentalProperty",
            "Com",
            "ProcState",
            "PCFG",
            "WellFormProgs",
            "Interpretation",
            "Labels",
            "WellFormed",
            "ValidPaths",
            "ProcSDG",
            "JVMCFG",
            "JVMInterpretation",
            "JVMCFG_wf",
            "JVMPostdomination",
            "JVMSDG",
            "HRBSlicing"
        ]
    },
    {
        "session": "Diophantine_Eqns_Lin_Hom",
        "title": "Homogeneous Linear Diophantine Equations",
        "authors": [
            "Florian Messner",
            "Julian Parsert",
            "Jonas Schöpf",
            "Christian Sternagel"
        ],
        "topics": [
            "Computer science/Algorithms/Mathematical",
            "Mathematics/Number theory",
            "Tools"
        ],
        "license": "LGPL",
        "date": "2017-10-14",
        "abstract": "\nWe formalize the theory of homogeneous linear diophantine equations,\nfocusing on two main results: (1) an abstract characterization of\nminimal complete sets of solutions, and (2) an algorithm computing\nthem. Both, the characterization and the algorithm are based on\nprevious work by Huet. Our starting point is a simple but inefficient\nvariant of Huet's lexicographic algorithm incorporating improved\nbounds due to Clausen and Fortenbacher. We proceed by proving its\nsoundness and completeness. Finally, we employ code equations to\nobtain a reasonably efficient implementation. Thus, we provide a\nformally verified solver for homogeneous linear diophantine equations.",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-15"
            }
        ],
        "theories": [
            "List_Vector",
            "Linear_Diophantine_Equations",
            "Sorted_Wrt",
            "Minimize_Wrt",
            "Simple_Algorithm",
            "Algorithm",
            "Solver_Code",
            "files/src/Main.hs"
        ]
    },
    {
        "session": "ConcurrentGC",
        "title": "Relaxing Safely: Verified On-the-Fly Garbage Collection for x86-TSO",
        "authors": [
            "Peter Gammie",
            "Tony Hosking",
            "Kai Engelhardt"
        ],
        "date": "2015-04-13",
        "topics": [
            "Computer science/Algorithms/Concurrent"
        ],
        "abstract": "\n<p>\nWe use ConcurrentIMP to model Schism, a state-of-the-art real-time\ngarbage collection scheme for weak memory, and show that it is safe\non x86-TSO.</p>\n<p>\nThis development accompanies the PLDI 2015 paper of the same name.\n</p>",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2015-04-15"
            }
        ],
        "dependencies": [
            "ConcurrentIMP"
        ],
        "theories": [
            "Model",
            "Proofs_Basis",
            "Global_Invariants",
            "Local_Invariants",
            "Tactics",
            "Global_Invariants_Lemmas",
            "Local_Invariants_Lemmas",
            "Initial_Conditions",
            "Noninterference",
            "Global_Noninterference",
            "MarkObject",
            "Phases",
            "StrongTricolour",
            "TSO",
            "Valid_Refs",
            "Worklists",
            "Proofs",
            "Concrete_heap",
            "Concrete"
        ]
    },
    {
        "session": "Minkowskis_Theorem",
        "title": "Minkowski's Theorem",
        "authors": [
            "Manuel Eberl"
        ],
        "topics": [
            "Mathematics/Geometry",
            "Mathematics/Number theory"
        ],
        "date": "2017-07-13",
        "abstract": "\n<p>Minkowski's theorem relates a subset of\n&#8477;<sup>n</sup>, the Lebesgue measure, and the\ninteger lattice &#8484;<sup>n</sup>: It states that\nany convex subset of &#8477;<sup>n</sup> with volume\ngreater than 2<sup>n</sup> contains at least one lattice\npoint from &#8484;<sup>n</sup>\\{0}, i.&thinsp;e. a\nnon-zero point with integer coefficients.</p>  <p>A\nrelated theorem which directly implies this is Blichfeldt's\ntheorem, which states that any subset of\n&#8477;<sup>n</sup> with a volume greater than 1\ncontains two different points whose difference vector has integer\ncomponents.</p>  <p>The entry contains a proof of both\ntheorems.</p>",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2017-07-15"
            }
        ],
        "theories": [
            "Minkowskis_Theorem"
        ]
    },
    {
        "session": "Planarity_Certificates",
        "title": "Planarity Certificates",
        "authors": [
            "Lars Noschinski"
        ],
        "date": "2015-11-11",
        "topics": [
            "Mathematics/Graph theory"
        ],
        "abstract": "\nThis development provides a formalization of planarity based on\ncombinatorial maps and proves that Kuratowski's theorem implies\ncombinatorial planarity.\nMoreover, it contains verified implementations of programs checking\ncertificates for planarity (i.e., a combinatorial map) or non-planarity\n(i.e., a Kuratowski subgraph).",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            }
        ],
        "dependencies": [
            "Simpl",
            "List-Index",
            "Transitive-Closure",
            "Case_Labeling",
            "Graph_Theory"
        ],
        "theories": [
            "Lib",
            "OptionMonad",
            "NonDetMonad",
            "NonDetMonadLemmas",
            "OptionMonadND",
            "WP",
            "files/WP-method.ML",
            "OptionMonadWP",
            "Graph_Genus",
            "List_Aux",
            "Executable_Permutations",
            "Digraph_Map_Impl",
            "Planar_Complete",
            "Reachablen",
            "Permutations_2",
            "Planar_Subdivision",
            "Planar_Subgraph",
            "Kuratowski_Combinatorial",
            "Simpl_Anno",
            "Check_Non_Planarity_Impl",
            "Check_Non_Planarity_Verification",
            "AutoCorres_Misc",
            "Setup_AutoCorres",
            "files/AFP/Case_Labeling/util.ML",
            "Check_Planarity_Verification",
            "Planarity_Certificates"
        ]
    },
    {
        "session": "UTP",
        "title": "Isabelle/UTP: Mechanised Theory Engineering for Unifying Theories of Programming",
        "authors": [
            "Simon Foster",
            "Frank Zeyda",
            "Yakoub Nemouchi",
            "Pedro Ribeiro",
            "Burkhart Wolff"
        ],
        "topics": [
            "Computer science/Programming languages/Logics"
        ],
        "date": "2019-02-01",
        "abstract": "\nIsabelle/UTP is a mechanised theory engineering toolkit based on Hoare\nand He’s Unifying Theories of Programming (UTP). UTP enables the\ncreation of denotational, algebraic, and operational semantics for\ndifferent programming languages using an alphabetised relational\ncalculus. We provide a semantic embedding of the alphabetised\nrelational calculus in Isabelle/HOL, including new type definitions,\nrelational constructors, automated proof tactics, and accompanying\nalgebraic laws. Isabelle/UTP can be used to both capture laws of\nprogramming for different languages, and put these fundamental\ntheorems to work in the creation of associated verification tools,\nusing calculi like Hoare logics. This document describes the\nrelational core of the UTP in Isabelle/HOL.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2019-02-06"
            }
        ],
        "dependencies": [
            "UTP-Toolkit"
        ],
        "theories": [
            "utp_parser_utils",
            "utp_var",
            "utp_expr",
            "utp_expr_insts",
            "utp_expr_funcs",
            "utp_unrest",
            "utp_usedby",
            "utp_subst",
            "utp_tactics",
            "files/uexpr_rep_eq.ML",
            "files/utp_tactics.ML",
            "utp_meta_subst",
            "utp_pred",
            "utp_alphabet",
            "utp_lift",
            "utp_pred_laws",
            "utp_healthy",
            "utp_rel",
            "utp_recursion",
            "utp_sequent",
            "utp_rel_laws",
            "utp_theory",
            "utp_hoare",
            "utp_wp",
            "utp_dynlog",
            "utp_state_parser",
            "utp_rel_opsem",
            "utp_sym_eval",
            "utp_sp",
            "utp_concurrency",
            "utp",
            "utp_expr_ovld",
            "utp_full",
            "utp_easy_parser",
            "sum_list",
            "utp_simple_time"
        ]
    },
    {
        "session": "HereditarilyFinite",
        "title": "The Hereditarily Finite Sets",
        "authors": [
            "Lawrence C. Paulson"
        ],
        "date": "2013-11-17",
        "topics": [
            "Logic/Set theory"
        ],
        "abstract": "The theory of hereditarily finite sets is formalised, following\nthe <a href=\"http://journals.impan.gov.pl/dm/Inf/422-0-1.html\">development</a> of Swierczkowski.\nAn HF set is a finite collection of other HF sets; they enjoy an induction principle\nand satisfy all the axioms of ZF set theory apart from the axiom of infinity, which is negated.\nAll constructions that are possible in ZF set theory (Cartesian products, disjoint sums, natural numbers,\nfunctions) without using infinite sets are possible here.\nThe definition of addition for the HF sets follows Kirby.\nThis development forms the foundation for the Isabelle proof of Gödel's incompleteness theorems,\nwhich has been <a href=\"Incompleteness.html\">formalised separately</a>.",
        "extra": {
            "Change history": "[2015-02-23] Added the theory \"Finitary\" defining the class of types that can be embedded in hf, including int, char, option, list, etc."
        },
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            }
        ],
        "theories": [
            "HF",
            "Ordinal",
            "Rank",
            "Finite_Automata",
            "Finitary",
            "OrdArith"
        ]
    },
    {
        "session": "VeriComp",
        "title": "A Generic Framework for Verified Compilers",
        "authors": [
            "Martin Desharnais"
        ],
        "topics": [
            "Computer science/Programming languages/Compiling"
        ],
        "date": "2020-02-10",
        "abstract": "\nThis is a generic framework for formalizing compiler transformations.\nIt leverages Isabelle/HOL’s locales to abstract over concrete\nlanguages and transformations. It states common definitions for\nlanguage semantics, program behaviours, forward and backward\nsimulations, and compilers. We provide generic operations, such as\nsimulation and compiler composition, and prove general (partial)\ncorrectness theorems, resulting in reusable proof components.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2020-02-25"
            }
        ],
        "theories": [
            "Behaviour",
            "Well_founded",
            "Inf",
            "Transfer_Extras",
            "Semantics",
            "Language",
            "Simulation",
            "Compiler",
            "Fixpoint"
        ]
    },
    {
        "session": "AI_Planning_Languages_Semantics",
        "title": "AI Planning Languages Semantics",
        "authors": [
            "Mohammad Abdulaziz",
            "Peter Lammich"
        ],
        "topics": [
            "Computer science/Artificial intelligence"
        ],
        "date": "2020-10-29",
        "abstract": "\nThis is an Isabelle/HOL formalisation of the semantics of the\nmulti-valued planning tasks language that is used by the planning\nsystem Fast-Downward, the STRIPS fragment of the Planning Domain\nDefinition Language (PDDL), and the STRIPS soundness meta-theory\ndeveloped by Vladimir Lifschitz. It also contains formally verified\ncheckers for checking the well-formedness of problems specified in\neither language as well the correctness of potential solutions. The\nformalisation in this entry was described in an earlier publication.",
        "licence": "BSD",
        "dependencies": [
            "Certification_Monads",
            "Show",
            "Containers",
            "Propositional_Proof_Systems"
        ],
        "theories": [
            "Error_Monad_Add",
            "Option_Monad_Add",
            "SASP_Semantics",
            "SASP_Checker",
            "PDDL_STRIPS_Semantics",
            "PDDL_STRIPS_Checker",
            "Lifschitz_Consistency"
        ]
    },
    {
        "session": "Probabilistic_System_Zoo",
        "title": "A Zoo of Probabilistic Systems",
        "authors": [
            "Johannes Hölzl",
            "Andreas Lochbihler",
            "Dmitriy Traytel"
        ],
        "date": "2015-05-27",
        "topics": [
            "Computer science/Automata and formal languages"
        ],
        "abstract": "\nNumerous models of probabilistic systems are studied in the literature.\nCoalgebra has been used to classify them into system types and compare their\nexpressiveness.  We formalize the resulting hierarchy of probabilistic system\ntypes by modeling the semantics of the different systems as codatatypes.\nThis approach yields simple and concise proofs, as bisimilarity coincides\nwith equality for codatatypes.\n<p>\nThis work is described in detail in the ITP 2015 publication by the authors.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-28"
            }
        ],
        "theories": [
            "Bool_Bounded_Set",
            "Finitely_Bounded_Set_Counterexample",
            "Nonempty_Bounded_Set",
            "Probabilistic_Hierarchy",
            "Vardi",
            "Vardi_Counterexample"
        ]
    },
    {
        "session": "Linear_Recurrences",
        "title": "Linear Recurrences",
        "authors": [
            "Manuel Eberl"
        ],
        "topics": [
            "Mathematics/Analysis"
        ],
        "date": "2017-10-12",
        "abstract": "\n<p> Linear recurrences with constant coefficients are an\ninteresting class of recurrence equations that can be solved\nexplicitly. The most famous example are certainly the Fibonacci\nnumbers with the equation <i>f</i>(<i>n</i>) =\n<i>f</i>(<i>n</i>-1) +\n<i>f</i>(<i>n</i> - 2) and the quite\nnon-obvious closed form\n(<i>&phi;</i><sup><i>n</i></sup>\n-\n(-<i>&phi;</i>)<sup>-<i>n</i></sup>)\n/ &radic;<span style=\"text-decoration:\noverline\">5</span> where &phi; is the golden ratio.\n</p> <p> In this work, I build on existing tools in\nIsabelle &ndash; such as formal power series and polynomial\nfactorisation algorithms &ndash; to develop a theory of these\nrecurrences and derive a fully executable solver for them that can be\nexported to programming languages like Haskell. </p>",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-17"
            }
        ],
        "dependencies": [
            "Count_Complex_Roots",
            "Polynomial_Factorization"
        ],
        "theories": [
            "RatFPS",
            "Pochhammer_Polynomials",
            "Linear_Recurrences_Misc",
            "Partial_Fraction_Decomposition",
            "Factorizations",
            "Rational_FPS_Solver",
            "Linear_Recurrences_Common",
            "Linear_Homogenous_Recurrences",
            "Eulerian_Polynomials",
            "Linear_Inhomogenous_Recurrences",
            "Rational_FPS_Asymptotics"
        ]
    },
    {
        "session": "DFS_Framework",
        "title": "A Framework for Verifying Depth-First Search Algorithms",
        "authors": [
            "Peter Lammich",
            "René Neumann"
        ],
        "date": "2016-07-05",
        "topics": [
            "Computer science/Algorithms/Graph"
        ],
        "abstract": "\n<p>\nThis entry presents a framework for the modular verification of\nDFS-based algorithms, which is described in our [CPP-2015] paper. It\nprovides a generic DFS algorithm framework, that can be parameterized\nwith user-defined actions on certain events (e.g. discovery of new\nnode).  It comes with an extensible library of invariants, which can\nbe used to derive invariants of a specific parameterization.  Using\nrefinement techniques, efficient implementations of the algorithms can\neasily be derived. Here, the framework comes with templates for a\nrecursive and a tail-recursive implementation, and also with several\ntemplates for implementing the data structures required by the DFS\nalgorithm.  Finally, this entry contains a set of re-usable DFS-based\nalgorithms, which illustrate the application of the framework.\n</p><p>\n[CPP-2015] Peter Lammich, René Neumann: A Framework for Verifying\nDepth-First Search Algorithms. CPP 2015: 137-146</p>",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2020-01-14"
            },
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-07-05"
            }
        ],
        "dependencies": [
            "CAVA_Automata"
        ],
        "theories": [
            "DFS_Framework_Misc",
            "DFS_Framework_Refine_Aux",
            "Impl_Rev_Array_Stack",
            "On_Stack",
            "DFS_Chapter_Framework",
            "Param_DFS",
            "DFS_Invars_Basic",
            "DFS_Invars_SCC",
            "General_DFS_Structure",
            "Tailrec_Impl",
            "Rec_Impl",
            "Simple_Impl",
            "Restr_Impl",
            "DFS_Framework",
            "DFS_Chapter_Examples",
            "Cyc_Check",
            "DFS_Find_Path",
            "Reachable_Nodes",
            "Feedback_Arcs",
            "Nested_DFS",
            "Tarjan_LowLink",
            "Tarjan",
            "DFS_All_Examples"
        ]
    },
    {
        "session": "First_Welfare_Theorem",
        "title": "Microeconomics and the First Welfare Theorem",
        "authors": [
            "Julian Parsert",
            "Cezary Kaliszyk"
        ],
        "topics": [
            "Mathematics/Games and economics"
        ],
        "license": "LGPL",
        "date": "2017-09-01",
        "abstract": "\nEconomic activity has always been a fundamental part of society. Due\nto modern day politics, economic theory has gained even more influence\non our lives. Thus we want models and theories to be as precise as\npossible. This can be achieved using certification with the help of\nformal proof technology. Hence we will use Isabelle/HOL to construct\ntwo economic models, that of the the pure exchange economy and a\nversion of the Arrow-Debreu Model. We will prove that the\n<i>First Theorem of Welfare Economics</i> holds within\nboth. The theorem is the mathematical formulation of Adam Smith's\nfamous <i>invisible hand</i> and states that a group of\nself-interested and rational actors will eventually achieve an\nefficient allocation of goods and services.",
        "extra": {
            "Change history": "[2018-06-17] Added some lemmas and a theory file, also introduced Microeconomics folder.\n<br>"
        },
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2017-09-05"
            },
            {
                "2016-1": "2017-09-04"
            }
        ],
        "theories": [
            "Syntax",
            "Argmax",
            "Preferences",
            "Utility_Functions",
            "Consumers",
            "Common",
            "Exchange_Economy",
            "Private_Ownership_Economy",
            "Arrow_Debreu_Model"
        ]
    },
    {
        "session": "Monomorphic_Monad",
        "title": "Effect polymorphism in higher-order logic",
        "authors": [
            "Andreas Lochbihler"
        ],
        "topics": [
            "Computer science/Functional programming"
        ],
        "date": "2017-05-05",
        "abstract": "\nThe notion of a monad cannot be expressed within higher-order logic\n(HOL) due to type system restrictions. We show that if a monad is used\nwith values of only one type, this notion can be formalised in HOL.\nBased on this idea, we develop a library of effect specifications and\nimplementations of monads and monad transformers. Hence, we can\nabstract over the concrete monad in HOL definitions and thus use the\nsame definition for different (combinations of) effects. We illustrate\nthe usefulness of effect polymorphism with a monadic interpreter for a\nsimple language.",
        "extra": {
            "Change history": "[2018-02-15]\nadded further specifications and implementations of non-determinism;\nmore examples\n(revision bc5399eea78e)<br>"
        },
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2017-05-11"
            }
        ],
        "theories": [
            "Monomorphic_Monad",
            "Monad_Overloading",
            "Interpreter",
            "Just_Do_It_Examples"
        ]
    },
    {
        "session": "Twelvefold_Way",
        "title": "The Twelvefold Way",
        "authors": [
            "Lukas Bulwahn"
        ],
        "topics": [
            "Mathematics/Combinatorics"
        ],
        "date": "2016-12-29",
        "abstract": "\nThis entry provides all cardinality theorems of the Twelvefold Way.\nThe Twelvefold Way systematically classifies twelve related\ncombinatorial problems concerning two finite sets, which include\ncounting permutations, combinations, multisets, set partitions and\nnumber partitions. This development builds upon the existing formal\ndevelopments with cardinality theorems for those structures. It\nprovides twelve bijections from the various structures to different\nequivalence classes on finite functions, and hence, proves cardinality\nformulae for these equivalence classes on finite functions.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-30"
            }
        ],
        "dependencies": [
            "Bell_Numbers_Spivey",
            "Card_Multisets",
            "Card_Number_Partitions",
            "Card_Partitions"
        ],
        "theories": [
            "Preliminaries",
            "Twelvefold_Way_Core",
            "Equiv_Relations_on_Functions",
            "Twelvefold_Way_Entry1",
            "Twelvefold_Way_Entry2",
            "Twelvefold_Way_Entry4",
            "Twelvefold_Way_Entry5",
            "Twelvefold_Way_Entry6",
            "Twelvefold_Way_Entry7",
            "Twelvefold_Way_Entry8",
            "Twelvefold_Way_Entry9",
            "Twelvefold_Way_Entry3",
            "Twelvefold_Way_Entry10",
            "Twelvefold_Way_Entry11",
            "Twelvefold_Way_Entry12",
            "Card_Bijections",
            "Card_Bijections_Direct",
            "Twelvefold_Way"
        ]
    },
    {
        "session": "Skew_Heap",
        "title": "Skew Heap",
        "authors": [
            "Tobias Nipkow"
        ],
        "date": "2014-08-13",
        "topics": [
            "Computer science/Data structures"
        ],
        "abstract": "\nSkew heaps are an amazingly simple and lightweight implementation of\npriority queues. They were invented by Sleator and Tarjan [SIAM 1986]\nand have logarithmic amortized complexity. This entry provides executable\nand verified functional skew heaps.\n<p>\nThe amortized complexity of skew heaps is analyzed in the AFP entry\n<a href=\"http://isa-afp.org/entries/Amortized_Complexity.html\">Amortized Complexity</a>.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-29"
            },
            {
                "2014": "2014-08-28"
            }
        ],
        "theories": [
            "Skew_Heap"
        ]
    },
    {
        "session": "Groebner_Bases",
        "title": "Gröbner Bases Theory",
        "authors": [
            "Fabian Immler",
            "Alexander Maletzky"
        ],
        "date": "2016-05-02",
        "topics": [
            "Mathematics/Algebra",
            "Computer science/Algorithms/Mathematical"
        ],
        "abstract": "\nThis formalization is concerned with the theory of Gröbner bases in\n(commutative) multivariate polynomial rings over fields, originally\ndeveloped by Buchberger in his 1965 PhD thesis. Apart from the\nstatement and proof of the main theorem of the theory, the\nformalization also implements Buchberger's algorithm for actually\ncomputing Gröbner bases as a tail-recursive function, thus allowing to\neffectively decide ideal membership in finitely generated polynomial\nideals. Furthermore, all functions can be executed on a concrete\nrepresentation of multivariate polynomials as association lists.",
        "extra": {
            "Change history": "[2019-04-18] Specialized Gröbner bases to less abstract representation of polynomials, where\npower-products are represented as polynomial mappings.<br>"
        },
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-05-02"
            }
        ],
        "dependencies": [
            "Polynomials",
            "Jordan_Normal_Form",
            "Deriving"
        ],
        "theories": [
            "General",
            "Confluence",
            "Reduction",
            "Groebner_Bases",
            "Algorithm_Schema",
            "Buchberger",
            "Benchmarks",
            "Algorithm_Schema_Impl",
            "Code_Target_Rat",
            "Buchberger_Examples",
            "More_MPoly_Type_Class",
            "Auto_Reduction",
            "Reduced_GB",
            "Reduced_GB_Examples",
            "Macaulay_Matrix",
            "F4",
            "F4_Examples",
            "Syzygy",
            "Syzygy_Examples",
            "Groebner_PM"
        ]
    },
    {
        "session": "Differential_Dynamic_Logic",
        "title": "Differential Dynamic Logic",
        "authors": [
            "Brandon Bohrer"
        ],
        "topics": [
            "Logic/General logic/Modal logic",
            "Computer science/Programming languages/Logics"
        ],
        "date": "2017-02-13",
        "abstract": "\nWe formalize differential dynamic logic, a logic for proving\nproperties of hybrid systems. The proof calculus in this formalization\nis based on the uniform substitution principle. We show it is sound\nwith respect to our denotational semantics, which provides increased\nconfidence in the correctness of the KeYmaera X theorem prover based\non this calculus. As an application, we include a proof term checker\nembedded in Isabelle/HOL with several example proofs.  Published in:\nBrandon Bohrer, Vincent Rahli, Ivana Vukotic, Marcus Völp, André\nPlatzer: Formally verified differential dynamic logic. CPP 2017.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2017-02-14"
            }
        ],
        "dependencies": [
            "Ordinary_Differential_Equations"
        ],
        "theories": [
            "Ids",
            "Lib",
            "Syntax",
            "Denotational_Semantics",
            "Axioms",
            "Frechet_Correctness",
            "Static_Semantics",
            "Coincidence",
            "Bound_Effect",
            "Differential_Axioms",
            "USubst",
            "USubst_Lemma",
            "Uniform_Renaming",
            "Pretty_Printer",
            "Proof_Checker",
            "Differential_Dynamic_Logic"
        ]
    },
    {
        "session": "Featherweight_OCL",
        "title": "Featherweight OCL: A Proposal for a Machine-Checked Formal Semantics for OCL 2.5",
        "authors": [
            "Achim D. Brucker",
            "Frédéric Tuong",
            "Burkhart Wolff"
        ],
        "date": "2014-01-16",
        "topics": [
            "Computer science/System description languages"
        ],
        "abstract": "The Unified Modeling Language (UML) is one of the few\nmodeling languages that is widely used in industry. While\nUML is mostly known as diagrammatic modeling language\n(e.g., visualizing class models), it is complemented by a\ntextual language, called Object Constraint Language\n(OCL). The current version of OCL is based on a four-valued\nlogic that turns UML into a formal language. Any type\ncomprises the elements \"invalid\" and \"null\" which are\npropagated as strict and non-strict, respectively.\nUnfortunately, the former semi-formal semantics of this\nspecification language, captured in the \"Annex A\" of the\nOCL standard, leads to different interpretations of corner\ncases. We formalize the core of OCL: denotational\ndefinitions, a logical calculus and operational rules that\nallow for the execution of OCL expressions by a mixture of\nterm rewriting and code compilation. Our formalization\nreveals several inconsistencies and contradictions in the\ncurrent version of the OCL standard. Overall, this document\nis intended to provide the basis for a machine-checked text\n\"Annex A\" of the OCL standard targeting at tool\nimplementors.",
        "extra": {
            "Change history": "[2015-10-13]\n<a href=\"https//foss.heptapod.net/isa-afp/afp-devel/-/commit/e68e1996d5d4926397c9244e786446e99ab17e63\">afp-devel@ea3b38fc54d6</a> and\n<a href=\"https//projects.brucker.ch/hol-testgen/log/trunk?rev=12148\">hol-testgen@12148</a><br>\n&nbsp;&nbsp;&nbsp;Update of Featherweight OCL including a change in the abstract.<br>\n[2014-01-16]\n<a href=\"https//foss.heptapod.net/isa-afp/afp-devel/-/commit/6217cc5b29c560f24ecc64c81047778becb69f51\">afp-devel@9091ce05cb20</a> and\n<a href=\"https//projects.brucker.ch/hol-testgen/log/trunk?rev=10241\">hol-testgen@10241</a><br>\n&nbsp;&nbsp;&nbsp;New Entry Featherweight OCL"
        },
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2014-01-16"
            }
        ],
        "theories": [
            "UML_Types",
            "UML_Logic",
            "UML_PropertyProfiles",
            "UML_Boolean",
            "UML_Void",
            "UML_Integer",
            "UML_Real",
            "UML_String",
            "UML_Pair",
            "UML_Bag",
            "UML_Set",
            "UML_Sequence",
            "UML_Library",
            "UML_State",
            "UML_Contracts",
            "UML_Tools",
            "UML_Main",
            "Analysis_UML",
            "Analysis_OCL",
            "Design_UML",
            "Design_OCL"
        ]
    },
    {
        "session": "Neumann_Morgenstern_Utility",
        "title": "Von-Neumann-Morgenstern Utility Theorem",
        "authors": [
            "Julian Parsert",
            "Cezary Kaliszyk"
        ],
        "topics": [
            "Mathematics/Games and economics"
        ],
        "license": "LGPL",
        "date": "2018-07-04",
        "abstract": "\nUtility functions form an essential part of game theory and economics.\nIn order to guarantee the existence of utility functions most of the\ntime sufficient properties are assumed in an axiomatic manner. One\nfamous and very common set of such assumptions is that of expected\nutility theory. Here, the rationality, continuity, and independence of\npreferences is assumed. The von-Neumann-Morgenstern Utility theorem\nshows that these assumptions are necessary and sufficient for an\nexpected utility function to exists. This theorem was proven by\nNeumann and Morgenstern in ``Theory of Games and Economic\nBehavior'' which is regarded as one of the most influential\nworks in game theory. The formalization includes formal definitions of\nthe underlying concepts including continuity and independence of\npreferences.",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2018-07-04"
            }
        ],
        "dependencies": [
            "First_Welfare_Theorem"
        ],
        "theories": [
            "PMF_Composition",
            "Lotteries",
            "Neumann_Morgenstern_Utility_Theorem",
            "Expected_Utility"
        ]
    },
    {
        "session": "Grothendieck_Schemes",
        "title": "Grothendieck's Schemes in Algebraic Geometry",
        "authors": [
            "Anthony Bordg",
            "Lawrence C. Paulson",
            "Wenda Li"
        ],
        "topics": [
            "Mathematics/Algebra",
            "Mathematics/Geometry"
        ],
        "date": "2021-03-29",
        "abstract": "\nWe formalize mainstream structures in algebraic geometry culminating\nin Grothendieck's schemes: presheaves of rings, sheaves of rings,\nringed spaces, locally ringed spaces, affine schemes and schemes. We\nprove that the spectrum of a ring is a locally ringed space, hence an\naffine scheme. Finally, we prove that any affine scheme is a scheme.",
        "licence": "BSD",
        "dependencies": [
            "Jacobson_Basic_Algebra"
        ],
        "theories": [
            "Set_Extras",
            "Group_Extras",
            "Topological_Space",
            "Comm_Ring",
            "Scheme"
        ]
    },
    {
        "session": "Datatype_Order_Generator",
        "title": "Generating linear orders for datatypes",
        "authors": [
            "René Thiemann"
        ],
        "date": "2012-08-07",
        "topics": [
            "Computer science/Data structures"
        ],
        "abstract": "\nWe provide a framework for registering automatic methods to derive\nclass instances of datatypes, as it is possible using Haskell's ``deriving Ord, Show, ...'' feature.\n<p>\nWe further implemented such automatic methods to derive (linear) orders or hash-functions which are\nrequired in the Isabelle Collection Framework. Moreover, for the tactic of Huffman and Krauss to show that a\ndatatype is countable, we implemented a wrapper so that this tactic becomes accessible in our framework.\n<p>\nOur formalization was performed as part of the <a href=\"http://cl-informatik.uibk.ac.at/software/ceta\">IsaFoR/CeTA</a> project.\nWith our new tactic we could completely remove\ntedious proofs for linear orders of two datatypes.\n<p>\nThis development is aimed at datatypes generated by the \"old_datatype\"\ncommand.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-03-02"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-08-07"
            }
        ],
        "dependencies": [
            "Deriving",
            "Native_Word"
        ],
        "theories": [
            "Derive_Aux",
            "files/derive_aux.ML",
            "Order_Generator",
            "files/order_generator.ML",
            "Hash_Generator",
            "files/hash_generator.ML",
            "Derive",
            "Derive_Examples"
        ]
    },
    {
        "session": "Ordinals_and_Cardinals",
        "title": "Ordinals and Cardinals",
        "authors": [
            "Andrei Popescu"
        ],
        "date": "2009-09-01",
        "topics": [
            "Logic/Set theory"
        ],
        "abstract": "We develop a basic theory of ordinals and cardinals in Isabelle/HOL, up to the point where some cardinality facts relevant for the ``working mathematician\" become available. Unlike in set theory, here we do not have at hand canonical notions of ordinal and cardinal. Therefore, here an ordinal is merely a well-order relation and a cardinal is an ordinal minim w.r.t. order embedding on its field.",
        "extra": {
            "Change history": "[2012-09-25] This entry has been discontinued because it is now part of the Isabelle distribution."
        },
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-24"
            },
            {
                "2011-1": "2011-10-11"
            },
            {
                "2011": "2011-02-11"
            },
            {
                "2009-2": "2010-07-01"
            },
            {
                "2009-1": "2009-12-12"
            },
            {
                "2009": "2009-09-09"
            },
            {
                "2009": "2009-09-07"
            }
        ],
        "theories": [
            "Cardinal_Order_Relation_discontinued"
        ]
    },
    {
        "session": "Ribbon_Proofs",
        "title": "Ribbon Proofs",
        "authors": [
            "John Wickerson"
        ],
        "date": "2013-01-19",
        "topics": [
            "Computer science/Programming languages/Logics"
        ],
        "abstract": "This document concerns the theory of ribbon proofs: a diagrammatic proof system, based on separation logic, for verifying program correctness. We include the syntax, proof rules, and soundness results for two alternative formalisations of ribbon proofs. <p> Compared to traditional proof outlines, ribbon proofs emphasise the structure of a proof, so are intelligible and pedagogical. Because they contain less redundancy than proof outlines, and allow each proof step to be checked locally, they may be more scalable. Where proof outlines are cumbersome to modify, ribbon proofs can be visually manoeuvred to yield proofs of variant programs.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-03-02"
            },
            {
                "2013": "2013-02-16"
            }
        ],
        "theories": [
            "More_Finite_Map",
            "JHelper",
            "Proofchain",
            "Ribbons_Basic",
            "Ribbons_Interfaces",
            "Ribbons_Stratified",
            "Ribbons_Graphical",
            "Ribbons_Graphical_Soundness"
        ]
    },
    {
        "session": "Finger-Trees",
        "title": "Finger Trees",
        "authors": [
            "Benedikt Nordhoff",
            "Stefan Körner",
            "Peter Lammich"
        ],
        "date": "2010-10-28",
        "topics": [
            "Computer science/Data structures"
        ],
        "abstract": "\nWe implement and prove correct 2-3 finger trees.\nFinger trees are a general purpose data structure, that can be used to\nefficiently implement other data structures, such as priority queues.\nIntuitively, a finger tree is an annotated sequence, where the annotations are\nelements of a monoid. Apart from operations to access the ends of the sequence,\nthe main operation is to split the sequence at the point where a\n<em>monotone predicate</em> over the sum of the left part of the sequence\nbecomes true for the first time.\nThe implementation follows the paper of Hinze and Paterson.\nThe code generator can be used to get efficient, verified code.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-03-02"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-24"
            },
            {
                "2011-1": "2011-10-11"
            },
            {
                "2011": "2011-02-11"
            },
            {
                "2009-2": "2010-10-28"
            }
        ],
        "theories": [
            "FingerTree",
            "Test"
        ]
    },
    {
        "session": "Dynamic_Tables",
        "title": "Parameterized Dynamic Tables",
        "authors": [
            "Tobias Nipkow"
        ],
        "date": "2015-06-07",
        "topics": [
            "Computer science/Data structures"
        ],
        "abstract": "\nThis article formalizes the amortized analysis of dynamic tables\nparameterized with their minimal and maximal load factors and the\nexpansion and contraction factors.\n<P>\nA full description is found in a\n<a href=\"http://www21.in.tum.de/~nipkow/pubs\">companion paper</a>.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-06-08"
            },
            {
                "2015": "2015-06-07"
            }
        ],
        "dependencies": [
            "Amortized_Complexity"
        ],
        "theories": [
            "Tables_real",
            "Tables_nat"
        ]
    },
    {
        "session": "Matrices_for_ODEs",
        "title": "Matrices for ODEs",
        "authors": [
            "Jonathan Julian Huerta y Munive"
        ],
        "topics": [
            "Mathematics/Analysis",
            "Mathematics/Algebra"
        ],
        "date": "2020-04-19",
        "abstract": "\nOur theories formalise various matrix properties that serve to\nestablish existence, uniqueness and characterisation of the solution\nto affine systems of ordinary differential equations (ODEs). In\nparticular, we formalise the operator and maximum norm of matrices.\nThen we use them to prove that square matrices form a Banach space,\nand in this setting, we show an instance of Picard-Lindelöf’s\ntheorem for affine systems of ODEs. Finally, we use this formalisation\nto verify three simple hybrid programs.",
        "licence": "BSD",
        "dependencies": [
            "Hybrid_Systems_VCs"
        ],
        "theories": [
            "MTX_Preliminaries",
            "MTX_Norms",
            "SQ_MTX",
            "MTX_Flows",
            "MTX_Examples"
        ]
    },
    {
        "session": "Probabilistic_Timed_Automata",
        "title": "Probabilistic Timed Automata",
        "authors": [
            "Simon Wimmer",
            "Johannes Hölzl"
        ],
        "topics": [
            "Mathematics/Probability theory",
            "Computer science/Automata and formal languages"
        ],
        "date": "2018-05-24",
        "abstract": "\nWe present a formalization of probabilistic timed automata (PTA) for\nwhich we try to follow the formula MDP + TA = PTA as far as possible:\nour work starts from our existing formalizations of Markov decision\nprocesses (MDP) and timed automata (TA) and combines them modularly.\nWe prove the fundamental result for probabilistic timed automata: the\nregion construction that is known from timed automata carries over to\nthe probabilistic setting. In particular, this allows us to prove that\nminimum and maximum reachability probabilities can be computed via a\nreduction to MDP model checking, including the case where one wants to\ndisregard unrealizable behavior. Further information can be found in\nour ITP paper [2].",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2018-05-25"
            }
        ],
        "dependencies": [
            "Markov_Models",
            "Timed_Automata"
        ],
        "theories": [
            "MDP_Aux",
            "Finiteness",
            "Basic",
            "Sequence",
            "Sequence_LTL",
            "Instantiate_Existentials",
            "More_List",
            "Stream_More",
            "Graphs",
            "Lib",
            "PTA",
            "PTA_Reachability"
        ]
    },
    {
        "session": "Chandy_Lamport",
        "title": "A Formal Proof of The Chandy--Lamport Distributed Snapshot Algorithm",
        "authors": [
            "Ben Fiedler",
            "Dmitriy Traytel"
        ],
        "topics": [
            "Computer science/Algorithms/Distributed"
        ],
        "date": "2020-07-21",
        "abstract": "\nWe provide a suitable distributed system model and implementation of the\nChandy--Lamport distributed snapshot algorithm [ACM Transactions on\nComputer Systems, 3, 63-75, 1985]. Our main result is a formal\ntermination and correctness proof of the Chandy--Lamport algorithm and\nits use in stable property detection.",
        "licence": "BSD",
        "theories": [
            "Distributed_System",
            "Trace",
            "Util",
            "Swap",
            "Snapshot",
            "Co_Snapshot",
            "Example"
        ]
    },
    {
        "session": "GenClock",
        "title": "Formalization of a Generalized Protocol for Clock Synchronization",
        "authors": [
            "Alwen Tiu"
        ],
        "date": "2005-06-24",
        "topics": [
            "Computer science/Algorithms/Distributed"
        ],
        "abstract": "We formalize the generalized Byzantine fault-tolerant clock synchronization protocol of Schneider. This protocol abstracts from particular algorithms or implementations for clock synchronization. This abstraction includes several assumptions on the behaviors of physical clocks and on general properties of concrete algorithms/implementations. Based on these assumptions the correctness of the protocol is proved by Schneider. His proof was later verified by Shankar using the theorem prover EHDM (precursor to PVS). Our formalization in Isabelle/HOL is based on Shankar's formalization.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-24"
            },
            {
                "2011-1": "2011-10-11"
            },
            {
                "2011": "2011-02-11"
            },
            {
                "2009-2": "2010-07-01"
            },
            {
                "2009-1": "2009-12-12"
            },
            {
                "2009": "2009-04-29"
            },
            {
                "2008": "2008-06-10"
            },
            {
                "2007": "2007-11-27"
            },
            {
                "2005": "2005-10-14"
            },
            {
                "2004": "2005-06-24"
            }
        ],
        "theories": [
            "GenClock"
        ]
    },
    {
        "session": "Password_Authentication_Protocol",
        "title": "Verification of a Diffie-Hellman Password-based Authentication Protocol by Extending the Inductive Method",
        "authors": [
            "Pasquale Noce"
        ],
        "topics": [
            "Computer science/Security"
        ],
        "date": "2017-01-03",
        "abstract": "\nThis paper constructs a formal model of a Diffie-Hellman\npassword-based authentication protocol between a user and a smart\ncard, and proves its security. The protocol provides for the dispatch\nof the user's password to the smart card on a secure messaging\nchannel established by means of Password Authenticated Connection\nEstablishment (PACE), where the mapping method being used is Chip\nAuthentication Mapping. By applying and suitably extending\nPaulson's Inductive Method, this paper proves that the protocol\nestablishes trustworthy secure messaging channels, preserves the\nsecrecy of users' passwords, and provides an effective mutual\nauthentication service. What is more, these security properties turn\nout to hold independently of the secrecy of the PACE authentication\nkey.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2017-01-06"
            }
        ],
        "theories": [
            "Propaedeutics",
            "Protocol"
        ]
    },
    {
        "session": "Stone_Kleene_Relation_Algebras",
        "title": "Stone-Kleene Relation Algebras",
        "authors": [
            "Walter Guttmann"
        ],
        "topics": [
            "Mathematics/Algebra"
        ],
        "date": "2017-07-06",
        "abstract": "\nWe develop Stone-Kleene relation algebras, which expand Stone relation\nalgebras with a Kleene star operation to describe reachability in\nweighted graphs. Many properties of the Kleene star arise as a special\ncase of a more general theory of iteration based on Conway semirings\nextended by simulation axioms. This includes several theorems\nrepresenting complex program transformations. We formally prove the\ncorrectness of Conway's automata-based construction of the Kleene\nstar of a matrix. We prove numerous results useful for reasoning about\nweighted graphs.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            }
        ],
        "dependencies": [
            "Stone_Relation_Algebras"
        ],
        "theories": [
            "Iterings",
            "Kleene_Algebras",
            "Kleene_Relation_Algebras",
            "Kleene_Relation_Subalgebras",
            "Matrix_Kleene_Algebras"
        ]
    },
    {
        "session": "Card_Equiv_Relations",
        "title": "Cardinality of Equivalence Relations",
        "authors": [
            "Lukas Bulwahn"
        ],
        "date": "2016-05-24",
        "topics": [
            "Mathematics/Combinatorics"
        ],
        "abstract": "\nThis entry provides formulae for counting the number of equivalence\nrelations and partial equivalence relations over a finite carrier set\nwith given cardinality.  To count the number of equivalence relations,\nwe provide bijections between equivalence relations and set\npartitions, and then transfer the main results of the two AFP entries,\nCardinality of Set Partitions and Spivey's Generalized Recurrence for\nBell Numbers, to theorems on equivalence relations. To count the\nnumber of partial equivalence relations, we observe that counting\npartial equivalence relations over a set A is equivalent to counting\nall equivalence relations over all subsets of the set A. From this\nobservation and the results on equivalence relations, we show that the\ncardinality of partial equivalence relations over a finite set of\ncardinality n is equal to the n+1-th Bell number.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-05-24"
            }
        ],
        "dependencies": [
            "Bell_Numbers_Spivey"
        ],
        "theories": [
            "Card_Equiv_Relations",
            "Card_Partial_Equiv_Relations"
        ]
    },
    {
        "session": "Matroids",
        "title": "Matroids",
        "authors": [
            "Jonas Keinholz"
        ],
        "topics": [
            "Mathematics/Combinatorics"
        ],
        "date": "2018-11-16",
        "abstract": "\n<p>This article defines the combinatorial structures known as\n<em>Independence Systems</em> and\n<em>Matroids</em> and provides basic concepts and theorems\nrelated to them. These structures play an important role in\ncombinatorial optimisation, e. g. greedy algorithms such as\nKruskal's algorithm. The development is based on Oxley's\n<a href=\"http://www.math.lsu.edu/~oxley/survey4.pdf\">`What\nis a Matroid?'</a>.</p>",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-11-20"
            }
        ],
        "theories": [
            "Indep_System",
            "Matroid"
        ]
    },
    {
        "session": "Clean",
        "title": "Clean - An Abstract Imperative Programming Language and its Theory",
        "authors": [
            "Frédéric Tuong",
            "Burkhart Wolff"
        ],
        "topics": [
            "Computer science/Programming languages",
            "Computer science/Semantics"
        ],
        "date": "2019-10-04",
        "abstract": "\nClean is based on a simple, abstract execution model for an imperative\ntarget language. “Abstract” is understood in contrast to “Concrete\nSemantics”; alternatively, the term “shallow-style embedding” could be\nused. It strives for a type-safe notion of program-variables, an\nincremental construction of the typed state-space, support of\nincremental verification, and open-world extensibility of new type\ndefinitions being intertwined with the program definitions. Clean is\nbased on a “no-frills” state-exception monad with the usual\ndefinitions of bind and unit for the compositional glue of state-based\ncomputations. Clean offers conditionals and loops supporting C-like\ncontrol-flow operators such as break and return. The state-space\nconstruction is based on the extensible record package. Direct\nrecursion of procedures is supported. Clean’s design strives for\nextreme simplicity. It is geared towards symbolic execution and proven\ncorrect verification tools. The underlying libraries of this package,\nhowever, deliberately restrict themselves to the most elementary\ninfrastructure for these tasks. The package is intended to serve as\ndemonstrator semantic backend for Isabelle/C, or for the\ntest-generation techniques.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-10-16"
            }
        ],
        "theories": [
            "MonadSE",
            "Seq_MonadSE",
            "Symbex_MonadSE",
            "Clean",
            "Hoare_MonadSE",
            "Hoare_Clean",
            "Clean_Symbex",
            "Test_Clean",
            "Clean_Main",
            "Quicksort_concept",
            "SquareRoot_concept"
        ]
    },
    {
        "session": "Mersenne_Primes",
        "title": "Mersenne primes and the Lucas–Lehmer test",
        "authors": [
            "Manuel Eberl"
        ],
        "topics": [
            "Mathematics/Number theory"
        ],
        "date": "2020-01-17",
        "abstract": "\n<p>This article provides formal proofs of basic properties of\nMersenne numbers, i. e. numbers of the form\n2<sup><em>n</em></sup> - 1, and especially of\nMersenne primes.</p> <p>In particular, an efficient,\nverified, and executable version of the Lucas&ndash;Lehmer test is\ndeveloped. This test decides primality for Mersenne numbers in time\npolynomial in <em>n</em>.</p>",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2020-01-20"
            }
        ],
        "dependencies": [
            "Pell",
            "Native_Word",
            "Probabilistic_Prime_Tests"
        ],
        "theories": [
            "Lucas_Lehmer_Auxiliary",
            "Lucas_Lehmer",
            "Lucas_Lehmer_Code"
        ]
    },
    {
        "session": "Echelon_Form",
        "title": "Echelon Form",
        "authors": [
            "Jose Divasón",
            "Jesús Aransay"
        ],
        "topics": [
            "Computer science/Algorithms/Mathematical",
            "Mathematics/Algebra"
        ],
        "date": "2015-02-12",
        "abstract": "We formalize an algorithm to compute the Echelon Form of a matrix. We have proved its existence over Bézout domains and made it executable over Euclidean domains, such as the integer ring and the univariate polynomials over a field. This allows us to compute determinants, inverses and characteristic polynomials of matrices. The work is based on the HOL-Multivariate Analysis library, and on both the Gauss-Jordan and Cayley-Hamilton AFP entries. As a by-product, some algebraic structures have been implemented (principal ideal domains, Bézout domains...). The algorithm has been refined to immutable arrays and code can be generated to functional languages as well.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2015-02-12"
            }
        ],
        "dependencies": [
            "Cayley_Hamilton",
            "Gauss_Jordan",
            "Rank_Nullity_Theorem"
        ],
        "theories": [
            "Rings2",
            "Cayley_Hamilton_Compatible",
            "Code_Cayley_Hamilton",
            "Echelon_Form",
            "Echelon_Form_Det",
            "Echelon_Form_Inverse",
            "Examples_Echelon_Form_Abstract",
            "Echelon_Form_IArrays",
            "Echelon_Form_Det_IArrays",
            "Code_Cayley_Hamilton_IArrays",
            "Echelon_Form_Inverse_IArrays",
            "Examples_Echelon_Form_IArrays"
        ]
    },
    {
        "session": "Liouville_Numbers",
        "title": "Liouville numbers",
        "authors": [
            "Manuel Eberl"
        ],
        "date": "2015-12-28",
        "topics": [
            "Mathematics/Analysis",
            "Mathematics/Number theory"
        ],
        "abstract": "\n<p>\nLiouville numbers are a class of transcendental numbers that can be approximated\nparticularly well with rational numbers. Historically, they were the first\nnumbers whose transcendence was proven.\n</p><p>\nIn this entry, we define the concept of Liouville numbers as well as the\nstandard construction to obtain Liouville numbers (including Liouville's\nconstant) and we prove their most important properties: irrationality and\ntranscendence.\n</p><p>\nThe proof is very elementary and requires only standard arithmetic, the Mean\nValue Theorem for polynomials, and the boundedness of polynomials on compact\nintervals.\n</p>",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2016-01-05"
            }
        ],
        "theories": [
            "Liouville_Numbers_Misc",
            "Liouville_Numbers"
        ]
    },
    {
        "session": "Lambert_W",
        "title": "The Lambert W Function on the Reals",
        "authors": [
            "Manuel Eberl"
        ],
        "topics": [
            "Mathematics/Analysis"
        ],
        "date": "2020-04-24",
        "abstract": "\n<p>The Lambert <em>W</em> function is a multi-valued\nfunction defined as the inverse function of <em>x</em>\n&#x21A6; <em>x</em>\ne<sup><em>x</em></sup>. Besides numerous\napplications in combinatorics, physics, and engineering, it also\nfrequently occurs when solving equations containing both\ne<sup><em>x</em></sup> and\n<em>x</em>, or both <em>x</em> and log\n<em>x</em>.</p> <p>This article provides a\ndefinition of the two real-valued branches\n<em>W</em><sub>0</sub>(<em>x</em>)\nand\n<em>W</em><sub>-1</sub>(<em>x</em>)\nand proves various properties such as basic identities and\ninequalities, monotonicity, differentiability, asymptotic expansions,\nand the MacLaurin series of\n<em>W</em><sub>0</sub>(<em>x</em>)\nat <em>x</em> = 0.</p>",
        "licence": "BSD",
        "dependencies": [
            "Bernoulli",
            "Stirling_Formula"
        ],
        "theories": [
            "Lambert_W",
            "Lambert_W_MacLaurin_Series"
        ]
    },
    {
        "session": "WebAssembly",
        "title": "WebAssembly",
        "authors": [
            "Conrad Watt"
        ],
        "topics": [
            "Computer science/Programming languages/Language definitions"
        ],
        "date": "2018-04-29",
        "abstract": "\nThis is a mechanised specification of the WebAssembly language, drawn\nmainly from the previously published paper formalisation of Haas et\nal. Also included is a full proof of soundness of the type system,\ntogether with a verified type checker and interpreter. We include only\na partial procedure for the extraction of the type checker and\ninterpreter here. For more details, please see our paper in CPP 2018.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2018-04-30"
            },
            {
                "2017": "2018-04-29"
            }
        ],
        "dependencies": [
            "Native_Word"
        ],
        "theories": [
            "Wasm_Ast",
            "Wasm_Type_Abs",
            "Wasm_Base_Defs",
            "Wasm",
            "Wasm_Axioms",
            "Wasm_Properties_Aux",
            "Wasm_Properties",
            "Wasm_Soundness",
            "Wasm_Checker_Types",
            "Wasm_Checker",
            "Wasm_Checker_Properties",
            "Wasm_Interpreter",
            "Wasm_Interpreter_Properties",
            "Wasm_Checker_Printing",
            "Wasm_Interpreter_Printing",
            "Wasm_Type_Abs_Printing",
            "Wasm_Printing",
            "Wasm_Interpreter_Printing_Pure"
        ]
    },
    {
        "session": "CAVA_LTL_Modelchecker",
        "title": "A Fully Verified Executable LTL Model Checker",
        "authors": [
            "Javier Esparza",
            "Peter Lammich",
            "René Neumann",
            "Tobias Nipkow",
            "Alexander Schimpf",
            "Jan-Georg Smaus"
        ],
        "date": "2014-05-28",
        "topics": [
            "Computer science/Automata and formal languages"
        ],
        "abstract": "\nWe present an LTL model checker whose code has been completely verified\nusing the Isabelle theorem prover. The checker consists of over 4000\nlines of ML code. The code is produced using the Isabelle Refinement\nFramework, which allows us to split its correctness proof into (1) the\nproof of an abstract version of the checker, consisting of a few hundred\nlines of ``formalized pseudocode'', and (2) a verified refinement step\nin which mathematical sets and other abstract structures are replaced by\nimplementations of efficient structures like red-black trees and\nfunctional arrays. This leads to a checker that,\nwhile still slower than unverified checkers, can already be used as a\ntrusted reference implementation against which advanced implementations\ncan be tested.\n<p>\nAn early version of this model checker is described in the\n<a href=\"http://www21.in.tum.de/~nipkow/pubs/cav13.html\">CAV 2013 paper</a>\nwith the same title.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2014-05-30"
            },
            {
                "2013-2": "2014-05-29"
            }
        ],
        "dependencies": [
            "CAVA_Setup"
        ],
        "theories": [
            "NDFS_SI_Statistics",
            "NDFS_SI",
            "CAVA_Abstract",
            "BoolProgs",
            "BoolProgs_Extras",
            "BoolProgs_LTL_Conv",
            "BoolProgs_Philosophers",
            "BoolProgs_ReaderWriter",
            "BoolProgs_Simple",
            "BoolProgs_LeaderFilters",
            "BoolProgs_Programs",
            "CAVA_Impl",
            "Mulog",
            "All_Of_Nested_DFS",
            "All_Of_CAVA_LTL_Modelchecker"
        ]
    },
    {
        "session": "Graph_Saturation",
        "title": "Graph Saturation",
        "authors": [
            "Sebastiaan J. C. Joosten"
        ],
        "topics": [
            "Logic/Rewriting",
            "Mathematics/Graph theory"
        ],
        "date": "2018-11-23",
        "abstract": "\nThis is an Isabelle/HOL formalisation of graph saturation, closely\nfollowing a <a href=\"https://doi.org/10.1016/j.jlamp.2018.06.005\">paper by the author</a> on graph saturation.\nNine out of ten lemmas of the original paper are proven in this\nformalisation. The formalisation additionally includes two theorems\nthat show the main premise of the paper: that consistency and\nentailment are decided through graph saturation. This formalisation\ndoes not give executable code, and it did not implement any of the\noptimisations suggested in the paper.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-11-28"
            }
        ],
        "theories": [
            "MissingRelation",
            "LabeledGraphs",
            "RulesAndChains",
            "GraphRewriting",
            "LabeledGraphSemantics",
            "StandardModels",
            "RuleSemanticsConnection",
            "StandardRules",
            "CombinedCorrectness"
        ]
    },
    {
        "session": "Gauss_Jordan",
        "title": "Gauss-Jordan Algorithm and Its Applications",
        "authors": [
            "Jose Divasón",
            "Jesús Aransay"
        ],
        "topics": [
            "Computer science/Algorithms/Mathematical"
        ],
        "date": "2014-09-03",
        "abstract": "The Gauss-Jordan algorithm states that any matrix over a field can be transformed by means of elementary row operations to a matrix in reduced row echelon form. The formalization is based on the Rank Nullity Theorem entry of the AFP and on the HOL-Multivariate-Analysis session of Isabelle, where matrices are represented as functions over finite types. We have set up the code generator to make this representation executable. In order to improve the performance, a refinement to immutable arrays has been carried out. We have formalized some of the applications of the Gauss-Jordan algorithm. Thanks to this development, the following facts can be computed over matrices whose elements belong to a field: Ranks, Determinants, Inverses, Bases and dimensions and Solutions of systems of linear equations. Code can be exported to SML and Haskell.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-09-03"
            }
        ],
        "dependencies": [
            "Rank_Nullity_Theorem"
        ],
        "theories": [
            "Rref",
            "Code_Set",
            "Code_Matrix",
            "Elementary_Operations",
            "Rank",
            "Gauss_Jordan",
            "Linear_Maps",
            "Gauss_Jordan_PA",
            "Determinants2",
            "Inverse",
            "Bases_Of_Fundamental_Subspaces",
            "System_Of_Equations",
            "Code_Z2",
            "Examples_Gauss_Jordan_Abstract",
            "IArray_Addenda",
            "Matrix_To_IArray",
            "Gauss_Jordan_IArrays",
            "Gauss_Jordan_PA_IArrays",
            "Bases_Of_Fundamental_Subspaces_IArrays",
            "System_Of_Equations_IArrays",
            "Determinants_IArrays",
            "Inverse_IArrays",
            "Examples_Gauss_Jordan_IArrays",
            "Code_Generation_IArrays",
            "Code_Generation_IArrays_SML",
            "Code_Real_Approx_By_Float_Haskell",
            "Code_Rational",
            "Code_Generation_IArrays_Haskell"
        ]
    },
    {
        "session": "Orbit_Stabiliser",
        "title": "Orbit-Stabiliser Theorem with Application to Rotational Symmetries",
        "authors": [
            "Jonas Rädle"
        ],
        "topics": [
            "Mathematics/Algebra"
        ],
        "date": "2017-08-20",
        "abstract": "\nThe Orbit-Stabiliser theorem is a basic result in the algebra of\ngroups that factors the order of a group into the sizes of its orbits\nand stabilisers.  We formalize the notion of a group action and the\nrelated concepts of orbits and stabilisers. This allows us to prove\nthe orbit-stabiliser theorem.  In the second part of this work, we\nformalize the tetrahedral group and use the orbit-stabiliser theorem\nto prove that there are twelve (orientation-preserving) rotations of\nthe tetrahedron.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2017-08-23"
            }
        ],
        "theories": [
            "Left_Coset",
            "Orbit_Stabiliser",
            "Tetrahedron"
        ]
    },
    {
        "session": "Partial_Function_MR",
        "title": "Mutually Recursive Partial Functions",
        "authors": [
            "René Thiemann"
        ],
        "topics": [
            "Computer science/Functional programming"
        ],
        "date": "2014-02-18",
        "license": "LGPL",
        "abstract": "We provide a wrapper around the partial-function command that supports mutual recursion.",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2014-02-19"
            }
        ],
        "theories": [
            "Partial_Function_MR",
            "files/partial_function_mr.ML",
            "Partial_Function_MR_Examples"
        ]
    },
    {
        "session": "Card_Multisets",
        "title": "Cardinality of Multisets",
        "authors": [
            "Lukas Bulwahn"
        ],
        "date": "2016-06-26",
        "topics": [
            "Mathematics/Combinatorics"
        ],
        "abstract": "\n<p>This entry provides three lemmas to count the number of multisets\nof a given size and finite carrier set. The first lemma provides a\ncardinality formula assuming that the multiset's elements are chosen\nfrom the given carrier set. The latter two lemmas provide formulas\nassuming that the multiset's elements also cover the given carrier\nset, i.e., each element of the carrier set occurs in the multiset at\nleast once.</p>  <p>The proof of the first lemma uses the argument of\nthe recurrence relation for counting multisets. The proof of the\nsecond lemma is straightforward, and the proof of the third lemma is\neasily obtained using the first cardinality lemma. A challenge for the\nformalization is the derivation of the required induction rule, which\nis a special combination of the induction rules for finite sets and\nnatural numbers. The induction rule is derived by defining a suitable\ninductive predicate and transforming the predicate's induction\nrule.</p>",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-06-26"
            }
        ],
        "theories": [
            "Card_Multisets"
        ]
    },
    {
        "session": "Lifting_Definition_Option",
        "title": "Lifting Definition Option",
        "authors": [
            "René Thiemann"
        ],
        "topics": [
            "Computer science/Functional programming"
        ],
        "date": "2014-10-13",
        "license": "LGPL",
        "abstract": "\nWe implemented a command that can be used to easily generate\nelements of a restricted type <tt>{x :: 'a. P x}</tt>,\nprovided the definition is of the form\n<tt>f ys = (if check ys then Some(generate ys :: 'a) else None)</tt> where\n<tt>ys</tt> is a list of variables <tt>y1 ... yn</tt> and\n<tt>check ys ==> P(generate ys)</tt> can be proved.\n<p>\nIn principle, such a definition is also directly possible using the\n<tt>lift_definition</tt> command. However, then this definition will not be\nsuitable for code-generation. To this end, we automated a more complex\nconstruction of Joachim Breitner which is amenable for code-generation, and\nwhere the test <tt>check ys</tt> will only be performed once.  In the\nautomation, one auxiliary type is created, and Isabelle's lifting- and\ntransfer-package is invoked several times.",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-10-15"
            }
        ],
        "theories": [
            "Lifting_Definition_Option_Examples"
        ]
    },
    {
        "session": "Completeness",
        "title": "Completeness theorem",
        "authors": [
            "James Margetson",
            "Tom Ridge"
        ],
        "date": "2004-09-20",
        "topics": [
            "Logic/Proof theory"
        ],
        "abstract": "The completeness of first-order logic is proved, following the first five pages of Wainer and Wallen's chapter of the book <i>Proof Theory</i> by Aczel et al., CUP, 1992. Their presentation of formulas allows the proofs to use symmetry arguments. Margetson formalized this theorem by early 2000. The Isar conversion is thanks to Tom Ridge. A paper describing the formalization is available <a href=\"Completeness-paper.pdf\">[pdf]</a>.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-03-02"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-24"
            },
            {
                "2011-1": "2011-10-11"
            },
            {
                "2011": "2011-02-11"
            },
            {
                "2009-2": "2010-06-30"
            },
            {
                "2009-1": "2009-12-12"
            },
            {
                "2009": "2009-04-29"
            },
            {
                "2008": "2008-06-10"
            },
            {
                "2007": "2007-11-27"
            },
            {
                "2005": "2005-10-14"
            },
            {
                "2004": "2005-07-22"
            },
            {
                "2004": "2004-09-21"
            },
            {
                "2004": "2004-09-20"
            }
        ],
        "theories": [
            "PermutationLemmas",
            "Base",
            "Formula",
            "Sequents",
            "Tree",
            "Completeness",
            "Soundness"
        ]
    },
    {
        "session": "LTL_to_DRA",
        "title": "Converting Linear Temporal Logic to Deterministic (Generalized) Rabin Automata",
        "topics": [
            "Computer science/Automata and formal languages"
        ],
        "authors": [
            "Salomon Sickert"
        ],
        "date": "2015-09-04",
        "abstract": "Recently, Javier Esparza and Jan Kretinsky proposed a new method directly translating linear temporal logic (LTL) formulas to deterministic (generalized) Rabin automata. Compared to the existing approaches of constructing a non-deterministic Buechi-automaton in the first step and then applying a determinization procedure (e.g. some variant of Safra's construction) in a second step, this new approach preservers a relation between the formula and the states of the resulting automaton. While the old approach produced a monolithic structure, the new method is compositional. Furthermore, in some cases the resulting automata are much smaller than the automata generated by existing approaches. In order to ensure the correctness of the construction, this entry contains a complete formalisation and verification of the translation. Furthermore from this basis executable code is generated.",
        "extra": {
            "Change history": "[2015-09-23] Enable code export for the eager unfolding optimisation and reduce running time of the generated tool. Moreover, add support for the mlton SML compiler.<br>\n[2016-03-24] Make use of the LTL entry and include the simplifier."
        },
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-09-04"
            }
        ],
        "dependencies": [
            "LTL",
            "List-Index",
            "Boolean_Expression_Checkers",
            "KBPs"
        ],
        "theories": [
            "Preliminaries2",
            "Map2",
            "Mapping2",
            "DTS",
            "Semi_Mojmir",
            "Mojmir",
            "Rabin",
            "List2",
            "Mojmir_Rabin",
            "LTL_FGXU",
            "af",
            "Logical_Characterization",
            "LTL_Rabin",
            "LTL_Rabin_Unfold_Opt",
            "LTL_Compat",
            "LTL_Impl",
            "af_Impl",
            "Mojmir_Rabin_Impl",
            "LTL_Rabin_Impl",
            "Export_Code"
        ]
    },
    {
        "session": "Sort_Encodings",
        "title": "Sound and Complete Sort Encodings for First-Order Logic",
        "authors": [
            "Jasmin Christian Blanchette",
            "Andrei Popescu"
        ],
        "date": "2013-06-27",
        "topics": [
            "Logic/General logic/Mechanization of proofs"
        ],
        "abstract": "\nThis is a formalization of the soundness and completeness properties\nfor various efficient encodings of sorts in unsorted first-order logic\nused by Isabelle's Sledgehammer tool.\n<p>\nEssentially, the encodings proceed as follows:\na many-sorted problem is decorated with (as few as possible) tags or\nguards that make the problem monotonic; then sorts can be soundly\nerased.\n<p>\nThe development employs a formalization of many-sorted first-order logic\nin clausal form (clauses, structures and the basic properties\nof the satisfaction relation), which could be of interest as the starting\npoint for other formalizations of first-order logic metatheory.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-07-04"
            },
            {
                "2013": "2013-07-01"
            }
        ],
        "theories": [
            "Preliminaries",
            "TermsAndClauses",
            "Sig",
            "M",
            "CM",
            "Mono",
            "Mcalc",
            "T_G_Prelim",
            "Mcalc2",
            "Mcalc2C",
            "G",
            "T",
            "U",
            "CU",
            "E",
            "Encodings"
        ]
    },
    {
        "session": "Network_Security_Policy_Verification",
        "title": "Network Security Policy Verification",
        "authors": [
            "Cornelius Diekmann"
        ],
        "date": "2014-07-04",
        "topics": [
            "Computer science/Security"
        ],
        "abstract": "\nWe present a unified theory for verifying network security policies.\nA security policy is represented as directed graph.\nTo check high-level security goals, security invariants over the policy are\nexpressed. We cover monotonic security invariants, i.e. prohibiting more does not harm\nsecurity. We provide the following contributions for the security invariant theory.\n<ul>\n<li>Secure auto-completion of scenario-specific knowledge, which eases usability.</li>\n<li>Security violations can be repaired by tightening the policy iff the\nsecurity invariants hold for the deny-all policy.</li>\n<li>An algorithm to compute a security policy.</li>\n<li>A formalization of stateful connection semantics in network security mechanisms.</li>\n<li>An algorithm to compute a secure stateful implementation of a policy.</li>\n<li>An executable implementation of all the theory.</li>\n<li>Examples, ranging from an aircraft cabin data network to the analysis\nof a large real-world firewall.</li>\n<li>More examples: A fully automated translation of high-level security goals to both\nfirewall and SDN configurations (see Examples/Distributed_WebApp.thy).</li>\n</ul>\nFor a detailed description, see\n<ul>\n<li>C. Diekmann, A. Korsten, and G. Carle.\n<a href=\"http://www.net.in.tum.de/fileadmin/bibtex/publications/papers/diekmann2015mansdnnfv.pdf\">Demonstrating\ntopoS: Theorem-prover-based synthesis of secure network configurations.</a>\nIn 2nd International Workshop on Management of SDN and NFV Systems, manSDN/NFV, Barcelona, Spain, November 2015.</li>\n<li>C. Diekmann, S.-A. Posselt, H. Niedermayer, H. Kinkelin, O. Hanka, and G. Carle.\n<a href=\"http://www.net.in.tum.de/pub/diekmann/forte14.pdf\">Verifying Security Policies using Host Attributes.</a>\nIn FORTE, 34th IFIP International Conference on Formal Techniques for Distributed Objects,\nComponents and Systems, Berlin, Germany, June 2014.</li>\n<li>C. Diekmann, L. Hupel, and G. Carle. Directed Security Policies:\n<a href=\"http://rvg.web.cse.unsw.edu.au/eptcs/paper.cgi?ESSS2014.3\">A Stateful Network Implementation.</a>\nIn J. Pang and Y. Liu, editors, Engineering Safety and Security Systems,\nvolume 150 of Electronic Proceedings in Theoretical Computer Science,\npages 20-34, Singapore, May 2014. Open Publishing Association.</li>\n</ul>",
        "extra": {
            "Change history": "[2015-04-14]\nAdded Distributed WebApp example and improved graphviz visualization\n(revision 4dde08ca2ab8)<br>"
        },
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2014-07-09"
            }
        ],
        "dependencies": [
            "Transitive-Closure",
            "Automatic_Refinement"
        ],
        "theories": [
            "ML_GraphViz",
            "ML_GraphViz_Disable",
            "FiniteGraph",
            "FiniteListGraph",
            "TopoS_Util",
            "Efficient_Distinct",
            "FiniteListGraph_Impl",
            "TopoS_Vertices",
            "TopoS_Interface",
            "TopoS_withOffendingFlows",
            "TopoS_ENF",
            "vertex_example_simps",
            "TopoS_Helper",
            "SINVAR_Subnets2",
            "SINVAR_BLPstrict",
            "SINVAR_Tainting",
            "SINVAR_BLPbasic",
            "SINVAR_TaintingTrusted",
            "SINVAR_BLPtrusted",
            "Analysis_Tainting",
            "TopoS_Interface_impl",
            "SINVAR_BLPbasic_impl",
            "SINVAR_Subnets",
            "SINVAR_Subnets_impl",
            "SINVAR_DomainHierarchyNG",
            "SINVAR_DomainHierarchyNG_impl",
            "SINVAR_BLPtrusted_impl",
            "SINVAR_SecGwExt",
            "SINVAR_SecGwExt_impl",
            "SINVAR_Sink",
            "SINVAR_Sink_impl",
            "SINVAR_SubnetsInGW",
            "SINVAR_SubnetsInGW_impl",
            "SINVAR_CommunicationPartners",
            "SINVAR_CommunicationPartners_impl",
            "SINVAR_NoRefl",
            "SINVAR_NoRefl_impl",
            "SINVAR_Tainting_impl",
            "SINVAR_TaintingTrusted_impl",
            "SINVAR_Dependability",
            "SINVAR_Dependability_impl",
            "SINVAR_NonInterference",
            "SINVAR_NonInterference_impl",
            "SINVAR_ACLcommunicateWith",
            "SINVAR_ACLnotCommunicateWith",
            "SINVAR_ACLnotCommunicateWith_impl",
            "SINVAR_ACLcommunicateWith_impl",
            "SINVAR_Dependability_norefl",
            "SINVAR_Dependability_norefl_impl",
            "TopoS_Library",
            "TopoS_Composition_Theory",
            "TopoS_Stateful_Policy",
            "TopoS_Composition_Theory_impl",
            "TopoS_Stateful_Policy_Algorithm",
            "TopoS_Stateful_Policy_impl",
            "METASINVAR_SystemBoundary",
            "TopoS_Impl",
            "Network_Security_Policy_Verification",
            "Example_BLP",
            "TopoS_generateCode",
            "attic",
            "Impl_List_Playground_ChairNetwork",
            "Impl_List_Playground_statefulpolicycompliance",
            "Example",
            "Example_NetModel",
            "Example_Forte14",
            "Distributed_WebApp",
            "I8_SSH_Landscape",
            "Impl_List_Playground",
            "Impl_List_Playground_ChairNetwork_statefulpolicy_example",
            "CryptoDB",
            "IDEM",
            "MeasrDroid",
            "SINVAR_Examples",
            "Imaginary_Factory_Network"
        ]
    },
    {
        "session": "KD_Tree",
        "title": "Multidimensional Binary Search Trees",
        "authors": [
            "Martin Rau"
        ],
        "topics": [
            "Computer science/Data structures"
        ],
        "date": "2019-05-30",
        "abstract": "\nThis entry provides a formalization of multidimensional binary trees,\nalso known as k-d trees. It includes a balanced build algorithm as\nwell as the nearest neighbor algorithm and the range search algorithm.\nIt is based on the papers <a\nhref=\"https://dl.acm.org/citation.cfm?doid=361002.361007\">Multidimensional\nbinary search trees used for associative searching</a> and <a\nhref=\"https://dl.acm.org/citation.cfm?doid=355744.355745\">\nAn Algorithm for Finding Best Matches in Logarithmic Expected\nTime</a>.",
        "extra": {
            "Change history": "[2020-15-04] Change representation of k-dimensional points from 'list' to\nHOL-Analysis.Finite_Cartesian_Product 'vec'. Update proofs\nto incorporate HOL-Analysis 'dist' and 'cbox' primitives."
        },
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2019-06-04"
            }
        ],
        "dependencies": [
            "Median_Of_Medians_Selection"
        ],
        "theories": [
            "KD_Tree",
            "Build",
            "Range_Search",
            "Nearest_Neighbors"
        ]
    },
    {
        "session": "BytecodeLogicJmlTypes",
        "title": "A Bytecode Logic for JML and Types",
        "authors": [
            "Lennart Beringer",
            "Martin Hofmann"
        ],
        "date": "2008-12-12",
        "topics": [
            "Computer science/Programming languages/Logics"
        ],
        "abstract": "This document contains the Isabelle/HOL sources underlying the paper <i>A bytecode logic for JML and types</i> by Beringer and Hofmann, updated to Isabelle 2008. We present a program logic for a subset of sequential Java bytecode that is suitable for representing both, features found in high-level specification language JML as well as interpretations of high-level type systems. To this end, we introduce a fine-grained collection of assertions, including strong invariants, local annotations and VDM-reminiscent partial-correctness specifications. Thanks to a goal-oriented structure and interpretation of judgements, verification may proceed without recourse to an additional control flow analysis. The suitability for interpreting intensional type systems is illustrated by the proof-carrying-code style encoding of a type system for a first-order functional language which guarantees a constant upper bound on the number of objects allocated throughout an execution, be the execution terminating or non-terminating. Like the published paper, the formal development is restricted to a comparatively small subset of the JVML, lacking (among other features) exceptions, arrays, virtual methods, and static fields. This shortcoming has been overcome meanwhile, as our paper has formed the basis of the Mobius base logic, a program logic for the full sequential fragment of the JVML. Indeed, the present formalisation formed the basis of a subsequent formalisation of the Mobius base logic in the proof assistant Coq, which includes a proof of soundness with respect to the Bicolano operational semantics by Pichardie.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-24"
            },
            {
                "2011-1": "2011-10-11"
            },
            {
                "2011": "2011-02-11"
            },
            {
                "2009-2": "2010-06-30"
            },
            {
                "2009-1": "2009-12-12"
            },
            {
                "2009": "2009-04-29"
            },
            {
                "2008": "2008-12-22"
            }
        ],
        "theories": [
            "AssocLists",
            "Language",
            "Logic",
            "MultiStep",
            "Reachability",
            "Sound",
            "Cachera"
        ]
    },
    {
        "session": "Attack_Trees",
        "title": "Attack Trees in Isabelle for GDPR compliance of IoT healthcare systems",
        "authors": [
            "Florian Kammüller"
        ],
        "topics": [
            "Computer science/Security"
        ],
        "date": "2020-04-27",
        "abstract": "\nIn this article, we present a proof theory for Attack Trees. Attack\nTrees are a well established and useful model for the construction of\nattacks on systems since they allow a stepwise exploration of high\nlevel attacks in application scenarios. Using the expressiveness of\nHigher Order Logic in Isabelle, we develop a generic\ntheory of Attack Trees with a state-based semantics based on Kripke\nstructures and CTL. The resulting framework\nallows mechanically supported logic analysis of the meta-theory of the\nproof calculus of Attack Trees and at the same time the developed\nproof theory enables application to case studies. A central\ncorrectness and completeness result proved in Isabelle establishes a\nconnection between the notion of Attack Tree validity and CTL. The\napplication is illustrated on the example of a healthcare IoT system\nand GDPR compliance verification.",
        "licence": "BSD",
        "theories": [
            "MC",
            "AT",
            "Infrastructure",
            "GDPRhealthcare"
        ]
    },
    {
        "session": "SC_DOM_Components",
        "title": "A Formalization of Safely Composable Web Components",
        "authors": [
            "Achim D. Brucker",
            "Michael Herzberg"
        ],
        "topics": [
            "Computer science/Data structures"
        ],
        "date": "2020-09-28",
        "abstract": "\nWhile the (safely composable) DOM with shadow trees provide the\ntechnical basis for defining web components, it does neither defines\nthe concept of web components nor specifies the safety properties that\nweb components should guarantee. Consequently, the standard also does\nnot discuss how or even if the methods for modifying the DOM respect\ncomponent boundaries. In AFP entry, we present a formally verified\nmodel of safely composable web components and define safety properties\nwhich ensure that different web components can only interact with each\nother using well-defined interfaces. Moreover, our verification of the\napplication programming interface (API) of the DOM revealed numerous\ninvariants that implementations of the DOM API need to preserve to\nensure the integrity of components.  In comparison to the strict\nstandard compliance formalization of Web Components in the AFP entry\n\"DOM_Components\", the notion of components in this entry\n(based on \"SC_DOM\" and \"Shadow_SC_DOM\") provides\nmuch stronger safety guarantees.",
        "licence": "BSD",
        "dependencies": [
            "Shadow_SC_DOM"
        ],
        "theories": [
            "Core_DOM_DOM_Components",
            "Core_DOM_SC_DOM_Components",
            "Shadow_DOM_DOM_Components",
            "Shadow_DOM_SC_DOM_Components"
        ]
    },
    {
        "session": "Selection_Heap_Sort",
        "title": "Verification of Selection and Heap Sort Using Locales",
        "authors": [
            "Danijela Petrovic"
        ],
        "date": "2014-02-11",
        "topics": [
            "Computer science/Algorithms"
        ],
        "abstract": "\nStepwise program refinement techniques can be used to simplify\nprogram verification. Programs are better understood since their\nmain properties are clearly stated, and verification of rather\ncomplex algorithms is reduced to proving simple statements\nconnecting successive program specifications. Additionally, it is\neasy to analyze similar algorithms and to compare their properties\nwithin a single formalization. Usually, formal analysis is not done\nin educational setting due to complexity of verification and a lack\nof tools and procedures to make comparison easy. Verification of an\nalgorithm should not only give correctness proof, but also better\nunderstanding of an algorithm. If the verification is based on small\nstep program refinement, it can become simple enough to be\ndemonstrated within the university-level computer science\ncurriculum. In this paper we demonstrate this and give a formal\nanalysis of two well known algorithms (Selection Sort and Heap Sort)\nusing proof assistant Isabelle/HOL and program refinement\ntechniques.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2014-02-18"
            }
        ],
        "theories": [
            "Sort",
            "RemoveMax",
            "SelectionSort_Functional",
            "Heap",
            "HeapFunctional",
            "HeapImperative"
        ]
    },
    {
        "session": "Hoare_Time",
        "title": "Hoare Logics for Time Bounds",
        "authors": [
            "Maximilian P. L. Haslbeck",
            "Tobias Nipkow"
        ],
        "topics": [
            "Computer science/Programming languages/Logics"
        ],
        "date": "2018-02-26",
        "abstract": "\nWe study three different Hoare logics for reasoning about time bounds\nof imperative programs and formalize them in Isabelle/HOL: a classical\nHoare like logic due to Nielson, a logic with potentials due to\nCarbonneaux <i>et al.</i> and a <i>separation\nlogic</i> following work by Atkey, Chaguérand and Pottier.\nThese logics are formally shown to be sound and complete. Verification\ncondition generators are developed and are shown sound and complete\ntoo.  We also consider variants of the systems where we abstract from\nmultiplicative constants in the running time bounds, thus supporting a\nbig-O style of reasoning.  Finally we compare the expressive power of\nthe three systems.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2018-02-26"
            }
        ],
        "dependencies": [
            "Separation_Algebra"
        ],
        "theories": [
            "AExp",
            "BExp",
            "Com",
            "Big_Step",
            "Big_StepT",
            "Nielson_Hoare",
            "Nielson_VCG",
            "Vars",
            "Nielson_VCGi",
            "Nielson_VCGi_complete",
            "Nielson_Examples",
            "Nielson_Sqrt",
            "Quant_Hoare",
            "Quant_VCG",
            "Quant_Examples",
            "QuantK_Hoare",
            "QuantK_VCG",
            "QuantK_Examples",
            "QuantK_Sqrt",
            "Partial_Evaluation",
            "Product_Separation_Algebra",
            "Sep_Algebra_Add",
            "Big_StepT_Partial",
            "SepLog_Hoare",
            "SepLog_Examples",
            "SepLogK_Hoare",
            "SepLogK_VCG",
            "Discussion",
            "DiscussionO",
            "Hoare_Time"
        ]
    },
    {
        "session": "KBPs",
        "title": "Knowledge-based programs",
        "authors": [
            "Peter Gammie"
        ],
        "topics": [
            "Computer science/Automata and formal languages"
        ],
        "date": "2011-05-17",
        "abstract": "Knowledge-based programs (KBPs) are a formalism for directly relating agents' knowledge and behaviour. Here we present a general scheme for compiling KBPs to executable automata with a proof of correctness in Isabelle/HOL. We develop the algorithm top-down, using Isabelle's locale mechanism to structure these proofs, and show that two classic examples can be synthesised using Isabelle's code generator.",
        "extra": {
            "Change history": "[2012-03-06] Add some more views and revive the code generation."
        },
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-03-02"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-24"
            },
            {
                "2011-1": "2011-10-11"
            },
            {
                "2011": "2011-05-19"
            }
        ],
        "dependencies": [
            "Transitive-Closure",
            "Trie"
        ],
        "theories": [
            "Extra",
            "List_local",
            "Kripke",
            "Traces",
            "KBPs",
            "KBPsAuto",
            "DFS",
            "MapOps",
            "KBPsAlg",
            "ODList",
            "Eval",
            "Trie2",
            "ClockView",
            "SPRView",
            "SPRViewDet",
            "SPRViewNonDet",
            "SPRViewNonDetIndInit",
            "SPRViewSingle",
            "Views",
            "MuddyChildren",
            "Robot",
            "Examples",
            "KBPs_Main"
        ]
    },
    {
        "session": "Polynomial_Interpolation",
        "title": "Polynomial Interpolation",
        "topics": [
            "Mathematics/Algebra"
        ],
        "authors": [
            "René Thiemann",
            "Akihisa Yamada"
        ],
        "date": "2016-01-29",
        "abstract": "\nWe formalized three algorithms for polynomial interpolation over arbitrary\nfields: Lagrange's explicit expression, the recursive algorithm of Neville\nand Aitken, and the Newton interpolation in combination with an efficient\nimplementation of divided differences.  Variants of these algorithms for\ninteger polynomials are also available, where sometimes the interpolation\ncan fail; e.g., there is no linear integer polynomial <i>p</i> such that\n<i>p(0) = 0</i> and <i>p(2) = 1</i>. Moreover, for the Newton interpolation\nfor integer polynomials, we proved that all intermediate results that are\ncomputed during the algorithm must be integers.  This admits an early\nfailure detection in the implementation.  Finally, we proved the uniqueness\nof polynomial interpolation.\n<p>\nThe development also contains improved code equations to speed up the\ndivision of integers in target languages.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            }
        ],
        "dependencies": [
            "Sqrt_Babylonian"
        ],
        "theories": [
            "Is_Rat_To_Rat",
            "Divmod_Int",
            "Improved_Code_Equations",
            "Ring_Hom",
            "Missing_Unsorted",
            "Missing_Polynomial",
            "Ring_Hom_Poly",
            "Newton_Interpolation",
            "Lagrange_Interpolation",
            "Neville_Aitken_Interpolation",
            "Polynomial_Interpolation"
        ]
    },
    {
        "session": "DiscretePricing",
        "title": "Pricing in discrete financial models",
        "authors": [
            "Mnacho Echenim"
        ],
        "topics": [
            "Mathematics/Probability theory",
            "Mathematics/Games and economics"
        ],
        "date": "2018-07-16",
        "abstract": "\nWe have formalized the computation of fair prices for derivative\nproducts in discrete financial models. As an application, we derive a\nway to compute fair prices of derivative products in the\nCox-Ross-Rubinstein model of a financial market, thus completing the\nwork that was presented in this <a\nhref=\"https://hal.archives-ouvertes.fr/hal-01562944\">paper</a>.",
        "extra": {
            "Change history": "[2019-05-12]\nRenamed discr_mkt predicate to stk_strict_subs and got rid of predicate A for a more natural definition of the type discrete_market;\nrenamed basic quantity processes for coherent notation;\nrenamed value_process into val_process and closing_value_process to cls_val_process;\nrelaxed hypothesis of lemma CRR_market_fair_price.\nAdded functions to price some basic options.\n(revision 0b813a1a833f)<br>"
        },
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2018-07-18"
            }
        ],
        "theories": [
            "Generated_Subalgebra",
            "Filtration",
            "Martingale",
            "Disc_Cond_Expect",
            "Infinite_Coin_Toss_Space",
            "Geometric_Random_Walk",
            "Fair_Price",
            "CRR_Model",
            "Option_Price_Examples"
        ]
    },
    {
        "session": "JiveDataStoreModel",
        "title": "Jive Data and Store Model",
        "authors": [
            "Nicole Rauch",
            "Norbert Schirmer"
        ],
        "date": "2005-06-20",
        "license": "LGPL",
        "topics": [
            "Computer science/Programming languages/Misc"
        ],
        "abstract": "This document presents the formalization of an object-oriented data and store model in Isabelle/HOL. This model is being used in the Java Interactive Verification Environment, Jive.",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-24"
            },
            {
                "2011-1": "2011-10-11"
            },
            {
                "2011": "2011-02-11"
            },
            {
                "2009-2": "2010-07-01"
            },
            {
                "2009-1": "2009-12-12"
            },
            {
                "2009": "2009-04-29"
            },
            {
                "2008": "2008-06-10"
            },
            {
                "2007": "2007-11-27"
            },
            {
                "2005": "2005-10-14"
            }
        ],
        "theories": [
            "TypeIds",
            "JavaType",
            "DirectSubtypes",
            "Subtype",
            "Attributes",
            "AttributesIndep",
            "Value",
            "Location",
            "Store",
            "StoreProperties",
            "JML",
            "UnivSpec"
        ]
    },
    {
        "session": "WorkerWrapper",
        "title": "The Worker/Wrapper Transformation",
        "authors": [
            "Peter Gammie"
        ],
        "date": "2009-10-30",
        "topics": [
            "Computer science/Programming languages/Transformations"
        ],
        "abstract": "Gill and Hutton formalise the worker/wrapper transformation, building on the work of Launchbury and Peyton-Jones who developed it as a way of changing the type at which a recursive function operates. This development establishes the soundness of the technique and several examples of its use.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-24"
            },
            {
                "2011-1": "2011-10-11"
            },
            {
                "2011": "2011-02-11"
            },
            {
                "2009-2": "2010-07-01"
            },
            {
                "2009-1": "2009-12-12"
            },
            {
                "2009": "2009-10-31"
            },
            {
                "2009": "2009-10-30"
            }
        ],
        "theories": [
            "Nats",
            "LList",
            "Maybe",
            "FixedPointTheorems",
            "WorkerWrapper",
            "CounterExample",
            "WorkerWrapperNew",
            "Accumulator",
            "UnboxedNats",
            "Streams",
            "Continuations",
            "Backtracking",
            "Nub",
            "Last"
        ]
    },
    {
        "session": "Inductive_Confidentiality",
        "title": "Inductive Study of Confidentiality",
        "authors": [
            "Giampaolo Bella"
        ],
        "date": "2012-05-02",
        "topics": [
            "Computer science/Security"
        ],
        "abstract": "This document contains the full theory files accompanying article <i>Inductive Study of Confidentiality --- for Everyone</i> in <i>Formal Aspects of Computing</i>. They aim at an illustrative and didactic presentation of the Inductive Method of protocol analysis, focusing on the treatment of one of the main goals of security protocols: confidentiality against a threat model. The treatment of confidentiality, which in fact forms a key aspect of all protocol analysis tools, has been found cryptic by many learners of the Inductive Method, hence the motivation for this work. The theory files in this document guide the reader step by step towards design and proof of significant confidentiality theorems. These are developed against two threat models, the standard Dolev-Yao and a more audacious one, the General Attacker, which turns out to be particularly useful also for teaching purposes.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-24"
            },
            {
                "2011-1": "2012-05-02"
            }
        ],
        "theories": [
            "Message",
            "Event",
            "Public",
            "NS_Public_Bad",
            "ConfidentialityDY",
            "MessageGA",
            "EventGA",
            "PublicGA",
            "NS_Public_Bad_GA",
            "ConfidentialityGA",
            "Knowledge"
        ]
    },
    {
        "session": "BinarySearchTree",
        "title": "Binary Search Trees",
        "authors": [
            "Viktor Kuncak"
        ],
        "date": "2004-04-05",
        "topics": [
            "Computer science/Data structures"
        ],
        "abstract": "The correctness is shown of binary search tree operations (lookup, insert and remove) implementing a set. Two versions are given, for both structured and linear (tactic-style) proofs. An implementation of integer-indexed maps is also verified.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-24"
            },
            {
                "2011-1": "2011-10-11"
            },
            {
                "2011": "2011-02-11"
            },
            {
                "2009-2": "2010-06-30"
            },
            {
                "2009-1": "2009-12-12"
            },
            {
                "2009": "2009-04-29"
            },
            {
                "2008": "2008-06-10"
            },
            {
                "2007": "2007-11-27"
            },
            {
                "2005": "2005-10-14"
            },
            {
                "2004": "2004-09-21"
            },
            {
                "2004": "2004-04-21"
            },
            {
                "2004": "2004-04-20"
            }
        ],
        "theories": [
            "BinaryTree",
            "BinaryTree_Map",
            "BinaryTree_TacticStyle"
        ]
    },
    {
        "session": "Consensus_Refined",
        "title": "Consensus Refined",
        "date": "2015-03-18",
        "authors": [
            "Ognjen Marić",
            "Christoph Sprenger"
        ],
        "topics": [
            "Computer science/Algorithms/Distributed"
        ],
        "abstract": "\nAlgorithms for solving the consensus problem are fundamental to\ndistributed computing. Despite their brevity, their\nability to operate in concurrent, asynchronous and failure-prone\nenvironments comes at the cost of complex and subtle\nbehaviors. Accordingly, understanding how they work and proving\ntheir correctness is a non-trivial endeavor where abstraction\nis immensely helpful.\nMoreover, research on consensus has yielded a large number of\nalgorithms, many of which appear to share common algorithmic\nideas. A natural question is whether and how these similarities can\nbe distilled and described in a precise, unified way.\nIn this work, we combine stepwise refinement and\nlockstep models to provide an abstract and unified\nview of a sizeable family of consensus algorithms. Our models\nprovide insights into the design choices underlying the different\nalgorithms, and classify them based on those choices.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2015-03-19"
            }
        ],
        "dependencies": [
            "Heard_Of",
            "Stuttering_Equivalence"
        ],
        "theories": [
            "Infra",
            "Consensus_Types",
            "Quorums",
            "Consensus_Misc",
            "Two_Steps",
            "Three_Steps",
            "Refinement",
            "HO_Transition_System",
            "Voting",
            "Voting_Opt",
            "OneThirdRule_Defs",
            "OneThirdRule_Proofs",
            "Ate_Defs",
            "Ate_Proofs",
            "Same_Vote",
            "Observing_Quorums",
            "Observing_Quorums_Opt",
            "Two_Step_Observing",
            "Uv_Defs",
            "Uv_Proofs",
            "BenOr_Defs",
            "BenOr_Proofs",
            "MRU_Vote",
            "MRU_Vote_Opt",
            "Three_Step_MRU",
            "New_Algorithm_Defs",
            "New_Algorithm_Proofs",
            "Paxos_Defs",
            "Paxos_Proofs",
            "CT_Defs",
            "CT_Proofs"
        ]
    },
    {
        "session": "Extended_Finite_State_Machine_Inference",
        "title": "Inference of Extended Finite State Machines",
        "authors": [
            "Michael Foster",
            "Achim D. Brucker",
            "Ramsay G. Taylor",
            "John Derrick"
        ],
        "topics": [
            "Computer science/Automata and formal languages"
        ],
        "date": "2020-09-07",
        "abstract": "\nIn this AFP entry, we provide a formal implementation of a\nstate-merging technique to infer extended finite state machines\n(EFSMs), complete with output and update functions, from black-box\ntraces. In particular, we define the subsumption in context relation\nas a means of determining whether one transition is able to account\nfor the behaviour of another. Building on this, we define the direct\nsubsumption relation, which lifts the subsumption in context relation\nto EFSM level such that we can use it to determine whether it is safe\nto merge a given pair of transitions. Key proofs include the\nconditions necessary for subsumption to occur and that subsumption\nand direct subsumption are preorder relations.  We also provide a\nnumber of different heuristics which can be used to abstract away\nconcrete values into registers so that more states and transitions can\nbe merged and provide proofs of the various conditions which must hold\nfor these abstractions to subsume their ungeneralised counterparts. A\nCode Generator setup to create executable Scala code is also defined.",
        "licence": "BSD",
        "dependencies": [
            "Extended_Finite_State_Machines"
        ],
        "theories": [
            "Subsumption",
            "Drinks_Subsumption",
            "Inference",
            "SelectionStrategies",
            "Store_Reuse",
            "Store_Reuse_Subsumption",
            "Increment_Reset",
            "Same_Register",
            "Least_Upper_Bound",
            "Distinguishing_Guards",
            "Weak_Subsumption",
            "Group_By",
            "PTA_Generalisation",
            "EFSM_Dot",
            "efsm2sal",
            "Code_Target_List",
            "Code_Target_Set",
            "Code_Target_FSet",
            "Code_Generation"
        ]
    },
    {
        "session": "DiskPaxos",
        "title": "Proving the Correctness of Disk Paxos",
        "date": "2005-06-22",
        "authors": [
            "Mauro Jaskelioff",
            "Stephan Merz"
        ],
        "topics": [
            "Computer science/Algorithms/Distributed"
        ],
        "abstract": "Disk Paxos is an algorithm for building arbitrary fault-tolerant distributed systems. The specification of Disk Paxos has been proved correct informally and tested using the TLC model checker, but up to now, it has never been fully formally verified. In this work we have formally verified its correctness using the Isabelle theorem prover and the HOL logic system, showing that Isabelle is a practical tool for verifying properties of TLA+ specifications.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-24"
            },
            {
                "2011-1": "2011-10-11"
            },
            {
                "2011": "2011-02-11"
            },
            {
                "2009-2": "2010-06-30"
            },
            {
                "2009-1": "2009-12-12"
            },
            {
                "2009": "2009-04-29"
            },
            {
                "2008": "2008-06-10"
            },
            {
                "2007": "2007-11-27"
            },
            {
                "2005": "2005-10-14"
            },
            {
                "2004": "2005-06-22"
            }
        ],
        "theories": [
            "DiskPaxos_Model",
            "DiskPaxos_Inv1",
            "DiskPaxos_Inv2",
            "DiskPaxos_Inv3",
            "DiskPaxos_Inv4",
            "DiskPaxos_Inv5",
            "DiskPaxos_Chosen",
            "DiskPaxos_Inv6",
            "DiskPaxos_Invariant",
            "DiskPaxos"
        ]
    },
    {
        "session": "XML",
        "title": "XML",
        "authors": [
            "Christian Sternagel",
            "René Thiemann"
        ],
        "date": "2014-10-03",
        "topics": [
            "Computer science/Functional programming",
            "Computer science/Data structures"
        ],
        "abstract": "\nThis entry provides an XML library for Isabelle/HOL. This includes parsing\nand pretty printing of XML trees as well as combinators for transforming XML\ntrees into arbitrary user-defined data. The main contribution of this entry is\nan interface (fit for code generation) that allows for communication between\nverified programs formalized in Isabelle/HOL and the outside world via XML.\nThis library was developed as part of the IsaFoR/CeTA project\nto which we refer for examples of its usage.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-10-08"
            }
        ],
        "dependencies": [
            "Certification_Monads",
            "Show"
        ],
        "theories": [
            "Xml",
            "Xmlt"
        ]
    },
    {
        "session": "Mason_Stothers",
        "title": "The Mason–Stothers Theorem",
        "authors": [
            "Manuel Eberl"
        ],
        "topics": [
            "Mathematics/Algebra"
        ],
        "date": "2017-12-21",
        "abstract": "\n<p>This article provides a formalisation of Snyder’s simple and\nelegant proof of the Mason&ndash;Stothers theorem, which is the\npolynomial analogue of the famous abc Conjecture for integers.\nRemarkably, Snyder found this very elegant proof when he was still a\nhigh-school student.</p> <p>In short, the statement of the\ntheorem is that three non-zero coprime polynomials\n<em>A</em>, <em>B</em>, <em>C</em>\nover a field which sum to 0 and do not all have vanishing derivatives\nfulfil max{deg(<em>A</em>), deg(<em>B</em>),\ndeg(<em>C</em>)} < deg(rad(<em>ABC</em>))\nwhere the rad(<em>P</em>) denotes the\n<em>radical</em> of <em>P</em>,\ni.&thinsp;e. the product of all unique irreducible factors of\n<em>P</em>.</p> <p>This theorem also implies a\nkind of polynomial analogue of Fermat’s Last Theorem for polynomials:\nexcept for trivial cases,\n<em>A<sup>n</sup></em> +\n<em>B<sup>n</sup></em> +\n<em>C<sup>n</sup></em> = 0 implies\nn&nbsp;&le;&nbsp;2 for coprime polynomials\n<em>A</em>, <em>B</em>, <em>C</em>\nover a field.</em></p>",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-12-22"
            }
        ],
        "theories": [
            "Mason_Stothers"
        ]
    },
    {
        "session": "Density_Compiler",
        "title": "A Verified Compiler for Probability Density Functions",
        "authors": [
            "Manuel Eberl",
            "Johannes Hölzl",
            "Tobias Nipkow"
        ],
        "date": "2014-10-09",
        "topics": [
            "Mathematics/Probability theory",
            "Computer science/Programming languages/Compiling"
        ],
        "abstract": "\n<a href=\"https://doi.org/10.1007/978-3-642-36742-7_35\">Bhat et al. [TACAS 2013]</a> developed an inductive compiler that computes\ndensity functions for probability spaces described by programs in a\nprobabilistic functional language. In this work, we implement such a\ncompiler for a modified version of this language within the theorem prover\nIsabelle and give a formal proof of its soundness w.r.t. the semantics of\nthe source and target language.  Together with Isabelle's code generation\nfor inductive predicates, this yields a fully verified, executable density\ncompiler. The proof is done in two steps: First, an abstract compiler\nworking with abstract functions modelled directly in the theorem prover's\nlogic is defined and proved sound.  Then, this compiler is refined to a\nconcrete version that returns a target-language expression.\n<p>\nAn article with the same title and authors is published in the proceedings\nof ESOP 2015.\nA detailed presentation of this work can be found in the first author's\nmaster's thesis.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-12-22"
            },
            {
                "2014": "2014-10-09"
            }
        ],
        "theories": [
            "Density_Predicates",
            "PDF_Transformations",
            "PDF_Values",
            "PDF_Semantics",
            "PDF_Density_Contexts",
            "PDF_Compiler_Pred",
            "PDF_Target_Semantics",
            "PDF_Target_Density_Contexts",
            "PDF_Compiler"
        ]
    },
    {
        "session": "Priority_Queue_Braun",
        "title": "Priority Queues Based on Braun Trees",
        "authors": [
            "Tobias Nipkow"
        ],
        "date": "2014-09-04",
        "topics": [
            "Computer science/Data structures"
        ],
        "abstract": "\nThis entry verifies priority queues based on Braun trees. Insertion\nand deletion take logarithmic time and preserve the balanced nature\nof Braun trees. Two implementations of deletion are provided.",
        "extra": {
            "Change history": "[2019-12-16] Added theory Priority_Queue_Braun2 with second version of del_min"
        },
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-09-04"
            }
        ],
        "theories": [
            "Priority_Queue_Braun",
            "Priority_Queue_Braun2",
            "Sorting_Braun"
        ]
    },
    {
        "session": "Noninterference_CSP",
        "title": "Noninterference Security in Communicating Sequential Processes",
        "authors": [
            "Pasquale Noce"
        ],
        "date": "2014-05-23",
        "topics": [
            "Computer science/Security"
        ],
        "abstract": "\n<p>\nAn extension of classical noninterference security for deterministic\nstate machines, as introduced by Goguen and Meseguer and elegantly\nformalized by Rushby, to nondeterministic systems should satisfy two\nfundamental requirements: it should be based on a mathematically precise\ntheory of nondeterminism, and should be equivalent to (or at least not\nweaker than) the classical notion in the degenerate deterministic case.\n</p>\n<p>\nThis paper proposes a definition of noninterference security applying\nto Hoare's Communicating Sequential Processes (CSP) in the general case of\na possibly intransitive noninterference policy, and proves the\nequivalence of this security property to classical noninterference\nsecurity for processes representing deterministic state machines.\n</p>\n<p>\nFurthermore, McCullough's generalized noninterference security is shown\nto be weaker than both the proposed notion of CSP noninterference security\nfor a generic process, and classical noninterference security for processes\nrepresenting deterministic state machines. This renders CSP noninterference\nsecurity preferable as an extension of classical noninterference security\nto nondeterministic systems.\n</p>",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-06-13"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2014-05-24"
            }
        ],
        "theories": [
            "CSPNoninterference",
            "ClassicalNoninterference",
            "GeneralizedNoninterference"
        ]
    },
    {
        "session": "No_FTL_observers",
        "title": "No Faster-Than-Light Observers",
        "authors": [
            "Mike Stannett",
            "István Németi"
        ],
        "date": "2016-04-28",
        "topics": [
            "Mathematics/Physics"
        ],
        "abstract": "\nWe provide a formal proof within First Order Relativity Theory that no\nobserver can travel faster than the speed of light. Originally\nreported in Stannett & Németi (2014) \"Using Isabelle/HOL to verify\nfirst-order relativity theory\", Journal of Automated Reasoning 52(4),\npp. 361-378.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-04-28"
            }
        ],
        "theories": [
            "SpaceTime",
            "SomeFunc",
            "Axioms",
            "SpecRel"
        ]
    },
    {
        "session": "Jordan_Hoelder",
        "title": "The Jordan-Hölder Theorem",
        "authors": [
            "Jakob von Raumer"
        ],
        "date": "2014-09-09",
        "topics": [
            "Mathematics/Algebra"
        ],
        "abstract": "This submission contains theories that lead to a formalization of the proof of the Jordan-Hölder theorem about composition series of finite groups. The theories formalize the notions of isomorphism classes of groups, simple groups, normal series, composition series, maximal normal subgroups. Furthermore, they provide proofs of the second isomorphism theorem for groups, the characterization theorem for maximal normal subgroups as well as many useful lemmas about normal subgroups and factor groups. The proof is inspired by course notes of Stuart Rankin.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-09-11"
            }
        ],
        "dependencies": [
            "Secondary_Sylow"
        ],
        "theories": [
            "SndIsomorphismGrp",
            "SubgroupsAndNormalSubgroups",
            "SimpleGroups",
            "MaximalNormalSubgroups",
            "CompositionSeries",
            "GroupIsoClasses",
            "JordanHolder"
        ]
    },
    {
        "session": "Separation_Logic_Imperative_HOL",
        "title": "A Separation Logic Framework for Imperative HOL",
        "authors": [
            "Peter Lammich",
            "Rene Meis"
        ],
        "date": "2012-11-14",
        "topics": [
            "Computer science/Programming languages/Logics"
        ],
        "license": "BSD",
        "abstract": "\nWe provide a framework for separation-logic based correctness proofs of\nImperative HOL programs. Our framework comes with a set of proof methods to\nautomate canonical tasks such as verification condition generation and\nframe inference. Moreover, we provide a set of examples that show the\napplicability of our framework. The examples include algorithms on lists,\nhash-tables, and union-find trees. We also provide abstract interfaces for\nlists, maps, and sets, that allow to develop generic imperative algorithms\nand use data-refinement techniques.\n<br>\nAs we target Imperative HOL, our programs can be translated to\nefficiently executable code in various target languages, including\nML, OCaml, Haskell, and Scala.",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-03-02"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-11-15"
            }
        ],
        "dependencies": [
            "Automatic_Refinement",
            "Collections",
            "Native_Word"
        ],
        "theories": [
            "Imperative_HOL_Add",
            "Syntax_Match",
            "Run",
            "Assertions",
            "Hoare_Triple",
            "Automation",
            "Sep_Main",
            "Imp_List_Spec",
            "List_Seg",
            "Open_List",
            "Circ_List",
            "Imp_Map_Spec",
            "Hash_Table",
            "Hash_Map",
            "Hash_Map_Impl",
            "Imp_Set_Spec",
            "Hash_Set_Impl",
            "To_List_GA",
            "Union_Find",
            "Idioms",
            "Default_Insts",
            "Array_Blit",
            "Array_Map_Impl",
            "Array_Set_Impl",
            "From_List_GA",
            "Sep_Examples"
        ]
    },
    {
        "session": "Factored_Transition_System_Bounding",
        "title": "Upper Bounding Diameters of State Spaces of Factored Transition Systems",
        "authors": [
            "Friedrich Kurz",
            "Mohammad Abdulaziz"
        ],
        "topics": [
            "Computer science/Automata and formal languages",
            "Mathematics/Graph theory"
        ],
        "date": "2018-10-12",
        "abstract": "\nA completeness threshold is required to guarantee the completeness of\nplanning as satisfiability, and bounded model checking of safety\nproperties. One valid completeness threshold is the diameter of the\nunderlying transition system. The diameter is the maximum element in\nthe set of lengths of all shortest paths between pairs of states. The\ndiameter is not calculated exactly in our setting, where the\ntransition system is succinctly described using a (propositionally)\nfactored representation. Rather, an upper bound on the diameter is\ncalculated compositionally, by bounding the diameters of small\nabstract subsystems, and then composing those.  We port a HOL4\nformalisation of a compositional algorithm for computing a relatively\ntight upper bound on the system diameter. This compositional algorithm\nexploits acyclicity in the state space to achieve compositionality,\nand it was introduced by Abdulaziz et. al. The formalisation that we\nport is described as a part of another paper by Abdulaziz et. al. As a\npart of this porting we developed a libray about transition systems,\nwhich shall be of use in future related mechanisation efforts.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-10-16"
            }
        ],
        "theories": [
            "FactoredSystemLib",
            "ListUtils",
            "FSSublist",
            "HoArithUtils",
            "FmapUtils",
            "FactoredSystem",
            "ActionSeqProcess",
            "RelUtils",
            "Dependency",
            "Invariants",
            "SetUtils",
            "TopologicalProps",
            "SystemAbstraction",
            "Acyclicity",
            "AcycSspace"
        ]
    },
    {
        "session": "Iptables_Semantics",
        "title": "Iptables Semantics",
        "authors": [
            "Cornelius Diekmann",
            "Lars Hupel"
        ],
        "date": "2016-09-09",
        "topics": [
            "Computer science/Networks"
        ],
        "abstract": "\nWe present a big step semantics of the filtering behavior of the\nLinux/netfilter iptables firewall. We provide algorithms to simplify\ncomplex iptables rulests to a simple firewall model (c.f. AFP entry <a\nhref=\"https://www.isa-afp.org/entries/Simple_Firewall.html\">Simple_Firewall</a>)\nand to verify spoofing protection of a ruleset.\nInternally, we embed our semantics into ternary logic, ultimately\nsupporting every iptables match condition by abstracting over\nunknowns. Using this AFP entry and all entries it depends on, we\ncreated an easy-to-use, stand-alone haskell tool called <a\nhref=\"http://iptables.isabelle.systems\">fffuu</a>. The tool does not\nrequire any input &mdash;except for the <tt>iptables-save</tt> dump of\nthe analyzed firewall&mdash; and presents interesting results about\nthe user's ruleset. Real-Word firewall errors have been uncovered, and\nthe correctness of rulesets has been proved, with the help of\nour tool.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-09-09"
            }
        ],
        "dependencies": [
            "Routing",
            "Native_Word"
        ],
        "theories": [
            "List_Misc",
            "Negation_Type",
            "WordInterval_Lists",
            "Repeat_Stabilize",
            "Firewall_Common",
            "Semantics",
            "Matching",
            "Ruleset_Update",
            "Call_Return_Unfolding",
            "Ternary",
            "Matching_Ternary",
            "Semantics_Ternary",
            "Datatype_Selectors",
            "IpAddresses",
            "L4_Protocol_Flags",
            "Ports",
            "Conntrack_State",
            "Tagged_Packet",
            "Common_Primitive_Syntax",
            "Unknown_Match_Tacs",
            "Common_Primitive_Matcher_Generic",
            "Common_Primitive_Matcher",
            "Example_Semantics",
            "Alternative_Semantics",
            "Semantics_Stateful",
            "Semantics_Goto",
            "Negation_Type_DNF",
            "Matching_Embeddings",
            "Fixed_Action",
            "Normalized_Matches",
            "Negation_Type_Matching",
            "Primitive_Normalization",
            "MatchExpr_Fold",
            "Common_Primitive_Lemmas",
            "Ports_Normalize",
            "IpAddresses_Normalize",
            "Interfaces_Normalize",
            "Word_Upto",
            "Protocols_Normalize",
            "Remdups_Rev",
            "Ipassmt",
            "No_Spoof",
            "Common_Primitive_toString",
            "Routing_IpAssmt",
            "Output_Interface_Replace",
            "Interface_Replace",
            "Optimizing",
            "Transform",
            "Conntrack_State_Transform",
            "Primitive_Abstract",
            "SimpleFw_Compliance",
            "Semantics_Embeddings",
            "Iptables_Semantics",
            "Code_Interface",
            "Parser6",
            "No_Spoof_Embeddings",
            "Parser",
            "Code_haskell",
            "Access_Matrix_Embeddings",
            "Documentation"
        ]
    },
    {
        "session": "Nat-Interval-Logic",
        "title": "Interval Temporal Logic on Natural Numbers",
        "authors": [
            "David Trachtenherz"
        ],
        "date": "2011-02-23",
        "topics": [
            "Logic/General logic/Temporal logic"
        ],
        "abstract": "We introduce a theory of temporal logic operators using sets of natural numbers as time domain, formalized in a shallow embedding manner. The theory comprises special natural intervals (theory IL_Interval: open and closed intervals, continuous and modulo intervals, interval traversing results), operators for shifting intervals to left/right on the number axis as well as expanding/contracting intervals by constant factors (theory IL_IntervalOperators.thy), and ultimately definitions and results for unary and binary temporal operators on arbitrary natural sets (theory IL_TemporalOperators).",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-24"
            },
            {
                "2011-1": "2011-10-11"
            },
            {
                "2011": "2011-02-24"
            }
        ],
        "dependencies": [
            "List-Infinite"
        ],
        "theories": [
            "IL_Interval",
            "IL_IntervalOperators",
            "IL_TemporalOperators"
        ]
    },
    {
        "session": "List-Infinite",
        "title": "Infinite Lists",
        "date": "2011-02-23",
        "authors": [
            "David Trachtenherz"
        ],
        "topics": [
            "Computer science/Data structures"
        ],
        "abstract": "We introduce a theory of infinite lists in HOL formalized as functions over naturals (folder ListInf, theories ListInf and ListInf_Prefix). It also provides additional results for finite lists (theory ListInf/List2), natural numbers (folder CommonArith, esp. division/modulo, naturals with infinity), sets (folder CommonSet, esp. cutting/truncating sets, traversing sets of naturals).",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-03-02"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-24"
            },
            {
                "2011-1": "2011-10-11"
            },
            {
                "2011": "2011-02-24"
            }
        ],
        "theories": [
            "Util_Set",
            "Util_MinMax",
            "Util_NatInf",
            "Util_Nat",
            "Util_Div",
            "SetInterval2",
            "SetIntervalCut",
            "SetIntervalStep",
            "List2",
            "InfiniteSet2",
            "ListInf",
            "ListInf_Prefix",
            "ListInfinite"
        ]
    },
    {
        "session": "Skip_Lists",
        "title": "Skip Lists",
        "authors": [
            "Max W. Haslbeck",
            "Manuel Eberl"
        ],
        "topics": [
            "Computer science/Data structures"
        ],
        "date": "2020-01-09",
        "abstract": "\n<p> Skip lists are sorted linked lists enhanced with shortcuts\nand are an alternative to binary search trees. A skip lists consists\nof multiple levels of sorted linked lists where a list on level n is a\nsubsequence of the list on level n − 1. In the ideal case, elements\nare skipped in such a way that a lookup in a skip lists takes O(log n)\ntime. In a randomised skip list the skipped elements are choosen\nrandomly. </p> <p> This entry contains formalized proofs\nof the textbook results about the expected height and the expected\nlength of a search path in a randomised skip list. </p>",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2020-01-10"
            }
        ],
        "dependencies": [
            "Monad_Normalisation"
        ],
        "theories": [
            "Pi_pmf",
            "Misc",
            "Geometric_PMF",
            "Skip_List"
        ]
    },
    {
        "session": "Secondary_Sylow",
        "title": "Secondary Sylow Theorems",
        "authors": [
            "Jakob von Raumer"
        ],
        "date": "2014-01-28",
        "topics": [
            "Mathematics/Algebra"
        ],
        "abstract": "These theories extend the existing proof of the first Sylow theorem\n(written by Florian Kammueller and L. C. Paulson) by what are often\ncalled the second, third and fourth Sylow theorems. These theorems\nstate propositions about the number of Sylow p-subgroups of a group\nand the fact that they are conjugate to each other. The proofs make\nuse of an implementation of group actions and their properties.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2014-01-29"
            }
        ],
        "theories": [
            "GroupAction",
            "SubgroupConjugation",
            "SndSylow"
        ]
    },
    {
        "session": "Simplex",
        "title": "An Incremental Simplex Algorithm with Unsatisfiable Core Generation",
        "authors": [
            "Filip Marić",
            "Mirko Spasić",
            "René Thiemann"
        ],
        "topics": [
            "Computer science/Algorithms/Optimization"
        ],
        "date": "2018-08-24",
        "abstract": "\nWe present an Isabelle/HOL formalization and total correctness proof\nfor the incremental version of the Simplex algorithm which is used in\nmost state-of-the-art SMT solvers. It supports extraction of\nsatisfying assignments, extraction of minimal unsatisfiable cores, incremental\nassertion of constraints and backtracking. The formalization relies on\nstepwise program refinement, starting from a simple specification,\ngoing through a number of refinement steps, and ending up in a fully\nexecutable functional implementation. Symmetries present in the\nalgorithm are handled with special care.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2020-01-14"
            },
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-27"
            }
        ],
        "theories": [
            "Simplex_Auxiliary",
            "Rel_Chain",
            "Simplex_Algebra",
            "Abstract_Linear_Poly",
            "Linear_Poly_Maps",
            "QDelta",
            "Simplex",
            "Simplex_Incremental"
        ]
    },
    {
        "session": "AWN",
        "title": "Mechanization of the Algebra for Wireless Networks (AWN)",
        "authors": [
            "Timothy Bourke"
        ],
        "date": "2014-03-08",
        "topics": [
            "Computer science/Concurrency/Process calculi"
        ],
        "abstract": "\n<p>\nAWN is a process algebra developed for modelling and analysing\nprotocols for Mobile Ad hoc Networks (MANETs) and Wireless Mesh\nNetworks (WMNs). AWN models comprise five distinct layers:\nsequential processes, local parallel compositions, nodes, partial\nnetworks, and complete networks.</p>\n<p>\nThis development mechanises the original operational semantics of\nAWN and introduces a variant 'open' operational semantics that\nenables the compositional statement and proof of invariants across\ndistinct network nodes. It supports labels (for weakening\ninvariants) and (abstract) data state manipulations. A framework for\ncompositional invariant proofs is developed, including a tactic\n(inv_cterms) for inductive invariant proofs of sequential processes,\nlifting rules for the open versions of the higher layers, and a rule\nfor transferring lifted properties back to the standard semantics. A\nnotion of 'control terms' reduces proof obligations to the subset of\nsubterms that act directly (in contrast to operators for combining\nterms and joining processes).</p>",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2014-03-15"
            }
        ],
        "theories": [
            "Lib",
            "TransitionSystems",
            "Invariants",
            "OInvariants",
            "AWN",
            "AWN_SOS",
            "AWN_Cterms",
            "AWN_Labels",
            "Inv_Cterms",
            "AWN_SOS_Labels",
            "Pnet",
            "Closed",
            "OAWN_SOS",
            "OAWN_SOS_Labels",
            "OPnet",
            "ONode_Lifting",
            "OPnet_Lifting",
            "OClosed_Lifting",
            "AWN_Invariants",
            "OAWN_Invariants",
            "OAWN_Convert",
            "Qmsg",
            "Qmsg_Lifting",
            "OClosed_Transfer",
            "AWN_Main",
            "Toy",
            "AWN_Term_Graph"
        ]
    },
    {
        "session": "Stateful_Protocol_Composition_and_Typing",
        "title": "Stateful Protocol Composition and Typing",
        "authors": [
            "Andreas V. Hess",
            "Sebastian Mödersheim",
            "Achim D. Brucker"
        ],
        "topics": [
            "Computer science/Security"
        ],
        "date": "2020-04-08",
        "abstract": "\nWe provide in this AFP entry several relative soundness results for\nsecurity protocols. In particular, we prove typing and\ncompositionality results for stateful protocols (i.e., protocols with\nmutable state that may span several sessions), and that focuses on\nreachability properties. Such results are useful to simplify protocol\nverification by reducing it to a simpler problem: Typing results give\nconditions under which it is safe to verify a protocol in a typed\nmodel where only \"well-typed\" attacks can occur whereas\ncompositionality results allow us to verify a composed protocol by\nonly verifying the component protocols in isolation. The conditions on\nthe protocols under which the results hold are furthermore syntactic\nin nature allowing for full automation. The foundation presented here\nis used in another entry to provide fully automated and formalized\nsecurity proofs of stateful protocols.",
        "licence": "BSD",
        "dependencies": [
            "First_Order_Terms"
        ],
        "theories": [
            "Miscellaneous",
            "Messages",
            "More_Unification",
            "Intruder_Deduction",
            "Strands_and_Constraints",
            "Lazy_Intruder",
            "Typed_Model",
            "Typing_Result",
            "Stateful_Strands",
            "Stateful_Typing",
            "Labeled_Strands",
            "Parallel_Compositionality",
            "Labeled_Stateful_Strands",
            "Stateful_Compositionality",
            "Example_Keyserver",
            "Example_TLS",
            "Examples"
        ]
    },
    {
        "session": "Hello_World",
        "title": "Hello World",
        "authors": [
            "Cornelius Diekmann",
            "Lars Hupel"
        ],
        "topics": [
            "Computer science/Functional programming"
        ],
        "date": "2020-03-07",
        "abstract": "\nIn this article, we present a formalization of the well-known\n\"Hello, World!\" code, including a formal framework for\nreasoning about IO. Our model is inspired by the handling of IO in\nHaskell. We start by formalizing the 🌍 and embrace the IO monad\nafterwards. Then we present a sample main :: IO (), followed by its\nproof of correctness.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2020-03-23"
            }
        ],
        "theories": [
            "IO",
            "HelloWorld",
            "HelloWorld_Proof",
            "RunningCodeFromIsabelle"
        ]
    },
    {
        "session": "General-Triangle",
        "title": "The General Triangle Is Unique",
        "authors": [
            "Joachim Breitner"
        ],
        "topics": [
            "Mathematics/Geometry"
        ],
        "date": "2011-04-01",
        "abstract": "Some acute-angled triangles are special, e.g. right-angled or isoscele triangles. Some are not of this kind, but, without measuring angles, look as if they were. In that sense, there is exactly one general triangle. This well-known fact is proven here formally.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-24"
            },
            {
                "2011-1": "2011-10-11"
            },
            {
                "2011": "2011-04-01"
            }
        ],
        "theories": [
            "GeneralTriangle"
        ]
    },
    {
        "session": "LambdaMu",
        "title": "The LambdaMu-calculus",
        "authors": [
            "Cristina Matache",
            "Victor B. F. Gomes",
            "Dominic P. Mulligan"
        ],
        "topics": [
            "Computer science/Programming languages/Lambda calculi",
            "Logic/General logic/Lambda calculus"
        ],
        "date": "2017-08-16",
        "abstract": "\nThe propositions-as-types correspondence is ordinarily presented as\nlinking the metatheory of typed λ-calculi and the proof theory of\nintuitionistic logic. Griffin observed that this correspondence could\nbe extended to classical logic through the use of control operators.\nThis observation set off a flurry of further research, leading to the\ndevelopment of Parigots λμ-calculus. In this work, we formalise λμ-\ncalculus in Isabelle/HOL and prove several metatheoretical properties\nsuch as type preservation and progress.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2017-08-21"
            }
        ],
        "theories": [
            "Syntax",
            "Types",
            "DeBruijn",
            "Substitution",
            "Reduction",
            "ContextFacts",
            "TypePreservation",
            "Progress",
            "Peirce"
        ]
    },
    {
        "session": "Irrationality_J_Hancl",
        "title": "Irrational Rapidly Convergent Series",
        "authors": [
            "Angeliki Koutsoukou-Argyraki",
            "Wenda Li"
        ],
        "topics": [
            "Mathematics/Number theory",
            "Mathematics/Analysis"
        ],
        "date": "2018-05-23",
        "abstract": "\nWe formalize with Isabelle/HOL a proof of a theorem by J. Hancl asserting the\nirrationality of the sum of a series consisting of rational numbers, built up\nby sequences that fulfill certain properties. Even though the criterion is a\nnumber theoretic result, the proof makes use only of analytical arguments. We\nalso formalize a corollary of the theorem for a specific series fulfilling the\nassumptions of the theorem.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2018-05-26"
            }
        ],
        "theories": [
            "Irrationality_J_Hancl"
        ]
    },
    {
        "session": "Program-Conflict-Analysis",
        "title": "Formalization of Conflict Analysis of Programs with Procedures, Thread Creation, and Monitors",
        "topics": [
            "Computer science/Programming languages/Static analysis"
        ],
        "authors": [
            "Peter Lammich",
            "Markus Müller-Olm"
        ],
        "date": "2007-12-14",
        "abstract": "In this work we formally verify the soundness and precision of a static program analysis that detects conflicts (e. g. data races) in programs with procedures, thread creation and monitors with the Isabelle theorem prover. As common in static program analysis, our program model abstracts guarded branching by nondeterministic branching, but completely interprets the call-/return behavior of procedures, synchronization by monitors, and thread creation. The analysis is based on the observation that all conflicts already occur in a class of particularly restricted schedules. These restricted schedules are suited to constraint-system-based program analysis. The formalization is based upon a flowgraph-based program model with an operational semantics as reference point.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-03-02"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-24"
            },
            {
                "2011-1": "2011-10-11"
            },
            {
                "2011": "2011-02-11"
            },
            {
                "2009-2": "2010-07-01"
            },
            {
                "2009-1": "2009-12-12"
            },
            {
                "2009": "2009-04-29"
            },
            {
                "2008": "2008-06-10"
            },
            {
                "2007": "2007-12-20"
            }
        ],
        "theories": [
            "Misc",
            "Interleave",
            "ConsInterleave",
            "AcquisitionHistory",
            "LTS",
            "ThreadTracking",
            "Flowgraph",
            "Semantics",
            "Normalization",
            "ConstraintSystems",
            "MainResult"
        ]
    },
    {
        "session": "MSO_Regex_Equivalence",
        "title": "Decision Procedures for MSO on Words Based on Derivatives of Regular Expressions",
        "authors": [
            "Dmitriy Traytel",
            "Tobias Nipkow"
        ],
        "topics": [
            "Computer science/Automata and formal languages",
            "Logic/General logic/Decidability of theories"
        ],
        "date": "2014-06-12",
        "abstract": "\nMonadic second-order logic on finite words (MSO) is a decidable yet\nexpressive logic into which many decision problems can be encoded. Since MSO\nformulas correspond to regular languages, equivalence of MSO formulas can be\nreduced to the equivalence of some regular structures (e.g. automata). We\nverify an executable decision procedure for MSO formulas that is not based\non automata but on regular expressions.\n<p>\nDecision procedures for regular expression equivalence have been formalized\nbefore, usually based on Brzozowski derivatives. Yet, for a straightforward\nembedding of MSO formulas into regular expressions an extension of regular\nexpressions with a projection operation is required. We prove total\ncorrectness and completeness of an equivalence checker for regular\nexpressions extended in that way. We also define a language-preserving\ntranslation of formulas into regular expressions with respect to two\ndifferent semantics of MSO.\n<p>\nThe formalization is described in this <a href=\"http://www21.in.tum.de/~nipkow/pubs/icfp13.html\">ICFP 2013 functional pearl</a>.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2014-06-12"
            }
        ],
        "dependencies": [
            "List-Index",
            "Deriving"
        ],
        "theories": [
            "List_More",
            "Pi_Regular_Set",
            "Pi_Regular_Exp",
            "Pi_Derivatives",
            "Pi_Regular_Operators",
            "Pi_Regular_Exp_Dual",
            "Pi_Equivalence_Checking",
            "Init_Normalization",
            "PNormalization",
            "Formula",
            "M2L",
            "M2L_Normalization",
            "M2L_Equivalence_Checking",
            "WS1S",
            "WS1S_Normalization",
            "WS1S_Equivalence_Checking",
            "M2L_Examples",
            "WS1S_Examples"
        ]
    },
    {
        "session": "Poincare_Disc",
        "title": "Poincaré Disc Model",
        "authors": [
            "Danijela Simić",
            "Filip Marić",
            "Pierre Boutry"
        ],
        "topics": [
            "Mathematics/Geometry"
        ],
        "date": "2019-12-16",
        "abstract": "\nWe describe formalization of the Poincaré disc model of hyperbolic\ngeometry within the Isabelle/HOL proof assistant. The model is defined\nwithin the extended complex plane (one dimensional complex projectives\nspace &#8450;P1), formalized in the AFP entry “Complex Geometry”.\nPoints, lines, congruence of pairs of points, betweenness of triples\nof points, circles, and isometries are defined within the model. It is\nshown that the model satisfies all Tarski's axioms except the\nEuclid's axiom. It is shown that it satisfies its negation and\nthe limiting parallels axiom (which proves it to be a model of\nhyperbolic geometry).",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2020-01-17"
            }
        ],
        "dependencies": [
            "Complex_Geometry"
        ],
        "theories": [
            "Hyperbolic_Functions",
            "Tarski",
            "Poincare_Lines",
            "Poincare_Lines_Ideal_Points",
            "Poincare_Distance",
            "Poincare_Circles",
            "Poincare_Between",
            "Poincare_Lines_Axis_Intersections",
            "Poincare_Perpendicular",
            "Poincare",
            "Poincare_Tarski"
        ]
    },
    {
        "session": "List_Update",
        "title": "Analysis of List Update Algorithms",
        "authors": [
            "Maximilian P. L. Haslbeck",
            "Tobias Nipkow"
        ],
        "date": "2016-02-17",
        "topics": [
            "Computer science/Algorithms/Online"
        ],
        "abstract": "\n<p>\nThese theories formalize the quantitative analysis of a number of classical algorithms for the list update problem: 2-competitiveness of move-to-front, the lower bound of 2 for the competitiveness of deterministic list update algorithms and 1.6-competitiveness of the randomized COMB algorithm, the best randomized list update algorithm known to date.\nThe material is based on the first two chapters of <i>Online Computation\nand Competitive Analysis</i> by Borodin and El-Yaniv.\n</p>\n<p>\nFor an informal description see the FSTTCS 2016 publication\n<a href=\"http://www21.in.tum.de/~nipkow/pubs/fsttcs16.html\">Verified Analysis of List Update Algorithms</a>\nby Haslbeck and Nipkow.\n</p>",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-10-15"
            },
            {
                "2016": "2016-02-23"
            },
            {
                "2016": "2016-02-22"
            }
        ],
        "dependencies": [
            "List-Index",
            "Regular-Sets"
        ],
        "theories": [
            "Inversion",
            "Swaps",
            "On_Off",
            "Prob_Theory",
            "Competitive_Analysis",
            "Move_to_Front",
            "Bit_Strings",
            "MTF2_Effects",
            "BIT",
            "Partial_Cost_Model",
            "RExp_Var",
            "OPT2",
            "Phase_Partitioning",
            "List_Factoring",
            "TS",
            "BIT_pairwise",
            "BIT_2comp_on2",
            "Comb"
        ]
    },
    {
        "session": "Randomised_Social_Choice",
        "title": "Randomised Social Choice Theory",
        "authors": [
            "Manuel Eberl"
        ],
        "date": "2016-05-05",
        "topics": [
            "Mathematics/Games and economics"
        ],
        "abstract": "\nThis work contains a formalisation of basic Randomised Social Choice,\nincluding Stochastic Dominance and Social Decision Schemes (SDSs)\nalong with some of their most important properties (Anonymity,\nNeutrality, ex-post- and SD-Efficiency, SD-Strategy-Proofness) and two\nparticular SDSs – Random Dictatorship and Random Serial Dictatorship\n(with proofs of the properties that they satisfy). Many important\nproperties of these concepts are also proven – such as the two\nequivalent characterisations of Stochastic Dominance and the fact that\nSD-efficiency of a lottery only depends on the support.  The entry\nalso provides convenient commands to define Preference Profiles, prove\ntheir well-formedness, and automatically derive restrictions that\nsufficiently nice SDSs need to satisfy on the defined profiles.\nCurrently, the formalisation focuses on weak preferences and\nStochastic Dominance, but it should be easy to extend it to other\ndomains – such as strict preferences – or other lottery extensions –\nsuch as Bilinear Dominance or Pairwise Comparison.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-05-05"
            }
        ],
        "dependencies": [
            "List-Index"
        ],
        "theories": [
            "Order_Predicates",
            "Preference_Profiles",
            "Elections",
            "Lotteries",
            "Utility_Functions",
            "Stochastic_Dominance",
            "SD_Efficiency",
            "Social_Decision_Schemes",
            "SDS_Lowering",
            "Random_Dictatorship",
            "Random_Serial_Dictatorship",
            "Randomised_Social_Choice",
            "Preference_Profile_Cmd",
            "files/preference_profiles.ML",
            "QSOpt_Exact",
            "SDS_Automation",
            "files/randomised_social_choice.ML",
            "files/sds_automation.ML"
        ]
    },
    {
        "session": "HyperCTL",
        "title": "A shallow embedding of HyperCTL*",
        "authors": [
            "Markus N. Rabe",
            "Peter Lammich",
            "Andrei Popescu"
        ],
        "date": "2014-04-16",
        "topics": [
            "Computer science/Security",
            "Logic/General logic/Temporal logic"
        ],
        "abstract": "We formalize HyperCTL*, a temporal logic for expressing security properties. We\nfirst define a shallow embedding of HyperCTL*, within which we prove inductive and coinductive\nrules for the operators. Then we show that a HyperCTL* formula captures Goguen-Meseguer\nnoninterference, a landmark information flow property. We also define a deep embedding and\nconnect it to the shallow embedding by a denotational semantics, for which we prove sanity w.r.t.\ndependence on the free variables. Finally, we show that under some finiteness assumptions about\nthe model, noninterference is given by a (finitary) syntactic formula.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2014-04-16"
            }
        ],
        "theories": [
            "Prelim",
            "Shallow",
            "Noninterference",
            "Deep",
            "Finite_Noninterference",
            "HyperCTL"
        ]
    },
    {
        "session": "Higher_Order_Terms",
        "title": "An Algebra for Higher-Order Terms",
        "authors": [
            "Lars Hupel"
        ],
        "contributors": [
            "Yu Zhang"
        ],
        "topics": [
            "Computer science/Programming languages/Lambda calculi"
        ],
        "date": "2019-01-15",
        "abstract": "\nIn this formalization, I introduce a higher-order term algebra,\ngeneralizing the notions of free variables, matching, and\nsubstitution. The need arose from the work on a <a\nhref=\"http://dx.doi.org/10.1007/978-3-319-89884-1_35\">verified\ncompiler from Isabelle to CakeML</a>. Terms can be thought of as\nconsisting of a generic (free variables, constants, application) and\na specific part. As example applications, this entry provides\ninstantiations for de-Bruijn terms, terms with named variables, and\n<a\nhref=\"https://www.isa-afp.org/entries/Lambda_Free_RPOs.html\">Blanchette’s\n&lambda;-free higher-order terms</a>. Furthermore, I\nimplement translation functions between de-Bruijn terms and named\nterms and prove their correctness.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2019-01-15"
            }
        ],
        "dependencies": [
            "Datatype_Order_Generator",
            "List-Index",
            "Lambda_Free_RPOs"
        ],
        "theories": [
            "Term_Utils",
            "Find_First",
            "Name",
            "Fresh_Monad",
            "Fresh_Class",
            "Term_Class",
            "Term",
            "Pats",
            "Nterm",
            "Term_to_Nterm",
            "Unification_Compat",
            "Lambda_Free_Compat"
        ]
    },
    {
        "session": "Propositional_Proof_Systems",
        "title": "Propositional Proof Systems",
        "authors": [
            "Julius Michaelis",
            "Tobias Nipkow"
        ],
        "topics": [
            "Logic/Proof theory"
        ],
        "date": "2017-06-21",
        "abstract": "\nWe formalize a range of proof systems for classical propositional\nlogic (sequent calculus, natural deduction, Hilbert systems,\nresolution) and prove the most important meta-theoretic results about\nsemantics and proofs: compactness, soundness, completeness,\ntranslations between proof systems, cut-elimination, interpolation and\nmodel existence.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2017-06-22"
            }
        ],
        "theories": [
            "Formulas",
            "Sema",
            "Substitution",
            "Substitution_Sema",
            "CNF",
            "CNF_Formulas",
            "CNF_Sema",
            "CNF_Formulas_Sema",
            "CNF_To_Formula",
            "Tseytin",
            "Tseytin_Sema",
            "MiniFormulas",
            "MiniFormulas_Sema",
            "Consistency",
            "Compactness",
            "Compactness_Consistency",
            "Sema_Craig",
            "SC",
            "SC_Cut",
            "SC_Depth",
            "SC_Gentzen",
            "SC_Sema",
            "SC_Depth_Limit",
            "SC_Compl_Consistency",
            "ND",
            "ND_Sound",
            "ND_Compl_Truthtable",
            "ND_Compl_Truthtable_Compact",
            "HC",
            "HC_Compl_Consistency",
            "Resolution",
            "Resolution_Sound",
            "Resolution_Compl",
            "Resolution_Compl_Consistency",
            "HCSC",
            "SCND",
            "NDHC",
            "HCSCND",
            "LSC",
            "LSC_Resolution",
            "ND_FiniteAssms",
            "ND_Compl_SC",
            "Resolution_Compl_SC_Small",
            "Resolution_Compl_SC_Full",
            "MiniSC",
            "MiniSC_HC",
            "MiniSC_Craig"
        ]
    },
    {
        "session": "Dependent_SIFUM_Type_Systems",
        "title": "A Dependent Security Type System for Concurrent Imperative Programs",
        "authors": [
            "Toby Murray",
            "Robert Sison",
            "Edward Pierzchalski",
            "Christine Rizkallah"
        ],
        "date": "2016-06-25",
        "topics": [
            "Computer science/Security",
            "Computer science/Programming languages/Type systems"
        ],
        "abstract": "\nThe paper \"Compositional Verification and Refinement of Concurrent\nValue-Dependent Noninterference\" by Murray et. al. (CSF 2016) presents\na dependent security type system for compositionally verifying a\nvalue-dependent noninterference property, defined in (Murray, PLAS\n2015), for concurrent programs. This development formalises that\nsecurity definition, the type system and its soundness proof, and\ndemonstrates its application on some small examples. It was derived\nfrom the SIFUM_Type_Systems AFP entry, by Sylvia Grewe, Heiko Mantel\nand Daniel Schoepe, and whose structure it inherits.",
        "extra": {
            "Change history": "[2016-08-19]\nRemoved unused \"stop\" parameter and \"stop_no_eval\" assumption from the sifum_security locale.\n(revision dbc482d36372)\n[2016-09-27]\nAdded security locale support for the imposition of requirements on the initial memory.\n(revision cce4ceb74ddb)"
        },
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-06-25"
            }
        ],
        "theories": [
            "Preliminaries",
            "Security",
            "Compositionality",
            "Language",
            "TypeSystem",
            "LocallySoundModeUse",
            "Example",
            "TypeSystemTactics",
            "Example_TypeSystem",
            "Example_Swap_Add"
        ]
    },
    {
        "session": "Falling_Factorial_Sum",
        "title": "The Falling Factorial of a Sum",
        "authors": [
            "Lukas Bulwahn"
        ],
        "topics": [
            "Mathematics/Combinatorics"
        ],
        "date": "2017-12-22",
        "abstract": "\nThis entry shows that the falling factorial of a sum can be computed\nwith an expression using binomial coefficients and the falling\nfactorial of its summands. The entry provides three different proofs:\na combinatorial proof, an induction proof and an algebraic proof using\nthe Vandermonde identity.  The three formalizations try to follow\ntheir informal presentations from a Mathematics Stack Exchange page as\nclose as possible. The induction and algebraic formalization end up to\nbe very close to their informal presentation, whereas the\ncombinatorial proof first requires the introduction of list\ninterleavings, and significant more detail than its informal\npresentation.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-12-30"
            }
        ],
        "dependencies": [
            "Discrete_Summation",
            "Card_Partitions"
        ],
        "theories": [
            "Falling_Factorial_Sum_Combinatorics",
            "Falling_Factorial_Sum_Induction",
            "Falling_Factorial_Sum_Vandermonde"
        ]
    },
    {
        "session": "Menger",
        "title": "Menger's Theorem",
        "authors": [
            "Christoph Dittmann"
        ],
        "topics": [
            "Mathematics/Graph theory"
        ],
        "date": "2017-02-26",
        "abstract": "\nWe present a formalization of Menger's Theorem for directed and\nundirected graphs in Isabelle/HOL.  This well-known result shows that\nif two non-adjacent distinct vertices u, v in a directed graph have no\nseparator smaller than n, then there exist n internally\nvertex-disjoint paths from u to v.  The version for undirected graphs\nfollows immediately because undirected graphs are a special case of\ndirected graphs.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2017-02-27"
            }
        ],
        "theories": [
            "Helpers",
            "Graph",
            "Separations",
            "DisjointPaths",
            "MengerInduction",
            "Y_eq_new_last",
            "Y_neq_new_last",
            "Menger"
        ]
    },
    {
        "session": "Relational_Paths",
        "title": "Relational Characterisations of Paths",
        "authors": [
            "Walter Guttmann",
            "Peter Höfner"
        ],
        "topics": [
            "Mathematics/Graph theory"
        ],
        "date": "2020-07-13",
        "abstract": "\nBinary relations are one of the standard ways to encode, characterise\nand reason about graphs. Relation algebras provide equational axioms\nfor a large fragment of the calculus of binary relations. Although\nrelations are standard tools in many areas of mathematics and\ncomputing, researchers usually fall back to point-wise reasoning when\nit comes to arguments about paths in a graph. We present a purely\nalgebraic way to specify different kinds of paths in Kleene relation\nalgebras, which are relation algebras equipped with an operation for\nreflexive transitive closure. We study the relationship between paths\nwith a designated root vertex and paths without such a vertex. Since\nwe stay in first-order logic this development helps with mechanising\nproofs. To demonstrate the applicability of the algebraic framework we\nverify the correctness of three basic graph algorithms.",
        "licence": "BSD",
        "dependencies": [
            "Relation_Algebra"
        ],
        "theories": [
            "More_Relation_Algebra",
            "Paths",
            "Rooted_Paths",
            "Path_Algorithms"
        ]
    },
    {
        "session": "LTL",
        "title": "Linear Temporal Logic",
        "authors": [
            "Salomon Sickert"
        ],
        "contributors": [
            "Benedikt Seidl"
        ],
        "date": "2016-03-01",
        "topics": [
            "Logic/General logic/Temporal logic",
            "Computer science/Automata and formal languages"
        ],
        "abstract": "\nThis theory provides a formalisation of linear temporal logic (LTL)\nand unifies previous formalisations within the AFP. This entry\nestablishes syntax and semantics for this logic and decouples it from\nexisting entries, yielding a common environment for theories reasoning\nabout LTL. Furthermore a parser written in SML and an executable\nsimplifier are provided.",
        "extra": {
            "Change history": "[2019-03-12]\nSupport for additional operators, implementation of common equivalence relations,\ndefinition of syntactic fragments of LTL and the minimal disjunctive normal form. <br>"
        },
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-03-02"
            }
        ],
        "dependencies": [
            "Boolean_Expression_Checkers"
        ],
        "theories": [
            "LTL",
            "Rewriting",
            "Equivalence_Relations",
            "Disjunctive_Normal_Form",
            "Code_Equations",
            "Example"
        ]
    },
    {
        "session": "Pi_Calculus",
        "title": "The pi-calculus in nominal logic",
        "authors": [
            "Jesper Bengtson"
        ],
        "date": "2012-05-29",
        "topics": [
            "Computer science/Concurrency/Process calculi"
        ],
        "abstract": "We formalise the pi-calculus using the nominal datatype package, based on ideas from the nominal logic by Pitts et al., and demonstrate an implementation in Isabelle/HOL. The purpose is to derive powerful induction rules for the semantics in order to conduct machine checkable proofs, closely following the intuitive arguments found in manual proofs. In this way we have covered many of the standard theorems of bisimulation equivalence and congruence, both late and early, and both strong and weak in a uniform manner. We thus provide one of the most extensive formalisations of a the pi-calculus ever done inside a theorem prover.\n<p>\nA significant gain in our formulation is that agents are identified up to alpha-equivalence, thereby greatly reducing the arguments about bound names. This is a normal strategy for manual proofs about the pi-calculus, but that kind of hand waving has previously been difficult to incorporate smoothly in an interactive theorem prover. We show how the nominal logic formalism and its support in Isabelle accomplishes this and thus significantly reduces the tedium of conducting completely formal proofs. This improves on previous work using weak higher order abstract syntax since we do not need extra assumptions to filter out exotic terms and can keep all arguments within a familiar first-order logic.\n<p>\nThis entry is described in detail in <a href=\"http://www.itu.dk/people/jebe/files/thesis.pdf\">Bengtson's thesis</a>.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-06-14"
            }
        ],
        "theories": [
            "Agent",
            "Late_Semantics",
            "Late_Semantics1",
            "Rel",
            "Strong_Late_Sim",
            "Strong_Late_Bisim",
            "Strong_Late_Bisim_Subst",
            "Strong_Late_Sim_Pres",
            "Strong_Late_Bisim_Pres",
            "Strong_Late_Bisim_Subst_Pres",
            "Late_Tau_Chain",
            "Weak_Late_Step_Semantics",
            "Weak_Late_Semantics",
            "Weak_Late_Sim",
            "Weak_Late_Bisim",
            "Weak_Late_Step_Sim",
            "Weak_Late_Cong",
            "Weak_Late_Bisim_Subst",
            "Weak_Late_Cong_Subst",
            "Strong_Late_Sim_SC",
            "Strong_Late_Bisim_SC",
            "Strong_Late_Bisim_Subst_SC",
            "Weak_Late_Cong_Subst_SC",
            "Weak_Late_Step_Sim_Pres",
            "Weak_Late_Bisim_SC",
            "Weak_Late_Sim_Pres",
            "Weak_Late_Bisim_Pres",
            "Weak_Late_Cong_Pres",
            "Early_Semantics",
            "Strong_Early_Sim",
            "Strong_Early_Bisim",
            "Strong_Early_Bisim_Subst",
            "Strong_Early_Sim_Pres",
            "Strong_Early_Bisim_Pres",
            "Strong_Early_Bisim_Subst_Pres",
            "Early_Tau_Chain",
            "Weak_Early_Step_Semantics",
            "Weak_Early_Semantics",
            "Weak_Early_Sim",
            "Weak_Early_Bisim",
            "Weak_Early_Step_Sim",
            "Weak_Early_Cong",
            "Weak_Early_Bisim_Subst",
            "Weak_Early_Cong_Subst",
            "Weak_Early_Step_Sim_Pres",
            "Weak_Early_Sim_Pres",
            "Strong_Early_Late_Comp",
            "Strong_Early_Bisim_SC",
            "Weak_Early_Bisim_SC",
            "Weak_Early_Bisim_Pres",
            "Weak_Early_Cong_Pres",
            "Weak_Early_Cong_Subst_Pres",
            "Strong_Late_Expansion_Law",
            "Strong_Late_Axiomatisation"
        ]
    },
    {
        "session": "VolpanoSmith",
        "title": "A Correctness Proof for the Volpano/Smith Security Typing System",
        "authors": [
            "Gregor Snelting",
            "Daniel Wasserrab"
        ],
        "date": "2008-09-02",
        "topics": [
            "Computer science/Programming languages/Type systems",
            "Computer science/Security"
        ],
        "abstract": "The Volpano/Smith/Irvine security type systems requires that variables are annotated as high (secret) or low (public), and provides typing rules which guarantee that secret values cannot leak to public output ports. This property of a program is called confidentiality. For a simple while-language without threads, our proof shows that typeability in the Volpano/Smith system guarantees noninterference. Noninterference means that if two initial states for program execution are low-equivalent, then the final states are low-equivalent as well. This indeed implies that secret values cannot leak to public ports. The proof defines an abstract syntax and operational semantics for programs, formalizes noninterference, and then proceeds by rule induction on the operational semantics. The mathematically most intricate part is the treatment of implicit flows. Note that the Volpano/Smith system is not flow-sensitive and thus quite unprecise, resulting in false alarms. However, due to the correctness property, all potential breaks of confidentiality are discovered.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-24"
            },
            {
                "2011-1": "2011-10-11"
            },
            {
                "2011": "2011-02-11"
            },
            {
                "2009-2": "2010-07-01"
            },
            {
                "2009-1": "2009-12-12"
            },
            {
                "2009": "2009-04-29"
            },
            {
                "2008": "2008-09-05"
            }
        ],
        "theories": [
            "Semantics",
            "secTypes",
            "Execute"
        ]
    },
    {
        "session": "CakeML",
        "title": "CakeML",
        "authors": [
            "Lars Hupel",
            "Yu Zhang"
        ],
        "contributors": [
            "Johannes Åman Pohjola"
        ],
        "topics": [
            "Computer science/Programming languages/Language definitions"
        ],
        "date": "2018-03-12",
        "abstract": "\nCakeML is a functional programming language with a proven-correct\ncompiler and runtime system. This entry contains an unofficial version\nof the CakeML semantics that has been exported from the Lem\nspecifications to Isabelle. Additionally, there are some hand-written\ntheory files that adapt the exported code to Isabelle and port proofs\nfrom the HOL4 formalization, e.g. termination and equivalence proofs.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            }
        ],
        "dependencies": [
            "LEM",
            "Coinductive",
            "IEEE_Floating_Point",
            "Show"
        ],
        "theories": [
            "Doc_Generated",
            "Lib",
            "Namespace",
            "FpSem",
            "Ast",
            "AstAuxiliary",
            "Ffi",
            "SemanticPrimitives",
            "SmallStep",
            "BigStep",
            "BigSmallInvariants",
            "Evaluate",
            "LibAuxiliary",
            "NamespaceAuxiliary",
            "PrimTypes",
            "SemanticPrimitivesAuxiliary",
            "SimpleIO",
            "Tokens",
            "TypeSystem",
            "TypeSystemAuxiliary",
            "Doc_Proofs",
            "Semantic_Extras",
            "Evaluate_Termination",
            "Evaluate_Clock",
            "Evaluate_Single",
            "Big_Step_Determ",
            "Big_Step_Total",
            "Big_Step_Fun_Equiv",
            "Big_Step_Unclocked",
            "Big_Step_Clocked",
            "Big_Step_Unclocked_Single",
            "Matching",
            "CakeML_Code",
            "CakeML_Quickcheck",
            "CakeML_Compiler",
            "files/Tools/cakeml_sexp.ML",
            "files/Tools/cakeml_compiler.ML",
            "Compiler_Test",
            "Code_Test_Haskell"
        ]
    },
    {
        "session": "Regression_Test_Selection",
        "title": "Regression Test Selection",
        "authors": [
            "Susannah Mansky"
        ],
        "topics": [
            "Computer science/Algorithms"
        ],
        "date": "2021-04-30",
        "abstract": "\nThis development provides a general definition for safe Regression\nTest Selection (RTS) algorithms. RTS algorithms select which tests to\nrerun on revised code, reducing the time required to check for newly\nintroduced errors. An RTS algorithm is considered safe if and only if\nall deselected tests would have unchanged results.  This definition is\ninstantiated with two class-collection-based RTS algorithms run over\nthe JVM as modeled by JinjaDCI. This is achieved with a general\ndefinition for Collection Semantics, small-step semantics instrumented\nto collect information during execution. As the RTS definition\nmandates safety, these instantiations include proofs of safety.  This\nwork is described in Mansky and Gunter's LSFA 2020 paper and\nMansky's doctoral thesis (UIUC, 2020).",
        "licence": "BSD",
        "dependencies": [
            "JinjaDCI"
        ],
        "theories": [
            "RTS_safe",
            "Semantics",
            "CollectionSemantics",
            "CollectionBasedRTS",
            "JVMSemantics",
            "ClassesChanged",
            "Subcls",
            "ClassesAbove",
            "JVMCollectionSemantics",
            "JVMExecStepInductive",
            "JVMCollectionBasedRTS",
            "RTS"
        ]
    },
    {
        "session": "LocalLexing",
        "title": "Local Lexing",
        "authors": [
            "Steven Obua"
        ],
        "topics": [
            "Computer science/Automata and formal languages"
        ],
        "date": "2017-04-28",
        "abstract": "\nThis formalisation accompanies the paper <a\nhref=\"https://arxiv.org/abs/1702.03277\">Local\nLexing</a> which introduces a novel parsing concept of the same\nname. The paper also gives a high-level algorithm for local lexing as\nan extension of Earley's algorithm. This formalisation proves the\nalgorithm to be correct with respect to its local lexing semantics. As\na special case, this formalisation thus also contains a proof of the\ncorrectness of Earley's algorithm. The paper contains a short\noutline of how this formalisation is organised.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2017-04-28"
            }
        ],
        "theories": [
            "CFG",
            "LocalLexing",
            "LLEarleyParsing",
            "Limit",
            "LocalLexingLemmas",
            "InductRules",
            "ListTools",
            "Derivations",
            "Validity",
            "TheoremD2",
            "TheoremD4",
            "TheoremD5",
            "TheoremD6",
            "TheoremD7",
            "TheoremD8",
            "TheoremD9",
            "Ladder",
            "TheoremD10",
            "TheoremD11",
            "TheoremD12",
            "TheoremD13",
            "TheoremD14",
            "PathLemmas",
            "MainTheorems"
        ]
    },
    {
        "session": "Floyd_Warshall",
        "title": "The Floyd-Warshall Algorithm for Shortest Paths",
        "authors": [
            "Simon Wimmer",
            "Peter Lammich"
        ],
        "topics": [
            "Computer science/Algorithms/Graph"
        ],
        "date": "2017-05-08",
        "abstract": "\nThe Floyd-Warshall algorithm [Flo62, Roy59, War62] is a classic\ndynamic programming algorithm to compute the length of all shortest\npaths between any two vertices in a graph (i.e. to solve the all-pairs\nshortest path problem, or APSP for short). Given a representation of\nthe graph as a matrix of weights M, it computes another matrix M'\nwhich represents a graph with the same path lengths and contains the\nlength of the shortest path between any two vertices i and j. This is\nonly possible if the graph does not contain any negative cycles.\nHowever, in this case the Floyd-Warshall algorithm will detect the\nsituation by calculating a negative diagonal entry. This entry\nincludes a formalization of the algorithm and of these key properties.\nThe algorithm is refined to an efficient imperative version using the\nImperative Refinement Framework.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2017-05-09"
            }
        ],
        "dependencies": [
            "Refine_Imperative_HOL"
        ],
        "theories": [
            "Floyd_Warshall",
            "Recursion_Combinators",
            "FW_Code"
        ]
    },
    {
        "session": "Algebraic_VCs",
        "title": "Program Construction and Verification Components Based on Kleene Algebra",
        "authors": [
            "Victor B. F. Gomes",
            "Georg Struth"
        ],
        "date": "2016-06-18",
        "topics": [
            "Mathematics/Algebra"
        ],
        "abstract": "\nVariants of Kleene algebra support program construction and\nverification by algebraic reasoning. This entry provides a\nverification component for Hoare logic based on Kleene algebra with\ntests, verification components for weakest preconditions and strongest\npostconditions based on Kleene algebra with domain and a component for\nstep-wise refinement based on refinement Kleene algebra with tests. In\naddition to these components for the partial correctness of while\nprograms, a verification component for total correctness based on\ndivergence Kleene algebras and one for (partial correctness) of\nrecursive programs based on domain quantales are provided. Finally we\nhave integrated memory models for programs with pointers and a program\ntrace semantics into the weakest precondition component.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-06-18"
            }
        ],
        "dependencies": [
            "KAT_and_DRA",
            "KAD"
        ],
        "theories": [
            "VC_KAT_scratch",
            "VC_KAD_scratch",
            "P2S2R",
            "VC_KAT",
            "VC_KAT_Examples",
            "VC_KAT_Examples2",
            "RKAT",
            "RKAT_Models",
            "VC_RKAT",
            "VC_RKAT_Examples",
            "VC_KAD",
            "VC_KAD_Examples",
            "VC_KAD_Examples2",
            "VC_KAD_dual",
            "VC_KAD_dual_Examples",
            "VC_KAD_wf",
            "VC_KAD_wf_Examples",
            "Path_Model_Example",
            "Pointer_Examples",
            "KAD_is_KAT",
            "Domain_Quantale"
        ]
    },
    {
        "session": "FileRefinement",
        "title": "File Refinement",
        "authors": [
            "Karen Zee",
            "Viktor Kuncak"
        ],
        "date": "2004-12-09",
        "topics": [
            "Computer science/Data structures"
        ],
        "abstract": "These theories illustrates the verification of basic file operations (file creation, file read and file write) in the Isabelle theorem prover. We describe a file at two levels of abstraction: an abstract file represented as a resizable array, and a concrete file represented using data blocks.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-24"
            },
            {
                "2011-1": "2011-10-11"
            },
            {
                "2011": "2011-02-11"
            },
            {
                "2009-2": "2010-06-30"
            },
            {
                "2009-1": "2009-12-12"
            },
            {
                "2009": "2009-04-29"
            },
            {
                "2008": "2008-06-10"
            },
            {
                "2007": "2007-11-27"
            },
            {
                "2005": "2005-10-14"
            },
            {
                "2004": "2004-12-15"
            }
        ],
        "theories": [
            "CArrays",
            "ResizableArrays",
            "FileRefinement"
        ]
    },
    {
        "session": "Banach_Steinhaus",
        "title": "Banach-Steinhaus Theorem",
        "authors": [
            "Dominique Unruh",
            "José Manuel Rodríguez Caballero"
        ],
        "topics": [
            "Mathematics/Analysis"
        ],
        "date": "2020-05-02",
        "abstract": "\nWe formalize in Isabelle/HOL a result\ndue to S. Banach and H. Steinhaus known as\nthe Banach-Steinhaus theorem or Uniform boundedness principle: a\npointwise-bounded family of continuous linear operators from a Banach\nspace to a normed space is uniformly bounded. Our approach is an\nadaptation to Isabelle/HOL of a proof due to A. Sokal.",
        "licence": "BSD",
        "theories": [
            "Banach_Steinhaus_Missing",
            "Banach_Steinhaus"
        ]
    },
    {
        "session": "ConcurrentIMP",
        "title": "Concurrent IMP",
        "authors": [
            "Peter Gammie"
        ],
        "date": "2015-04-13",
        "topics": [
            "Computer science/Programming languages/Logics"
        ],
        "abstract": "\nConcurrentIMP extends the small imperative language IMP with control\nnon-determinism and constructs for synchronous message passing.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2015-04-15"
            }
        ],
        "theories": [
            "CIMP_pred",
            "Infinite_Sequences",
            "LTL",
            "CIMP_lang",
            "CIMP_vcg",
            "CIMP_vcg_rules",
            "CIMP",
            "files/cimp.ML",
            "CIMP_locales",
            "CIMP_one_place_buffer",
            "CIMP_unbounded_buffer"
        ]
    },
    {
        "session": "Modular_arithmetic_LLL_and_HNF_algorithms",
        "title": "Two algorithms based on modular arithmetic: lattice basis reduction and Hermite normal form computation",
        "authors": [
            "Ralph Bottesch",
            "Jose Divasón",
            "René Thiemann"
        ],
        "topics": [
            "Computer science/Algorithms/Mathematical"
        ],
        "date": "2021-03-12",
        "abstract": "\nWe verify two algorithms for which modular arithmetic plays an\nessential role: Storjohann's variant of the LLL lattice basis\nreduction algorithm and Kopparty's algorithm for computing the\nHermite normal form of a matrix. To do this, we also formalize some\nfacts about the modulo operation with symmetric range. Our\nimplementations are based on the original papers, but are otherwise\nefficient. For basis reduction we formalize two versions: one that\nincludes all of the optimizations/heuristics from Storjohann's\npaper, and one excluding a heuristic that we observed to often\ndecrease efficiency. We also provide a fast, self-contained certifier\nfor basis reduction, based on the efficient Hermite normal form\nalgorithm.",
        "licence": "BSD",
        "dependencies": [
            "Smith_Normal_Form",
            "LLL_Basis_Reduction",
            "Show",
            "Jordan_Normal_Form",
            "Hermite"
        ],
        "theories": [
            "Matrix_Change_Row",
            "Signed_Modulo",
            "Storjohann_Mod_Operation",
            "Storjohann",
            "Storjohann_Impl",
            "Uniqueness_Hermite",
            "Uniqueness_Hermite_JNF",
            "HNF_Mod_Det_Algorithm",
            "HNF_Mod_Det_Soundness",
            "LLL_Certification_via_HNF"
        ]
    },
    {
        "session": "Routing",
        "title": "Routing",
        "authors": [
            "Julius Michaelis",
            "Cornelius Diekmann"
        ],
        "date": "2016-08-31",
        "topics": [
            "Computer science/Networks"
        ],
        "abstract": "\nThis entry contains definitions for routing with routing\ntables/longest prefix matching.  A routing table entry is modelled as\na record of a prefix match, a metric, an output port, and an optional\nnext hop. A routing table is a list of entries, sorted by prefix\nlength and metric. Additionally, a parser and serializer for the\noutput of the ip-route command, a function to create a relation from\noutput port to corresponding destination IP space, and a model of a\nLinux-style router are included.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-08-31"
            }
        ],
        "dependencies": [
            "Simple_Firewall"
        ],
        "theories": [
            "Linorder_Helper",
            "Routing_Table",
            "Linux_Router",
            "IpRoute_Parser",
            "files/IpRoute_Parser.ml"
        ]
    },
    {
        "session": "Ordinal",
        "title": "Countable Ordinals",
        "authors": [
            "Brian Huffman"
        ],
        "date": "2005-11-11",
        "topics": [
            "Logic/Set theory"
        ],
        "abstract": "This development defines a well-ordered type of countable ordinals. It includes notions of continuous and normal functions, recursively defined functions over ordinals, least fixed-points, and derivatives. Much of ordinal arithmetic is formalized, including exponentials and logarithms. The development concludes with formalizations of Cantor Normal Form and Veblen hierarchies over normal functions.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-24"
            },
            {
                "2011-1": "2011-10-11"
            },
            {
                "2011": "2011-02-11"
            },
            {
                "2009-2": "2010-07-01"
            },
            {
                "2009-1": "2009-12-12"
            },
            {
                "2009": "2009-04-29"
            },
            {
                "2008": "2008-06-10"
            },
            {
                "2007": "2007-11-27"
            },
            {
                "2005": "2005-11-16"
            }
        ],
        "theories": [
            "OrdinalDef",
            "OrdinalInduct",
            "OrdinalCont",
            "OrdinalRec",
            "OrdinalArith",
            "OrdinalInverse",
            "OrdinalFix",
            "OrdinalOmega",
            "OrdinalVeblen",
            "Ordinal"
        ]
    },
    {
        "session": "Flyspeck-Tame",
        "title": "Flyspeck I: Tame Graphs",
        "authors": [
            "Gertrud Bauer",
            "Tobias Nipkow"
        ],
        "date": "2006-05-22",
        "topics": [
            "Mathematics/Graph theory"
        ],
        "abstract": "\nThese theories present the verified enumeration of <i>tame</i> plane graphs\nas defined by Thomas C. Hales in his proof of the Kepler Conjecture in his\nbook <i>Dense Sphere Packings. A Blueprint for Formal Proofs.</i> [CUP 2012].\nThe values of the constants in the definition of tameness are identical to\nthose in the <a href=\"https://code.google.com/p/flyspeck/\">Flyspeck project</a>.\nThe <a href=\"http://www21.in.tum.de/~nipkow/pubs/Flyspeck/\">IJCAR 2006 paper by Nipkow, Bauer and Schultz</a> refers to the original version of Hales' proof,\nthe <a href=\"http://www21.in.tum.de/~nipkow/pubs/itp11.html\">ITP 2011 paper by Nipkow</a> refers to the Blueprint version of the proof.",
        "extra": {
            "Change history": "[2010-11-02] modified theories to reflect the modified definition of tameness in Hales' revised proof.<br>\n[2014-07-03] modified constants in def of tameness and Archive according to the final state of the Flyspeck proof."
        },
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-17"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-03-02"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-25"
            },
            {
                "2011-1": "2011-10-11"
            },
            {
                "2011": "2011-02-11"
            },
            {
                "2009-2": "2010-06-30"
            },
            {
                "2009-1": "2009-12-12"
            },
            {
                "2009": "2009-04-29"
            },
            {
                "2008": "2008-06-10"
            },
            {
                "2007": "2008-01-04"
            },
            {
                "2007": "2007-11-27"
            }
        ],
        "dependencies": [
            "Trie"
        ],
        "theories": [
            "ListAux",
            "Quasi_Order",
            "PlaneGraphIso",
            "Rotation",
            "Graph",
            "IArray_Syntax",
            "Enumerator",
            "FaceDivision",
            "RTranCl",
            "Plane",
            "Plane1",
            "GraphProps",
            "EnumeratorProps",
            "FaceDivisionProps",
            "Invariants",
            "PlaneProps",
            "ListSum",
            "Tame",
            "Plane1Props",
            "Generator",
            "TameProps",
            "TameEnum",
            "ScoreProps",
            "LowerBound",
            "GeneratorProps",
            "TameEnumProps",
            "Worklist",
            "Maps",
            "Arch",
            "ArchCompAux",
            "ArchCompProps",
            "Relative_Completeness"
        ]
    },
    {
        "session": "Noninterference_Generic_Unwinding",
        "title": "The Generic Unwinding Theorem for CSP Noninterference Security",
        "authors": [
            "Pasquale Noce"
        ],
        "date": "2015-06-11",
        "topics": [
            "Computer science/Security",
            "Computer science/Concurrency/Process calculi"
        ],
        "abstract": "\n<p>\nThe classical definition of noninterference security for a deterministic state\nmachine with outputs requires to consider the outputs produced by machine\nactions after any trace, i.e. any indefinitely long sequence of actions, of the\nmachine. In order to render the verification of the security of such a machine\nmore straightforward, there is a need of some sufficient condition for security\nsuch that just individual actions, rather than unbounded sequences of actions,\nhave to be considered.\n</p><p>\nBy extending previous results applying to transitive noninterference policies,\nRushby has proven an unwinding theorem that provides a sufficient condition of\nthis kind in the general case of a possibly intransitive policy. This condition\nhas to be satisfied by a generic function mapping security domains into\nequivalence relations over machine states.\n</p><p>\nAn analogous problem arises for CSP noninterference security, whose definition\nrequires to consider any possible future, i.e. any indefinitely long sequence of\nsubsequent events and any indefinitely large set of refused events associated to\nthat sequence, for each process trace.\n</p><p>\nThis paper provides a sufficient condition for CSP noninterference security,\nwhich indeed requires to just consider individual accepted and refused events\nand applies to the general case of a possibly intransitive policy. This\ncondition follows Rushby's one for classical noninterference security, and has\nto be satisfied by a generic function mapping security domains into equivalence\nrelations over process traces; hence its name, Generic Unwinding Theorem.\nVariants of this theorem applying to deterministic processes and trace set\nprocesses are also proven. Finally, the sufficient condition for security\nexpressed by the theorem is shown not to be a necessary condition as well, viz.\nthere exists a secure process such that no domain-relation map satisfying the\ncondition exists.\n</p>",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-06-13"
            }
        ],
        "dependencies": [
            "Noninterference_Ipurge_Unwinding"
        ],
        "theories": [
            "GenericUnwinding"
        ]
    },
    {
        "session": "Epistemic_Logic",
        "title": "Epistemic Logic",
        "authors": [
            "Asta Halkjær From"
        ],
        "topics": [
            "Logic/General logic/Logics of knowledge and belief"
        ],
        "date": "2018-10-29",
        "abstract": "\nThis work is a formalization of epistemic logic with countably many\nagents. It includes proofs of soundness and completeness for the axiom\nsystem K. The completeness proof is based on the textbook\n\"Reasoning About Knowledge\" by Fagin, Halpern, Moses and\nVardi (MIT Press 1995).",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-10-29"
            }
        ],
        "theories": [
            "Epistemic_Logic"
        ]
    },
    {
        "session": "Stone_Relation_Algebras",
        "title": "Stone Relation Algebras",
        "authors": [
            "Walter Guttmann"
        ],
        "topics": [
            "Mathematics/Algebra"
        ],
        "date": "2017-02-07",
        "abstract": "\nWe develop Stone relation algebras, which generalise relation algebras\nby replacing the underlying Boolean algebra structure with a Stone\nalgebra. We show that finite matrices over extended real numbers form\nan instance. As a consequence, relation-algebraic concepts and methods\ncan be used for reasoning about weighted graphs. We also develop a\nfixpoint calculus and apply it to compare different definitions of\nreflexive-transitive closures in semirings.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2017-02-09"
            }
        ],
        "dependencies": [
            "Stone_Algebras"
        ],
        "theories": [
            "Fixpoints",
            "Semirings",
            "Relation_Algebras",
            "Relation_Subalgebras",
            "Matrix_Relation_Algebras",
            "Linear_Order_Matrices"
        ]
    },
    {
        "session": "List_Inversions",
        "title": "The Inversions of a List",
        "authors": [
            "Manuel Eberl"
        ],
        "topics": [
            "Computer science/Algorithms"
        ],
        "date": "2019-02-01",
        "abstract": "\n<p>This entry defines the set of <em>inversions</em>\nof a list, i.e. the pairs of indices that violate sortedness. It also\nproves the correctness of the well-known\n<em>O</em>(<em>n log n</em>)\ndivide-and-conquer algorithm to compute the number of\ninversions.</p>",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2019-02-21"
            }
        ],
        "theories": [
            "List_Inversions"
        ]
    },
    {
        "session": "Psi_Calculi",
        "title": "Psi-calculi in Isabelle",
        "authors": [
            "Jesper Bengtson"
        ],
        "date": "2012-05-29",
        "topics": [
            "Computer science/Concurrency/Process calculi"
        ],
        "abstract": "Psi-calculi are extensions of the pi-calculus, accommodating arbitrary nominal datatypes to represent not only data but also communication channels, assertions and conditions, giving it an expressive power beyond the applied pi-calculus and the concurrent constraint pi-calculus.\n<p>\nWe have formalised psi-calculi in the interactive theorem prover Isabelle using its nominal datatype package. One distinctive feature is that the framework needs to treat binding sequences, as opposed to single binders, in an efficient way. While different methods for formalising single binder calculi have been proposed over the last decades, representations for such binding sequences are not very well explored.\n<p>\nThe main effort in the formalisation is to keep the machine checked proofs as close to their pen-and-paper counterparts as possible. This includes treating all binding sequences as atomic elements, and creating custom induction and inversion rules that to remove the bulk of manual alpha-conversions.\n<p>\nThis entry is described in detail in <a href=\"http://www.itu.dk/people/jebe/files/thesis.pdf\">Bengtson's thesis</a>.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-06-14"
            }
        ],
        "theories": [
            "Chain",
            "Subst_Term",
            "Agent",
            "Frame",
            "Semantics",
            "Simulation",
            "Tau_Chain",
            "Weak_Simulation",
            "Weak_Stat_Imp",
            "Bisimulation",
            "Sim_Pres",
            "Bisim_Pres",
            "Sim_Struct_Cong",
            "Structural_Congruence",
            "Bisim_Struct_Cong",
            "Weak_Bisimulation",
            "Weak_Sim_Pres",
            "Weak_Stat_Imp_Pres",
            "Weak_Bisim_Pres",
            "Weak_Bisim_Struct_Cong",
            "Close_Subst",
            "Bisim_Subst",
            "Weak_Bisim_Subst",
            "Weakening",
            "Weaken_Transition",
            "Weaken_Stat_Imp",
            "Weaken_Simulation",
            "Weaken_Bisimulation",
            "Weak_Cong_Simulation",
            "Weak_Psi_Congruence",
            "Weak_Cong_Sim_Pres",
            "Weak_Cong_Pres",
            "Weak_Cong_Struct_Cong",
            "Weak_Congruence",
            "Tau",
            "Sum",
            "Tau_Sim",
            "Tau_Stat_Imp",
            "Tau_Laws_Weak",
            "Tau_Laws_No_Weak"
        ]
    },
    {
        "session": "Ordinary_Differential_Equations",
        "title": "Ordinary Differential Equations",
        "authors": [
            "Fabian Immler",
            "Johannes Hölzl"
        ],
        "topics": [
            "Mathematics/Analysis"
        ],
        "date": "2012-04-26",
        "abstract": "\n<p>Session Ordinary-Differential-Equations formalizes ordinary differential equations (ODEs) and initial value\nproblems. This work comprises proofs for local and global existence of unique solutions\n(Picard-Lindelöf theorem). Moreover, it contains a formalization of the (continuous or even\ndifferentiable) dependency of the flow on initial conditions as the <i>flow</i> of ODEs.</p>\n<p>\nNot in the generated document are the following sessions:\n<ul>\n<li> HOL-ODE-Numerics:\nRigorous numerical algorithms for computing enclosures of solutions based on Runge-Kutta methods\nand affine arithmetic. Reachability analysis with splitting and reduction at hyperplanes.</li>\n<li> HOL-ODE-Examples:\nApplications of the numerical algorithms to concrete systems of ODEs.</li>\n<li> Lorenz_C0, Lorenz_C1:\nVerified algorithms for checking C1-information according to Tucker's proof,\ncomputation of C0-information.</li>\n</ul>\n</p>",
        "extra": {
            "Change history": "[2014-02-13] added an implementation of the Euler method based on affine arithmetic<br>\n[2016-04-14] added flow and variational equation<br>\n[2016-08-03] numerical algorithms for reachability analysis (using second-order Runge-Kutta methods, splitting, and reduction) implemented using Lammich's framework for automatic refinement<br>\n[2017-09-20] added Poincare map and propagation of variational equation in\nreachability analysis, verified algorithms for C1-information and computations\nfor C0-information of the Lorenz attractor."
        },
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-24"
            }
        ],
        "dependencies": [
            "Triangle",
            "List-Index",
            "Affine_Arithmetic"
        ],
        "theories": [
            "ODE_Auxiliarities",
            "MVT_Ex",
            "Vector_Derivative_On",
            "Interval_Integral_HK",
            "Gronwall",
            "Initial_Value_Problem",
            "Picard_Lindeloef_Qualitative",
            "Bounded_Linear_Operator",
            "Multivariate_Taylor",
            "Flow",
            "Upper_Lower_Solution",
            "Poincare_Map",
            "Reachability_Analysis",
            "Flow_Congs",
            "Cones",
            "Linear_ODE",
            "ODE_Analysis"
        ]
    },
    {
        "session": "Safe_Distance",
        "title": "A Formally Verified Checker of the Safe Distance Traffic Rules for Autonomous Vehicles",
        "authors": [
            "Albert Rizaldi",
            "Fabian Immler"
        ],
        "topics": [
            "Computer science/Algorithms/Mathematical",
            "Mathematics/Physics"
        ],
        "date": "2020-06-01",
        "abstract": "\nThe Vienna Convention on Road Traffic defines the safe distance\ntraffic rules informally. This could make autonomous vehicle liable\nfor safe-distance-related accidents because there is no clear\ndefinition of how large a safe distance is. We provide a formally\nproven prescriptive definition of a safe distance, and checkers which\ncan decide whether an autonomous vehicle is obeying the safe distance\nrule. Not only does our work apply to the domain of law, but it also\nserves as a specification for autonomous vehicle manufacturers and for\nonline verification of path planners.",
        "licence": "BSD",
        "dependencies": [
            "Sturm_Sequences"
        ],
        "theories": [
            "Safe_Distance",
            "Safe_Distance_Reaction",
            "Evaluation"
        ]
    },
    {
        "session": "Launchbury",
        "title": "The Correctness of Launchbury's Natural Semantics for Lazy Evaluation",
        "authors": [
            "Joachim Breitner"
        ],
        "date": "2013-01-31",
        "topics": [
            "Computer science/Programming languages/Lambda calculi",
            "Computer science/Semantics"
        ],
        "abstract": "In his seminal paper \"Natural Semantics for Lazy Evaluation\", John Launchbury proves his semantics correct with respect to a denotational semantics, and outlines an adequacy proof. We have formalized both semantics and machine-checked the correctness proof, clarifying some details. Furthermore, we provide a new and more direct adequacy proof that does not require intermediate operational semantics.",
        "extra": {
            "Change history": "[2014-05-24] Added the proof of adequacy, as well as simplified and improved the existing proofs. Adjusted abstract accordingly.\n[2015-03-16] Booleans and if-then-else added to syntax and semantics, making this entry suitable to be used by the entry \"Call_Arity\"."
        },
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2014-05-25"
            },
            {
                "2013-2": "2014-05-24"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-02-25"
            },
            {
                "2013": "2013-02-24"
            }
        ],
        "dependencies": [
            "FinFun",
            "Nominal2"
        ],
        "theories": [
            "AList-Utils",
            "HOLCF-Join",
            "HOLCF-Join-Classes",
            "Env",
            "Pointwise",
            "HOLCF-Utils",
            "EvalHeap",
            "Nominal-Utils",
            "AList-Utils-Nominal",
            "Nominal-HOLCF",
            "Env-HOLCF",
            "HasESem",
            "Iterative",
            "Env-Nominal",
            "HeapSemantics",
            "Vars",
            "Terms",
            "AbstractDenotational",
            "Substitution",
            "Abstract-Denotational-Props",
            "Value",
            "Value-Nominal",
            "Denotational",
            "Launchbury",
            "CorrectnessOriginal",
            "Mono-Nat-Fun",
            "C",
            "CValue",
            "CValue-Nominal",
            "HOLCF-Meet",
            "C-Meet",
            "C-restr",
            "ResourcedDenotational",
            "CorrectnessResourced",
            "ResourcedAdequacy",
            "ValueSimilarity",
            "Denotational-Related",
            "Adequacy",
            "EverythingAdequacy"
        ]
    },
    {
        "session": "Robbins-Conjecture",
        "title": "A Complete Proof of the Robbins Conjecture",
        "authors": [
            "Matthew Wampler-Doty"
        ],
        "date": "2010-05-22",
        "topics": [
            "Mathematics/Algebra"
        ],
        "abstract": "This document gives a formalization of the proof of the Robbins conjecture, following A. Mann, <i>A Complete Proof of the Robbins Conjecture</i>, 2003.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-24"
            },
            {
                "2011-1": "2011-10-11"
            },
            {
                "2011": "2011-02-11"
            },
            {
                "2009-2": "2010-07-01"
            },
            {
                "2009-1": "2010-05-27"
            }
        ],
        "theories": [
            "Robbins_Conjecture"
        ]
    },
    {
        "session": "WOOT_Strong_Eventual_Consistency",
        "title": "Strong Eventual Consistency of the Collaborative Editing Framework WOOT",
        "authors": [
            "Emin Karayel",
            "Edgar Gonzàlez"
        ],
        "topics": [
            "Computer science/Algorithms/Distributed"
        ],
        "date": "2020-03-25",
        "abstract": "\nCommutative Replicated Data Types (CRDTs) are a promising new class of\ndata structures for large-scale shared mutable content in applications\nthat only require eventual consistency. The WithOut Operational\nTransforms (WOOT) framework is a CRDT for collaborative text editing\nintroduced by Oster et al. (CSCW 2006) for which the eventual\nconsistency property was verified only for a bounded model to date. We\ncontribute a formal proof for WOOTs strong eventual consistency.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2020-03-26"
            }
        ],
        "dependencies": [
            "Certification_Monads",
            "Datatype_Order_Generator"
        ],
        "theories": [
            "ErrorMonad",
            "Data",
            "BasicAlgorithms",
            "CreateAlgorithms",
            "IntegrateAlgorithm",
            "DistributedExecution",
            "SortKeys",
            "Psi",
            "Sorting",
            "Consistency",
            "CreateConsistent",
            "IntegrateInsertCommute",
            "StrongConvergence",
            "SEC",
            "Example"
        ]
    },
    {
        "session": "Transitive-Closure-II",
        "title": "Executable Transitive Closures",
        "topics": [
            "Computer science/Algorithms/Graph"
        ],
        "date": "2012-02-29",
        "authors": [
            "René Thiemann"
        ],
        "license": "LGPL",
        "abstract": "\n<p>\nWe provide a generic work-list algorithm to compute the\n(reflexive-)transitive closure of relations where only successors of newly\ndetected states are generated.\nIn contrast to our previous work, the relations do not have to be finite,\nbut each element must only have finitely many (indirect) successors.\nMoreover, a subsumption relation can be used instead of pure equality.\nAn executable variant of the algorithm is available where the generic operations\nare instantiated with list operations.\n</p><p>\nThis formalization was performed as part of the IsaFoR/CeTA project,\nand it has been used to certify size-change\ntermination proofs where large transitive closures have to be computed.\n</p>",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-24"
            },
            {
                "2011-1": "2012-03-15"
            },
            {
                "2011-1": "2012-02-29"
            }
        ],
        "dependencies": [
            "Regular-Sets"
        ],
        "theories": [
            "RTrancl"
        ]
    },
    {
        "session": "Monad_Normalisation",
        "title": "Monad normalisation",
        "authors": [
            "Joshua Schneider",
            "Manuel Eberl",
            "Andreas Lochbihler"
        ],
        "topics": [
            "Tools",
            "Computer science/Functional programming",
            "Logic/Rewriting"
        ],
        "date": "2017-05-05",
        "abstract": "\nThe usual monad laws can directly be used as rewrite rules for Isabelle’s\nsimplifier to normalise monadic HOL terms and decide equivalences.\nIn a commutative monad, however, the commutativity law is a\nhigher-order permutative rewrite rule that makes the simplifier loop.\nThis AFP entry implements a simproc that normalises monadic\nexpressions in commutative monads using ordered rewriting. The\nsimproc can also permute computations across control operators like if\nand case.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2017-05-11"
            }
        ],
        "theories": [
            "Monad_Normalisation",
            "files/monad_rules.ML",
            "files/monad_normalisation.ML",
            "Monad_Normalisation_Test"
        ]
    },
    {
        "session": "Promela",
        "title": "Promela Formalization",
        "authors": [
            "René Neumann"
        ],
        "date": "2014-05-28",
        "topics": [
            "Computer science/System description languages"
        ],
        "abstract": "\nWe present an executable formalization of the language Promela, the\ndescription language for models of the model checker SPIN. This\nformalization is part of the work for a completely verified model\nchecker (CAVA), but also serves as a useful (and executable!)\ndescription of the semantics of the language itself, something that is\ncurrently missing.\nThe formalization uses three steps: It takes an abstract syntax tree\ngenerated from an SML parser, removes syntactic sugar and enriches it\nwith type information. This further gets translated into a transition\nsystem, on which the semantic engine (read: successor function) operates.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2014-05-29"
            }
        ],
        "dependencies": [
            "CAVA_Automata",
            "LTL"
        ],
        "theories": [
            "PromelaStatistics",
            "PromelaAST",
            "PromelaDatastructures",
            "PromelaInvariants",
            "Promela",
            "PromelaLTL",
            "PromelaLTLConv",
            "All_Of_Promela"
        ]
    },
    {
        "session": "Native_Word",
        "title": "Native Word",
        "authors": [
            "Andreas Lochbihler"
        ],
        "contributors": [
            "Peter Lammich"
        ],
        "date": "2013-09-17",
        "topics": [
            "Computer science/Data structures"
        ],
        "abstract": "This entry makes machine words and machine arithmetic available for code generation from Isabelle/HOL.  It provides a common abstraction that hides the differences between the different target languages.  The code generator maps these operations to the APIs of the target languages.  Apart from that, we extend the available bit operations on types int and integer, and map them to the operations in the target languages.",
        "extra": {
            "Change history": "[2013-11-06]\nadded conversion function between native words and characters\n(revision fd23d9a7fe3a)<br>\n[2014-03-31]\nadded words of default size in the target language (by Peter Lammich)\n(revision 25caf5065833)<br>\n[2014-10-06]\nproper test setup with compilation and execution of tests in all target languages\n(revision 5d7a1c9ae047)<br>\n[2017-09-02]\nadded 64-bit words (revision c89f86244e3c)<br>\n[2018-07-15]\nadded cast operators for default-size words (revision fc1f1fb8dd30)<br>"
        },
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            }
        ],
        "dependencies": [
            "Word_Lib"
        ],
        "theories": [
            "More_Bits_Int",
            "Code_Symbolic_Bits_Int",
            "Bits_Integer",
            "Code_Target_Bits_Int",
            "Code_Target_Word_Base",
            "Uint64",
            "Uint32",
            "Uint16",
            "Uint8",
            "Uint",
            "Native_Cast",
            "Native_Cast_Uint",
            "Native_Word_Imperative_HOL",
            "Native_Word_Test",
            "Native_Word_Test_Emu",
            "Native_Word_Test_PolyML",
            "Native_Word_Test_PolyML2",
            "Native_Word_Test_PolyML64",
            "Native_Word_Test_Scala",
            "Native_Word_Test_GHC",
            "Native_Word_Test_MLton",
            "Native_Word_Test_MLton2",
            "Native_Word_Test_OCaml",
            "Native_Word_Test_OCaml2",
            "Native_Word_Test_SMLNJ",
            "Native_Word_Test_SMLNJ2",
            "Uint_Userguide"
        ]
    },
    {
        "session": "Isabelle_Marries_Dirac",
        "title": "Isabelle Marries Dirac: a Library for Quantum Computation and Quantum Information",
        "authors": [
            "Anthony Bordg",
            "Hanna Lachnitt",
            "Yijun He"
        ],
        "topics": [
            "Computer science/Algorithms/Quantum computing",
            "Mathematics/Physics/Quantum information"
        ],
        "date": "2020-11-22",
        "abstract": "\nThis work is an effort to formalise some quantum algorithms and\nresults in quantum information theory. Formal methods being critical\nfor the safety and security of algorithms and protocols, we foresee\ntheir widespread use for quantum computing in the future. We have\ndeveloped a large library for quantum computing in Isabelle based on a\nmatrix representation for quantum circuits, successfully formalising\nthe no-cloning theorem, quantum teleportation, Deutsch's\nalgorithm, the Deutsch-Jozsa algorithm and the quantum Prisoner's\nDilemma.",
        "licence": "BSD",
        "dependencies": [
            "Jordan_Normal_Form",
            "Matrix_Tensor",
            "VectorSpace"
        ],
        "theories": [
            "Basics",
            "Binary_Nat",
            "Quantum",
            "Complex_Vectors",
            "Tensor",
            "More_Tensor",
            "Measurement",
            "Entanglement",
            "Quantum_Teleportation",
            "Deutsch",
            "Deutsch_Jozsa",
            "No_Cloning",
            "Quantum_Prisoners_Dilemma"
        ]
    },
    {
        "session": "Complete_Non_Orders",
        "title": "Complete Non-Orders and Fixed Points",
        "authors": [
            "Akihisa Yamada",
            "Jérémy Dubut"
        ],
        "topics": [
            "Mathematics/Order"
        ],
        "date": "2019-06-27",
        "abstract": "\nWe develop an Isabelle/HOL library of order-theoretic concepts, such\nas various completeness conditions and fixed-point theorems. We keep\nour formalization as general as possible: we reprove several\nwell-known results about complete orders, often without any properties\nof ordering, thus complete non-orders. In particular, we generalize\nthe Knaster–Tarski theorem so that we ensure the existence of a\nquasi-fixed point of monotone maps over complete non-orders, and show\nthat the set of quasi-fixed points is complete under a mild\ncondition—attractivity—which is implied by either antisymmetry or\ntransitivity. This result generalizes and strengthens a result by\nStauti and Maaden. Finally, we recover Kleene’s fixed-point theorem\nfor omega-complete non-orders, again using attractivity to prove that\nKleene’s fixed points are least quasi-fixed points.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-28"
            }
        ],
        "theories": [
            "Binary_Relations",
            "Complete_Relations",
            "Fixed_Points",
            "Kleene_Fixed_Point"
        ]
    },
    {
        "session": "Max-Card-Matching",
        "title": "Maximum Cardinality Matching",
        "authors": [
            "Christine Rizkallah"
        ],
        "date": "2011-07-21",
        "topics": [
            "Mathematics/Graph theory"
        ],
        "abstract": "\n<p>\nA <em>matching</em> in a graph <i>G</i> is a subset <i>M</i> of the\nedges of <i>G</i> such that no two share an endpoint. A matching has maximum\ncardinality if its cardinality is at least as large as that of any other\nmatching. An <em>odd-set cover</em> <i>OSC</i> of a graph <i>G</i> is a\nlabeling of the nodes of <i>G</i> with integers such that every edge of\n<i>G</i> is either incident to a node labeled 1 or connects two nodes\nlabeled with the same number <i>i &ge; 2</i>.\n</p><p>\nThis article proves Edmonds theorem:<br>\nLet <i>M</i> be a matching in a graph <i>G</i> and let <i>OSC</i> be an\nodd-set cover of <i>G</i>.\nFor any <i>i &ge; 0</i>, let <var>n(i)</var> be the number of nodes\nlabeled <i>i</i>. If <i>|M| = n(1) +\n&sum;<sub>i &ge; 2</sub>(n(i) div 2)</i>,\nthen <i>M</i> is a maximum cardinality matching.\n</p>",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-24"
            },
            {
                "2011-1": "2011-10-11"
            },
            {
                "2011": "2011-08-19"
            },
            {
                "2011": "2011-08-15"
            }
        ],
        "theories": [
            "Matching"
        ]
    },
    {
        "session": "Cauchy",
        "title": "Cauchy's Mean Theorem and the Cauchy-Schwarz Inequality",
        "authors": [
            "Benjamin Porter"
        ],
        "date": "2006-03-14",
        "topics": [
            "Mathematics/Analysis"
        ],
        "abstract": "This document presents the mechanised proofs of two popular theorems attributed to Augustin Louis Cauchy - Cauchy's Mean Theorem and the Cauchy-Schwarz Inequality.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-24"
            },
            {
                "2011-1": "2011-10-11"
            },
            {
                "2011": "2011-02-11"
            },
            {
                "2009-2": "2010-06-30"
            },
            {
                "2009-1": "2009-12-12"
            },
            {
                "2009": "2009-04-29"
            },
            {
                "2008": "2008-06-10"
            },
            {
                "2007": "2007-11-27"
            },
            {
                "2005": "2006-03-14"
            }
        ],
        "theories": [
            "CauchysMeanTheorem",
            "CauchySchwarz"
        ]
    },
    {
        "session": "Lower_Semicontinuous",
        "title": "Lower Semicontinuous Functions",
        "authors": [
            "Bogdan Grechuk"
        ],
        "topics": [
            "Mathematics/Analysis"
        ],
        "date": "2011-01-08",
        "abstract": "We define the notions of lower and upper semicontinuity for functions from a metric space to the extended real line. We prove that a function is both lower and upper semicontinuous if and only if it is continuous. We also give several equivalent characterizations of lower semicontinuity. In particular, we prove that a function is lower semicontinuous if and only if its epigraph is a closed set. Also, we introduce the notion of the lower semicontinuous hull of an arbitrary function and prove its basic properties.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-24"
            },
            {
                "2011-1": "2011-10-11"
            },
            {
                "2011": "2011-02-11"
            }
        ],
        "theories": [
            "Lower_Semicontinuous"
        ]
    },
    {
        "session": "Collections",
        "title": "Collections Framework",
        "authors": [
            "Peter Lammich"
        ],
        "contributors": [
            "Andreas Lochbihler",
            "Thomas Tuerk"
        ],
        "date": "2009-11-25",
        "topics": [
            "Computer science/Data structures"
        ],
        "abstract": "This development provides an efficient, extensible, machine checked collections framework. The library adopts the concepts of interface, implementation and generic algorithm from object-oriented programming and implements them in Isabelle/HOL. The framework features the use of data refinement techniques to refine an abstract specification (using high-level concepts like sets) to a more concrete implementation (using collection datastructures, like red-black-trees). The code-generator of Isabelle/HOL can be used to generate efficient code.",
        "extra": {
            "Change history": "[2010-10-08] New Interfaces OrderedSet, OrderedMap, List.\nFifo now implements list-interface Function names changed put/get --> enqueue/dequeue.\nNew Implementations ArrayList, ArrayHashMap, ArrayHashSet, TrieMap, TrieSet.\nInvariant-free datastructures Invariant implicitely hidden in typedef.\nRecord-interfaces All operations of an interface encapsulated as record.\nExamples moved to examples subdirectory.<br>\n[2010-12-01] New Interfaces Priority Queues, Annotated Lists. Implemented by finger trees, (skew) binomial queues.<br>\n[2011-10-10] SetSpec Added operations sng, isSng, bexists, size_abort, diff, filter, iterate_rule_insertP\nMapSpec Added operations sng, isSng, iterate_rule_insertP, bexists, size, size_abort, restrict,\nmap_image_filter, map_value_image_filter\nSome maintenance changes<br>\n[2012-04-25] New iterator foundation by Tuerk. Various maintenance changes.<br>\n[2012-08] Collections V2. New features Polymorphic iterators. Generic algorithm instantiation where required. Naming scheme changed from xx_opname to xx.opname.\nA compatibility file CollectionsV1 tries to simplify porting of existing theories, by providing old naming scheme and the old monomorphic iterator locales.<br>\n[2013-09] Added Generic Collection Framework based on Autoref. The GenCF provides Arbitrary nesting, full integration with Autoref.<br>\n[2014-06] Maintenace changes to GenCF Optimized inj_image on list_set. op_set_cart (Cartesian product). big-Union operation. atLeastLessThan - operation ({a..&lt;b})<br>"
        },
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-03-02"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-24"
            },
            {
                "2011-1": "2011-10-12"
            },
            {
                "2011-1": "2011-10-11"
            },
            {
                "2011": "2011-02-11"
            },
            {
                "2009-2": "2010-06-30"
            },
            {
                "2009-1": "2009-12-13"
            },
            {
                "2009-1": "2009-12-12"
            },
            {
                "2009": "2009-11-29"
            }
        ],
        "dependencies": [
            "Refine_Monadic",
            "Binomial-Heaps",
            "Finger-Trees",
            "Native_Word",
            "Trie"
        ],
        "theories": [
            "Sorted_List_Operations",
            "HashCode",
            "Code_Target_ICF",
            "SetIterator",
            "SetIteratorOperations",
            "Proper_Iterator",
            "It_to_It",
            "SetIteratorGA",
            "Gen_Iterator",
            "Idx_Iterator",
            "Iterator",
            "RBT_add",
            "Dlist_add",
            "Assoc_List",
            "Diff_Array",
            "Partial_Equivalence_Relation",
            "ICF_Tools",
            "Ord_Code_Preproc",
            "Record_Intf",
            "Locale_Code",
            "ICF_Spec_Base",
            "MapSpec",
            "Robdd",
            "Locale_Code_Ex",
            "DatRef",
            "SetAbstractionIterator",
            "GenCF_Chapter",
            "GenCF_Intf_Chapter",
            "Intf_Map",
            "Intf_Set",
            "Intf_Hash",
            "Intf_Comp",
            "GenCF_Gen_Chapter",
            "Gen_Set",
            "Gen_Map",
            "Gen_Map2Set",
            "Gen_Comp",
            "GenCF_Impl_Chapter",
            "Impl_Array_Stack",
            "Impl_List_Set",
            "Array_Iterator",
            "Impl_List_Map",
            "Impl_Array_Hash_Map",
            "Impl_RBT_Map",
            "Impl_Cfun_Set",
            "Impl_Array_Map",
            "Impl_Bit_Set",
            "Impl_Uv_Set",
            "Gen_Hash",
            "GenCF",
            "ICF_Chapter",
            "ICF_Spec_Chapter",
            "SetSpec",
            "ListSpec",
            "AnnotatedListSpec",
            "PrioSpec",
            "PrioUniqueSpec",
            "ICF_Gen_Algo_Chapter",
            "SetIteratorCollectionsGA",
            "MapGA",
            "SetGA",
            "SetByMap",
            "ListGA",
            "SetIndex",
            "Algos",
            "PrioByAnnotatedList",
            "PrioUniqueByAnnotatedList",
            "ICF_Impl_Chapter",
            "ListMapImpl",
            "ListMapImpl_Invar",
            "RBTMapImpl",
            "HashMap_Impl",
            "HashMap",
            "Trie_Impl",
            "Trie2",
            "TrieMapImpl",
            "ArrayHashMap_Impl",
            "ArrayHashMap",
            "ArrayMapImpl",
            "MapStdImpl",
            "ListSetImpl",
            "ListSetImpl_Invar",
            "ListSetImpl_NotDist",
            "ListSetImpl_Sorted",
            "RBTSetImpl",
            "HashSet",
            "TrieSetImpl",
            "ArrayHashSet",
            "ArraySetImpl",
            "SetStdImpl",
            "Fifo",
            "BinoPrioImpl",
            "SkewPrioImpl",
            "FTAnnotatedListImpl",
            "FTPrioImpl",
            "FTPrioUniqueImpl",
            "ICF_Impl",
            "ICF_Refine_Monadic",
            "ICF_Autoref",
            "ICF_Entrypoints_Chapter",
            "Collections",
            "CollectionsV1",
            "Collections_Entrypoints_Chapter",
            "Refine_Dflt",
            "Refine_Dflt_ICF",
            "Refine_Dflt_Only_ICF",
            "Userguides_Chapter",
            "Refine_Monadic_Userguide",
            "ICF_Userguide"
        ]
    },
    {
        "session": "Szpilrajn",
        "title": "Szpilrajn Extension Theorem",
        "authors": [
            "Peter Zeller"
        ],
        "topics": [
            "Mathematics/Order"
        ],
        "date": "2019-07-27",
        "abstract": "\nWe formalize the Szpilrajn extension theorem, also known as\norder-extension principal: Every strict partial order can be extended\nto a strict linear order.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-07-28"
            }
        ],
        "theories": [
            "Szpilrajn"
        ]
    },
    {
        "session": "LTL_Master_Theorem",
        "title": "A Compositional and Unified Translation of LTL into ω-Automata",
        "authors": [
            "Benedikt Seidl",
            "Salomon Sickert"
        ],
        "topics": [
            "Computer science/Automata and formal languages"
        ],
        "date": "2019-04-16",
        "abstract": "\nWe present a formalisation of the unified translation approach of\nlinear temporal logic (LTL) into ω-automata from [1]. This approach\ndecomposes LTL formulas into ``simple'' languages and allows\na clear separation of concerns: first, we formalise the purely logical\nresult yielding this decomposition; second, we instantiate this\ngeneric theory to obtain a construction for deterministic\n(state-based) Rabin automata (DRA). We extract from this particular\ninstantiation an executable tool translating LTL to DRAs. To the best\nof our knowledge this is the first verified translation from LTL to\nDRAs that is proven to be double exponential in the worst case which\nasymptotically matches the known lower bound.\n<p>\n[1] Javier Esparza, Jan Kretínský, Salomon Sickert. One Theorem to Rule Them All:\nA Unified Translation of LTL into ω-Automata. LICS 2018",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2019-04-17"
            }
        ],
        "dependencies": [
            "Transition_Systems_and_Automata",
            "LTL",
            "Deriving"
        ],
        "theories": [
            "Syntactic_Fragments_and_Stability",
            "After",
            "Advice",
            "Master_Theorem",
            "Asymmetric_Master_Theorem",
            "Restricted_Master_Theorem",
            "Transition_Functions",
            "Quotient_Type",
            "Omega_Words_Fun_Stream",
            "DRA_Construction",
            "DRA_Implementation",
            "Extra_Equivalence_Relations",
            "DRA_Instantiation",
            "Code_Export"
        ]
    },
    {
        "session": "Approximation_Algorithms",
        "title": "Verified Approximation Algorithms",
        "authors": [
            "Robin Eßmann",
            "Tobias Nipkow",
            "Simon Robillard"
        ],
        "topics": [
            "Computer science/Algorithms/Approximation"
        ],
        "date": "2020-01-16",
        "abstract": "\nWe present the first formal verification of approximation algorithms\nfor NP-complete optimization problems: vertex cover, set cover, independent set,\nload balancing, and bin packing. The proofs correct incompletenesses\nin existing proofs and improve the approximation ratio in one case.\nA detailed description of our work has been published in the proceedings of\n<a href=\"https://doi.org/10.1007/978-3-030-51054-1_17\">IJCAR 2020</a>.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2020-01-16"
            }
        ],
        "theories": [
            "Approx_VC_Hoare",
            "Approx_SC_Hoare",
            "Approx_MIS_Hoare",
            "Approx_LB_Hoare",
            "Approx_BP_Hoare"
        ]
    },
    {
        "session": "Sturm_Sequences",
        "title": "Sturm's Theorem",
        "authors": [
            "Manuel Eberl"
        ],
        "date": "2014-01-11",
        "topics": [
            "Mathematics/Analysis"
        ],
        "abstract": "Sturm's Theorem states that polynomial sequences with certain\nproperties, so-called Sturm sequences, can be used to count the number\nof real roots of a real polynomial. This work contains a proof of\nSturm's Theorem and code for constructing Sturm sequences efficiently.\nIt also provides the “sturm” proof method, which can decide certain\nstatements about the roots of real polynomials, such as “the polynomial\nP has exactly n roots in the interval I” or “P(x) > Q(x) for all x\n&#8712; &#8477;”.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2014-01-12"
            }
        ],
        "theories": [
            "Sturm_Library_Document",
            "Misc_Polynomial",
            "Sturm_Library",
            "Sturm_Theorem",
            "Sturm_Method",
            "files/sturm.ML",
            "Sturm",
            "Sturm_Ex"
        ]
    },
    {
        "session": "Combinatorics_Words_Lyndon",
        "title": "Lyndon words",
        "authors": [
            "Štěpán Holub",
            "Štěpán Starosta"
        ],
        "topics": [
            "Computer science/Automata and formal languages"
        ],
        "date": "2021-05-24",
        "abstract": "\nLyndon words are words lexicographically minimal in their conjugacy\nclass. We formalize their basic properties and characterizations, in\nparticular the concepts of the longest Lyndon suffix and the Lyndon\nfactorization. Most of the work assumes a fixed lexicographical order.\nNevertheless we also define the smallest relation guaranteeing\nlexicographical minimality of a given word (in its conjugacy class).",
        "licence": "BSD",
        "dependencies": [
            "Combinatorics_Words",
            "Szpilrajn"
        ],
        "theories": [
            "Lyndon",
            "Lyndon_Addition"
        ]
    },
    {
        "session": "Farkas",
        "title": "Farkas' Lemma and Motzkin's Transposition Theorem",
        "authors": [
            "Ralph Bottesch",
            "Max W. Haslbeck",
            "René Thiemann"
        ],
        "topics": [
            "Mathematics/Algebra"
        ],
        "date": "2019-01-17",
        "abstract": "\nWe formalize a proof of Motzkin's transposition theorem and\nFarkas' lemma in Isabelle/HOL. Our proof is based on the\nformalization of the simplex algorithm which, given a set of linear\nconstraints, either returns a satisfying assignment to the problem or\ndetects unsatisfiability. By reusing facts about the simplex algorithm\nwe show that a set of linear constraints is unsatisfiable if and only\nif there is a linear combination of the constraints which evaluates to\na trivially unsatisfiable inequality.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2019-01-21"
            }
        ],
        "dependencies": [
            "Simplex",
            "Jordan_Normal_Form"
        ],
        "theories": [
            "Farkas",
            "Matrix_Farkas",
            "Simplex_for_Reals"
        ]
    },
    {
        "session": "Deriving",
        "title": "Deriving class instances for datatypes",
        "authors": [
            "Christian Sternagel",
            "René Thiemann"
        ],
        "date": "2015-03-11",
        "topics": [
            "Computer science/Data structures"
        ],
        "abstract": "\n<p>We provide a framework for registering automatic methods\nto derive class instances of datatypes,\nas it is possible using Haskell's ``deriving Ord, Show, ...'' feature.</p>\n<p>We further implemented such automatic methods to derive comparators, linear orders, parametrizable equality functions,\nand hash-functions which are required in the\nIsabelle Collection Framework and the Container Framework.\nMoreover, for the tactic of Blanchette to show that a datatype is countable, we implemented a\nwrapper so that this tactic becomes accessible in our framework. All of the generators are based on\nthe infrastructure that is provided by the BNF-based datatype package.</p>\n<p>Our formalization was performed as part of the <a href=\"http://cl-informatik.uibk.ac.at/software/ceta\">IsaFoR/CeTA</a> project.\nWith our new tactics we could remove\nseveral tedious proofs for (conditional) linear orders, and conditional equality operators\nwithin IsaFoR and the Container Framework.</p>",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            }
        ],
        "dependencies": [
            "Collections"
        ],
        "theories": [
            "Derive_Manager",
            "files/derive_manager.ML",
            "Generator_Aux",
            "files/bnf_access.ML",
            "files/generator_aux.ML",
            "Comparator",
            "Compare",
            "files/compare_code.ML",
            "RBT_Compare_Order_Impl",
            "RBT_Comparator_Impl",
            "Comparator_Generator",
            "files/comparator_generator.ML",
            "Compare_Generator",
            "files/compare_generator.ML",
            "Compare_Instances",
            "Compare_Order_Instances",
            "Compare_Rat",
            "Compare_Real",
            "Equality_Generator",
            "files/equality_generator.ML",
            "Equality_Instances",
            "Hash_Generator",
            "files/hash_generator.ML",
            "Hash_Instances",
            "Countable_Generator",
            "Derive",
            "Derive_Examples"
        ]
    },
    {
        "session": "UPF_Firewall",
        "title": "Formal Network Models and Their Application to Firewall Policies",
        "authors": [
            "Achim D. Brucker",
            "Lukas Brügger",
            "Burkhart Wolff"
        ],
        "topics": [
            "Computer science/Security",
            "Computer science/Networks"
        ],
        "date": "2017-01-08",
        "abstract": "\nWe present a formal model of network protocols and their application\nto modeling firewall policies. The formalization is based on the\nUnified Policy Framework (UPF). The formalization was originally\ndeveloped with for generating test cases for testing the security\nconfiguration actual firewall and router (middle-boxes) using\nHOL-TestGen. Our work focuses on modeling application level protocols\non top of tcp/ip.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2017-01-11"
            }
        ],
        "dependencies": [
            "UPF"
        ],
        "theories": [
            "NetworkCore",
            "DatatypeAddress",
            "DatatypePort",
            "IntegerAddress",
            "IntegerPort",
            "IntegerPort_TCPUDP",
            "IPv4",
            "IPv4_TCPUDP",
            "NetworkModels",
            "PolicyCore",
            "PolicyCombinators",
            "PortCombinators",
            "ProtocolPortCombinators",
            "Ports",
            "PacketFilter",
            "NAT",
            "FWNormalisationCore",
            "NormalisationGenericProofs",
            "NormalisationIntegerPortProof",
            "NormalisationIPPProofs",
            "ElementaryRules",
            "FWNormalisation",
            "LTL_alike",
            "StatefulCore",
            "FTP",
            "FTP_WithPolicy",
            "VOIP",
            "FTPVOIP",
            "StatefulFW",
            "UPF-Firewall",
            "DMZDatatype",
            "DMZInteger",
            "DMZ",
            "Voice_over_IP",
            "Transformation01",
            "Transformation02",
            "Transformation",
            "NAT-FW",
            "PersonalFirewallInt",
            "PersonalFirewallIpv4",
            "PersonalFirewallDatatype",
            "PersonalFirewall",
            "Examples"
        ]
    },
    {
        "session": "Boolean_Expression_Checkers",
        "title": "Boolean Expression Checkers",
        "authors": [
            "Tobias Nipkow"
        ],
        "date": "2014-06-08",
        "topics": [
            "Computer science/Algorithms",
            "Logic/General logic/Mechanization of proofs"
        ],
        "abstract": "\nThis entry provides executable checkers for the following properties of\nboolean expressions: satisfiability, tautology and equivalence. Internally,\nthe checkers operate on binary decision trees and are reasonably efficient\n(for purely functional algorithms).",
        "extra": {
            "Change history": "[2015-09-23] Salomon Sickert added an interface that does not require the usage of the Boolean formula datatype. Furthermore the general Mapping type is used instead of an association list."
        },
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2014-06-08"
            }
        ],
        "theories": [
            "Boolean_Expression_Checkers",
            "Boolean_Expression_Checkers_AList_Mapping",
            "Boolean_Expression_Example"
        ]
    },
    {
        "session": "Bell_Numbers_Spivey",
        "title": "Spivey's Generalized Recurrence for Bell Numbers",
        "authors": [
            "Lukas Bulwahn"
        ],
        "date": "2016-05-04",
        "topics": [
            "Mathematics/Combinatorics"
        ],
        "abstract": "\nThis entry defines the Bell numbers as the cardinality of set partitions for\na carrier set of given size, and derives Spivey's generalized recurrence\nrelation for Bell numbers following his elegant and intuitive combinatorial\nproof.\n<p>\nAs the set construction for the combinatorial proof requires construction of\nthree intermediate structures, the main difficulty of the formalization is\nhandling the overall combinatorial argument in a structured way.\nThe introduced proof structure allows us to compose the combinatorial argument\nfrom its subparts, and supports to keep track how the detailed proof steps are\nrelated to the overall argument. To obtain this structure, this entry uses set\nmonad notation for the set construction's definition, introduces suitable\npredicates and rules, and follows a repeating structure in its Isar proof.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-05-04"
            }
        ],
        "dependencies": [
            "Card_Partitions"
        ],
        "theories": [
            "Bell_Numbers"
        ]
    },
    {
        "session": "Knuth_Morris_Pratt",
        "title": "The string search algorithm by Knuth, Morris and Pratt",
        "authors": [
            "Fabian Hellauer",
            "Peter Lammich"
        ],
        "topics": [
            "Computer science/Algorithms"
        ],
        "date": "2017-12-18",
        "abstract": "\nThe Knuth-Morris-Pratt algorithm is often used to show that the\nproblem of finding a string <i>s</i> in a text\n<i>t</i> can be solved deterministically in\n<i>O(|s| + |t|)</i> time. We use the Isabelle\nRefinement Framework to formulate and verify the algorithm. Via\nrefinement, we apply some optimisations and finally use the\n<em>Sepref</em> tool to obtain executable code in\n<em>Imperative/HOL</em>.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-12-18"
            }
        ],
        "dependencies": [
            "Sepref_IICF"
        ],
        "theories": [
            "KMP"
        ]
    },
    {
        "session": "AnselmGod",
        "title": "Anselm's God in Isabelle/HOL",
        "authors": [
            "Ben Blumson"
        ],
        "topics": [
            "Logic/Philosophical aspects"
        ],
        "date": "2017-09-06",
        "abstract": "\nPaul Oppenheimer and Edward Zalta's formalisation of\nAnselm's ontological argument for the existence of God is\nautomated by embedding a free logic for definite descriptions within\nIsabelle/HOL.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2017-09-18"
            },
            {
                "2016-1": "2017-09-11"
            },
            {
                "2016-1": "2017-09-08"
            }
        ],
        "theories": [
            "AnselmGod"
        ]
    },
    {
        "session": "Pell",
        "title": "Pell's Equation",
        "authors": [
            "Manuel Eberl"
        ],
        "topics": [
            "Mathematics/Number theory"
        ],
        "date": "2018-06-23",
        "abstract": "\n<p> This article gives the basic theory of Pell's equation\n<em>x</em><sup>2</sup> = 1 +\n<em>D</em>&thinsp;<em>y</em><sup>2</sup>,\nwhere\n<em>D</em>&thinsp;&isin;&thinsp;&#8469; is\na parameter and <em>x</em>, <em>y</em> are\ninteger variables. </p> <p> The main result that is proven\nis the following: If <em>D</em> is not a perfect square,\nthen there exists a <em>fundamental solution</em>\n(<em>x</em><sub>0</sub>,\n<em>y</em><sub>0</sub>) that is not the\ntrivial solution (1, 0) and which generates all other solutions\n(<em>x</em>, <em>y</em>) in the sense that\nthere exists some\n<em>n</em>&thinsp;&isin;&thinsp;&#8469;\nsuch that |<em>x</em>| +\n|<em>y</em>|&thinsp;&radic;<span\nstyle=\"text-decoration:\noverline\"><em>D</em></span> =\n(<em>x</em><sub>0</sub> +\n<em>y</em><sub>0</sub>&thinsp;&radic;<span\nstyle=\"text-decoration:\noverline\"><em>D</em></span>)<sup><em>n</em></sup>.\nThis also implies that the set of solutions is infinite, and it gives\nus an explicit and executable characterisation of all the solutions.\n</p> <p> Based on this, simple executable algorithms for\ncomputing the fundamental solution and the infinite sequence of all\nnon-negative solutions are also provided. </p>",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2018-06-25"
            }
        ],
        "theories": [
            "Efficient_Discrete_Sqrt",
            "Pell",
            "Pell_Algorithm",
            "Pell_Algorithm_Test"
        ]
    },
    {
        "session": "Abstract_Completeness",
        "title": "Abstract Completeness",
        "authors": [
            "Jasmin Christian Blanchette",
            "Andrei Popescu",
            "Dmitriy Traytel"
        ],
        "date": "2014-04-16",
        "topics": [
            "Logic/Proof theory"
        ],
        "abstract": "A formalization of an abstract property of possibly infinite derivation trees (modeled by a codatatype),  representing the core of a proof (in Beth/Hintikka style) of the first-order logic completeness theorem, independent of the concrete syntax or inference rules. This work is described in detail in the IJCAR 2014 publication by the authors.\nThe abstract proof can be instantiated for a wide range of Gentzen and tableau systems as well as various flavors of FOL---e.g., with or without predicates, equality, or sorts. Here, we give only a toy example instantiation with classical propositional logic. A more serious instance---many-sorted FOL with equality---is described elsewhere [Blanchette and Popescu, FroCoS 2013].",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2014-04-16"
            }
        ],
        "dependencies": [
            "Collections"
        ],
        "theories": [
            "Abstract_Completeness",
            "Propositional_Logic"
        ]
    },
    {
        "session": "IEEE_Floating_Point",
        "title": "A Formal Model of IEEE Floating Point Arithmetic",
        "authors": [
            "Lei Yu"
        ],
        "contributors": [
            "Fabian Hellauer",
            "Fabian Immler"
        ],
        "date": "2013-07-27",
        "topics": [
            "Computer science/Data structures"
        ],
        "abstract": "This development provides a formal model of IEEE-754 floating-point arithmetic. This formalization, including formal specification of the standard and proofs of important properties of floating-point arithmetic, forms the foundation for verifying programs with floating-point computation. There is also a code generation setup for floats so that we can execute programs using this formalization in functional programming languages.",
        "extra": {
            "Change history": "[2017-09-25] Added conversions from and to software floating point numbers\n(by Fabian Hellauer and Fabian Immler).<br>\n[2018-02-05] 'Modernized' representation following the formalization in HOL4\nformer \"float_format\" and predicate \"is_valid\" is now encoded in a type \"('e, 'f) float\" where\n'e and 'f encode the size of exponent and fraction."
        },
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-07-28"
            }
        ],
        "dependencies": [
            "Word_Lib"
        ],
        "theories": [
            "IEEE",
            "IEEE_Properties",
            "FP64",
            "Conversion_IEEE_Float",
            "Double"
        ]
    },
    {
        "session": "Chord_Segments",
        "title": "Intersecting Chords Theorem",
        "authors": [
            "Lukas Bulwahn"
        ],
        "date": "2016-10-11",
        "topics": [
            "Mathematics/Geometry"
        ],
        "abstract": "\nThis entry provides a geometric proof of the intersecting chords\ntheorem. The theorem states that when two chords intersect each other\ninside a circle, the products of their segments are equal.  After a\nshort review of existing proofs in the literature, I decided to use a\nproof approach that employs reasoning about lengths of line segments,\nthe orthogonality of two lines and the Pythagoras Law. Hence, one can\nunderstand the formalized proof easily with the knowledge of a few\ngeneral geometric facts that are commonly taught in high-school.  This\ntheorem is the 55th theorem of the Top 100 Theorems list.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-10-11"
            }
        ],
        "dependencies": [
            "Triangle"
        ],
        "theories": [
            "Chord_Segments"
        ]
    },
    {
        "session": "Topological_Semantics",
        "title": "Topological semantics for paraconsistent and paracomplete logics",
        "authors": [
            "David Fuenmayor"
        ],
        "topics": [
            "Logic/General logic"
        ],
        "date": "2020-12-17",
        "abstract": "\nWe introduce a generalized topological semantics for paraconsistent\nand paracomplete logics by drawing upon early works on topological\nBoolean algebras (cf. works by Kuratowski, Zarycki, McKinsey &\nTarski, etc.). In particular, this work exemplarily illustrates the\nshallow semantical embeddings approach (<a\nhref=\"http://dx.doi.org/10.1007/s11787-012-0052-y\">SSE</a>)\nemploying the proof assistant Isabelle/HOL. By means of the SSE\ntechnique we can effectively harness theorem provers, model finders\nand 'hammers' for reasoning with quantified non-classical\nlogics.",
        "licence": "BSD",
        "theories": [
            "sse_boolean_algebra",
            "sse_boolean_algebra_quantification",
            "sse_operation_positive",
            "sse_operation_positive_quantification",
            "sse_operation_negative",
            "sse_operation_negative_quantification",
            "topo_operators_basic",
            "topo_operators_derivative",
            "topo_alexandrov",
            "topo_frontier_algebra",
            "topo_negation_conditions",
            "topo_negation_fixedpoints",
            "ex_LFIs",
            "topo_strict_implication",
            "ex_subminimal_logics",
            "topo_derivative_algebra",
            "ex_LFUs",
            "topo_border_algebra",
            "topo_closure_algebra",
            "topo_interior_algebra"
        ]
    },
    {
        "session": "Lambda_Free_KBOs",
        "title": "Formalization of Knuth–Bendix Orders for Lambda-Free Higher-Order Terms",
        "authors": [
            "Heiko Becker",
            "Jasmin Christian Blanchette",
            "Uwe Waldmann",
            "Daniel Wand"
        ],
        "date": "2016-11-12",
        "topics": [
            "Logic/Rewriting"
        ],
        "abstract": "This Isabelle/HOL formalization defines Knuth–Bendix orders for higher-order terms without lambda-abstraction and proves many useful properties about them. The main order fully coincides with the standard transfinite KBO with subterm coefficients on first-order terms. It appears promising as the basis of a higher-order superposition calculus.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            }
        ],
        "dependencies": [
            "Nested_Multisets_Ordinals",
            "Regular-Sets",
            "Lambda_Free_RPOs",
            "Polynomials"
        ],
        "theories": [
            "Lambda_Free_KBO_Util",
            "Lambda_Free_KBO_App",
            "Lambda_Free_KBO_Std",
            "Lambda_Free_KBO_Basic",
            "Lambda_Free_TKBO_Coefs",
            "Lambda_Encoding_KBO",
            "Lambda_Free_KBOs"
        ]
    },
    {
        "session": "Forcing",
        "title": "Formalization of Forcing in Isabelle/ZF",
        "authors": [
            "Emmanuel Gunther",
            "Miguel Pagano",
            "Pedro Sánchez Terraf"
        ],
        "topics": [
            "Logic/Set theory"
        ],
        "date": "2020-05-06",
        "abstract": "\nWe formalize the theory of forcing in the set theory framework of\nIsabelle/ZF. Under the assumption of the existence of a countable\ntransitive model of ZFC, we construct a proper generic extension and\nshow that the latter also satisfies ZFC.",
        "licence": "BSD",
        "theories": [
            "Utils",
            "files/utils.ML",
            "Forcing_Notions",
            "Pointed_DC",
            "Rasiowa_Sikorski",
            "Nat_Miscellanea",
            "Internalizations",
            "Recursion_Thms",
            "Relative_Univ",
            "Synthetic_Definition",
            "Interface",
            "Forcing_Data",
            "Internal_ZFC_Axioms",
            "Renaming",
            "Renaming_Auto",
            "files/renaming.ML",
            "Names",
            "FrecR",
            "Arities",
            "Forces_Definition",
            "Forcing_Theorems",
            "Separation_Rename",
            "Separation_Axiom",
            "Pairing_Axiom",
            "Union_Axiom",
            "Powerset_Axiom",
            "Extensionality_Axiom",
            "Foundation_Axiom",
            "Least",
            "Replacement_Axiom",
            "Infinity_Axiom",
            "Choice_Axiom",
            "Ordinals_In_MG",
            "Proper_Extension",
            "Succession_Poset",
            "Forcing_Main"
        ]
    },
    {
        "session": "ArrowImpossibilityGS",
        "title": "Arrow and Gibbard-Satterthwaite",
        "authors": [
            "Tobias Nipkow"
        ],
        "date": "2008-09-01",
        "topics": [
            "Mathematics/Games and economics"
        ],
        "abstract": "This article formalizes two proofs of Arrow's impossibility theorem due to Geanakoplos and derives the Gibbard-Satterthwaite theorem as a corollary. One formalization is based on utility functions, the other one on strict partial orders.<br><br>An article about these proofs is found <a href=\"http://www21.in.tum.de/~nipkow/pubs/arrow.html\">here</a>.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-03-02"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-24"
            },
            {
                "2011-1": "2011-10-11"
            },
            {
                "2011": "2011-02-11"
            },
            {
                "2009-2": "2010-06-30"
            },
            {
                "2009-1": "2009-12-12"
            },
            {
                "2009": "2009-09-29"
            },
            {
                "2009": "2009-04-29"
            }
        ],
        "theories": [
            "Arrow_Utility",
            "Arrow_Order",
            "GS"
        ]
    },
    {
        "session": "Koenigsberg_Friendship",
        "title": "The Königsberg Bridge Problem and the Friendship Theorem",
        "authors": [
            "Wenda Li"
        ],
        "date": "2013-07-19",
        "topics": [
            "Mathematics/Graph theory"
        ],
        "abstract": "This development provides a formalization of undirected graphs and simple graphs, which are based on Benedikt Nordhoff and Peter Lammich's simple formalization of labelled directed graphs in the archive. Then, with our formalization of graphs, we show both necessary and sufficient conditions for Eulerian trails and circuits as well as the fact that the Königsberg Bridge Problem does not have a solution. In addition, we show the Friendship Theorem in simple graphs.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-07-26"
            }
        ],
        "dependencies": [
            "Dijkstra_Shortest_Path"
        ],
        "theories": [
            "MoreGraph",
            "KoenigsbergBridge",
            "FriendshipTheory"
        ]
    },
    {
        "session": "FFT",
        "title": "Fast Fourier Transform",
        "authors": [
            "Clemens Ballarin"
        ],
        "date": "2005-10-12",
        "topics": [
            "Computer science/Algorithms/Mathematical"
        ],
        "abstract": "We formalise a functional implementation of the FFT algorithm over the complex numbers, and its inverse. Both are shown equivalent to the usual definitions of these operations through Vandermonde matrices. They are also shown to be inverse to each other, more precisely, that composition of the inverse and the transformation yield the identity up to a scalar.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-24"
            },
            {
                "2011-1": "2011-10-11"
            },
            {
                "2011": "2011-02-11"
            },
            {
                "2009-2": "2010-06-30"
            },
            {
                "2009-1": "2009-12-12"
            },
            {
                "2009": "2009-04-29"
            },
            {
                "2008": "2008-06-10"
            },
            {
                "2007": "2007-11-27"
            },
            {
                "2005": "2005-10-14"
            }
        ],
        "theories": [
            "FFT"
        ]
    },
    {
        "session": "ADS_Functor",
        "title": "Authenticated Data Structures As Functors",
        "authors": [
            "Andreas Lochbihler",
            "Ognjen Marić"
        ],
        "topics": [
            "Computer science/Data structures"
        ],
        "date": "2020-04-16",
        "abstract": "\nAuthenticated data structures allow several systems to convince each\nother that they are referring to the same data structure, even if each\nof them knows only a part of the data structure. Using inclusion\nproofs, knowledgeable systems can selectively share their knowledge\nwith other systems and the latter can verify the authenticity of what\nis being shared.  In this article, we show how to modularly define\nauthenticated data structures, their inclusion proofs, and operations\nthereon as datatypes in Isabelle/HOL, using a shallow embedding.\nModularity allows us to construct complicated trees from reusable\nbuilding blocks, which we call Merkle functors. Merkle functors\ninclude sums, products, and function spaces and are closed under\ncomposition and least fixpoints.  As a practical application, we model\nthe hierarchical transactions of <a\nhref=\"https://www.canton.io\">Canton</a>, a\npractical interoperability protocol for distributed ledgers, as\nauthenticated data structures. This is a first step towards\nformalizing the Canton protocol and verifying its integrity and\nsecurity guarantees.",
        "licence": "BSD",
        "theories": [
            "Merkle_Interface",
            "ADS_Construction",
            "Generic_ADS_Construction",
            "Inclusion_Proof_Construction",
            "Canton_Transaction_Tree"
        ]
    },
    {
        "session": "Auto2_Imperative_HOL",
        "title": "Verifying Imperative Programs using Auto2",
        "authors": [
            "Bohua Zhan"
        ],
        "topics": [
            "Computer science/Algorithms",
            "Computer science/Data structures"
        ],
        "date": "2018-12-21",
        "abstract": "\nThis entry contains the application of auto2 to verifying functional\nand imperative programs. Algorithms and data structures that are\nverified include linked lists, binary search trees, red-black trees,\ninterval trees, priority queue, quicksort, union-find, Dijkstra's\nalgorithm, and a sweep-line algorithm for detecting rectangle\nintersection. The imperative verification is based on Imperative HOL\nand its separation logic framework. A major goal of this work is to\nset up automation in order to reduce the length of proof that the user\nneeds to provide, both for verifying functional programs and for\nworking with separation logic.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2019-01-22"
            }
        ],
        "dependencies": [
            "Auto2_HOL"
        ],
        "theories": [
            "Mapping_Str",
            "Lists_Ex",
            "BST",
            "Partial_Equiv_Rel",
            "Union_Find",
            "Connectivity",
            "Arrays_Ex",
            "Dijkstra",
            "Interval",
            "Interval_Tree",
            "Quicksort",
            "Indexed_PQueue",
            "RBTree",
            "Rect_Intersect",
            "SepLogic_Base",
            "files/sep_util_base.ML",
            "files/assn_matcher.ML",
            "files/sep_steps.ML",
            "SepAuto",
            "files/sep_util.ML",
            "files/sep_steps_test.ML",
            "GCD_Impl",
            "LinkedList",
            "files/list_matcher_test.ML",
            "BST_Impl",
            "RBTree_Impl",
            "Arrays_Impl",
            "Quicksort_Impl",
            "Union_Find_Impl",
            "Connectivity_Impl",
            "DynamicArray",
            "Indexed_PQueue_Impl",
            "Dijkstra_Impl",
            "IntervalTree_Impl",
            "Rect_Intersect_Impl",
            "Sep_Examples"
        ]
    },
    {
        "session": "Source_Coding_Theorem",
        "title": "Source Coding Theorem",
        "authors": [
            "Quentin Hibon",
            "Lawrence C. Paulson"
        ],
        "date": "2016-10-19",
        "topics": [
            "Mathematics/Probability theory"
        ],
        "abstract": "\nThis document contains a proof of the necessary condition on the code\nrate of a source code, namely that this code rate is bounded by the\nentropy of the source. This represents one half of Shannon's source\ncoding theorem, which is itself an equivalence.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-10-19"
            }
        ],
        "theories": [
            "Source_Coding_Theorem"
        ]
    },
    {
        "session": "ClockSynchInst",
        "title": "Instances of Schneider's generalized protocol of clock synchronization",
        "authors": [
            "Damián Barsotti"
        ],
        "date": "2006-03-15",
        "topics": [
            "Computer science/Algorithms/Distributed"
        ],
        "abstract": "F. B. Schneider (\"Understanding protocols for Byzantine clock synchronization\") generalizes a number of protocols for Byzantine fault-tolerant clock synchronization and presents a uniform proof for their correctness. In Schneider's schema, each processor maintains a local clock by periodically adjusting each value to one computed by a convergence function applied to the readings of all the clocks. Then, correctness of an algorithm, i.e. that the readings of two clocks at any time are within a fixed bound of each other, is based upon some conditions on the convergence function. To prove that a particular clock synchronization algorithm is correct it suffices to show that the convergence function used by the algorithm meets Schneider's conditions. Using the theorem prover Isabelle, we formalize the proofs that the convergence functions of two algorithms, namely, the Interactive Convergence Algorithm (ICA) of Lamport and Melliar-Smith and the Fault-tolerant Midpoint algorithm of Lundelius-Lynch, meet Schneider's conditions. Furthermore, we experiment on handling some parts of the proofs with fully automatic tools like ICS and CVC-lite. These theories are part of a joint work with Alwen Tiu and Leonor P. Nieto <a href=\"http://users.rsise.anu.edu.au/~tiu/clocksync.pdf\">\"Verification of Clock Synchronization Algorithms: Experiments on a combination of deductive tools\"</a> in proceedings of AVOCS 2005. In this work the correctness of Schneider schema was also verified using Isabelle (entry <a href=\"GenClock.html\">GenClock</a> in AFP).",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-03-02"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-24"
            },
            {
                "2011-1": "2011-10-11"
            },
            {
                "2011": "2011-02-11"
            },
            {
                "2009-2": "2010-06-30"
            },
            {
                "2009-1": "2009-12-12"
            },
            {
                "2009": "2009-04-29"
            },
            {
                "2008": "2008-06-10"
            },
            {
                "2007": "2007-11-27"
            },
            {
                "2005": "2006-03-15"
            }
        ],
        "theories": [
            "ICAInstance",
            "LynchInstance"
        ]
    },
    {
        "session": "GoedelGod",
        "title": "Gödel's God in Isabelle/HOL",
        "authors": [
            "Christoph Benzmüller",
            "Bruno Woltzenlogel Paleo"
        ],
        "date": "2013-11-12",
        "topics": [
            "Logic/Philosophical aspects"
        ],
        "abstract": "Dana Scott's version of Gödel's proof of God's existence is formalized in quantified\nmodal logic KB (QML KB).\nQML KB is modeled as a fragment of classical higher-order logic (HOL);\nthus, the formalization is essentially a formalization in HOL.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-19"
            },
            {
                "2013-1": "2013-11-18"
            }
        ],
        "theories": [
            "GoedelGod"
        ]
    },
    {
        "session": "VectorSpace",
        "title": "Vector Spaces",
        "authors": [
            "Holden Lee"
        ],
        "date": "2014-08-29",
        "topics": [
            "Mathematics/Algebra"
        ],
        "abstract": "This formalisation of basic linear algebra is based completely on locales, building off HOL-Algebra. It includes basic definitions: linear combinations, span, linear independence; linear transformations; interpretation of function spaces as vector spaces; the direct sum of vector spaces, sum of subspaces; the replacement theorem; existence of bases in finite-dimensional; vector spaces, definition of dimension; the rank-nullity theorem. Some concepts are actually defined and proved for modules as they also apply there. Infinite-dimensional vector spaces are supported, but dimension is only supported for finite-dimensional vector spaces. The proofs are standard; the proofs of the replacement theorem and rank-nullity theorem roughly follow the presentation in Linear Algebra by Friedberg, Insel, and Spence. The rank-nullity theorem generalises the existing development in the Archive of Formal Proof (originally using type classes, now using a mix of type classes and locales).",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-09-07"
            },
            {
                "2014": "2014-08-31"
            },
            {
                "2014": "2014-08-29"
            }
        ],
        "theories": [
            "RingModuleFacts",
            "FunctionLemmas",
            "MonoidSums",
            "LinearCombinations",
            "SumSpaces",
            "VectorSpace"
        ]
    },
    {
        "session": "GewirthPGCProof",
        "title": "Formalisation and Evaluation of Alan Gewirth's Proof for the Principle of Generic Consistency in Isabelle/HOL",
        "authors": [
            "David Fuenmayor",
            "Christoph Benzmüller"
        ],
        "topics": [
            "Logic/Philosophical aspects"
        ],
        "date": "2018-10-30",
        "abstract": "\nAn ambitious ethical theory ---Alan Gewirth's \"Principle of\nGeneric Consistency\"--- is encoded and analysed in Isabelle/HOL.\nGewirth's theory has stirred much attention in philosophy and\nethics and has been proposed as a potential means to bound the impact\nof artificial general intelligence.",
        "extra": {
            "Change history": "[2019-04-09]\nadded proof for a stronger variant of the PGC and examplary inferences\n(revision 88182cb0a2f6)<br>"
        },
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-10-31"
            }
        ],
        "theories": [
            "CJDDLplus",
            "ExtendedDDL",
            "GewirthArgument"
        ]
    },
    {
        "session": "FOL-Fitting",
        "title": "First-Order Logic According to Fitting",
        "authors": [
            "Stefan Berghofer"
        ],
        "contributors": [
            "Asta Halkjær From"
        ],
        "date": "2007-08-02",
        "topics": [
            "Logic/General logic/Classical first-order logic"
        ],
        "abstract": "We present a formalization of parts of Melvin Fitting's book \"First-Order Logic and Automated Theorem Proving\". The formalization covers the syntax of first-order logic, its semantics, the model existence theorem, a natural deduction proof calculus together with a proof of correctness and completeness, as well as the Löwenheim-Skolem theorem.",
        "extra": {
            "Change history": "[2018-07-21] Proved completeness theorem for open formulas. Proofs are now written in the declarative style. Enumeration of pairs and datatypes is automated using the Countable theory."
        },
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-03-02"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-24"
            },
            {
                "2011-1": "2011-10-11"
            },
            {
                "2011": "2011-02-11"
            },
            {
                "2009-2": "2010-06-30"
            },
            {
                "2009-1": "2009-12-12"
            },
            {
                "2009": "2009-04-29"
            },
            {
                "2008": "2008-06-10"
            },
            {
                "2007": "2007-11-27"
            }
        ],
        "theories": [
            "FOL_Fitting"
        ]
    },
    {
        "session": "Sliding_Window_Algorithm",
        "title": "Formalization of an Algorithm for Greedily Computing Associative Aggregations on Sliding Windows",
        "authors": [
            "Lukas Heimes",
            "Dmitriy Traytel",
            "Joshua Schneider"
        ],
        "topics": [
            "Computer science/Algorithms"
        ],
        "date": "2020-04-10",
        "abstract": "\nBasin et al.'s <a\nhref=\"https://doi.org/10.1016/j.ipl.2014.09.009\">sliding\nwindow algorithm (SWA)</a> is an algorithm for combining the\nelements of subsequences of a sequence with an associative operator.\nIt is greedy and minimizes the number of operator applications. We\nformalize the algorithm and verify its functional correctness. We\nextend the algorithm with additional operations and provide an\nalternative interface to the slide operation that does not require the\nentire input sequence.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2020-04-12"
            }
        ],
        "theories": [
            "SWA"
        ]
    },
    {
        "session": "Physical_Quantities",
        "title": "A Sound Type System for Physical Quantities, Units, and Measurements",
        "authors": [
            "Simon Foster",
            "Burkhart Wolff"
        ],
        "topics": [
            "Mathematics/Physics",
            "Computer science/Programming languages/Type systems"
        ],
        "date": "2020-10-20",
        "abstract": "\nThe present Isabelle theory builds a formal model for both the\nInternational System of Quantities (ISQ) and the International System\nof Units (SI), which are both fundamental for physics and engineering.\nBoth the ISQ and the SI are deeply integrated into Isabelle's\ntype system. Quantities are parameterised by dimension types, which\ncorrespond to base vectors, and thus only quantities of the same\ndimension can be equated. Since the underlying \"algebra of\nquantities\" induces congruences on quantity and SI types,\nspecific tactic support is developed to capture these. Our\nconstruction is validated by a test-set of known equivalences between\nboth quantities and SI units. Moreover, the presented theory can be\nused for type-safe conversions between the SI system and others, like\nthe British Imperial System (BIS).",
        "licence": "BSD",
        "theories": [
            "Power_int",
            "Enum_extra",
            "Groups_mult",
            "ISQ_Dimensions",
            "ISQ_Quantities",
            "ISQ_Proof",
            "ISQ_Algebra",
            "ISQ_Units",
            "ISQ_Conversion",
            "ISQ",
            "SI_Units",
            "CGS",
            "SI_Constants",
            "SI_Prefix",
            "SI_Derived",
            "SI_Accepted",
            "SI_Imperial",
            "SI",
            "SI_Astronomical",
            "SI_Pretty",
            "BIS"
        ]
    },
    {
        "session": "Lam-ml-Normalization",
        "title": "Strong Normalization of Moggis's Computational Metalanguage",
        "authors": [
            "Christian Doczkal"
        ],
        "date": "2010-08-29",
        "topics": [
            "Computer science/Programming languages/Lambda calculi"
        ],
        "abstract": "Handling variable binding is one of the main difficulties in formal proofs. In this context, Moggi's computational metalanguage serves as an interesting case study. It features monadic types and a commuting conversion rule that rearranges the binding structure. Lindley and Stark have given an elegant proof of strong normalization for this calculus. The key construction in their proof is a notion of relational TT-lifting, using stacks of elimination contexts to obtain a Girard-Tait style logical relation. I give a formalization of their proof in Isabelle/HOL-Nominal with a particular emphasis on the treatment of bound variables.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-24"
            },
            {
                "2011-1": "2011-10-11"
            },
            {
                "2011": "2011-02-11"
            },
            {
                "2009-2": "2010-09-01"
            }
        ],
        "theories": [
            "Lam_ml"
        ]
    },
    {
        "session": "DPT-SAT-Solver",
        "title": "A Fast SAT Solver for Isabelle in Standard ML",
        "topics": [
            "Tools"
        ],
        "authors": [
            "Armin Heller"
        ],
        "date": "2009-12-09",
        "abstract": "This contribution contains a fast SAT solver for Isabelle written in Standard ML. By loading the theory <tt>DPT_SAT_Solver</tt>, the SAT solver installs itself (under the name ``dptsat'') and certain Isabelle tools like Refute will start using it automatically. This is a port of the DPT (Decision Procedure Toolkit) SAT Solver written in OCaml.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-07-27"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-24"
            },
            {
                "2011-1": "2011-10-11"
            },
            {
                "2011": "2011-02-11"
            },
            {
                "2009-2": "2010-06-30"
            },
            {
                "2009-1": "2009-12-12"
            }
        ],
        "theories": [
            "DPT_SAT_Solver",
            "files/dpt_sat_solver.ML",
            "DPT_SAT_Tests"
        ]
    },
    {
        "session": "MFOTL_Monitor",
        "title": "Formalization of a Monitoring Algorithm for Metric First-Order Temporal Logic",
        "authors": [
            "Joshua Schneider",
            "Dmitriy Traytel"
        ],
        "topics": [
            "Computer science/Algorithms",
            "Logic/General logic/Temporal logic",
            "Computer science/Automata and formal languages"
        ],
        "date": "2019-07-04",
        "abstract": "\nA monitor is a runtime verification tool that solves the following\nproblem: Given a stream of time-stamped events and a policy formulated\nin a specification language, decide whether the policy is satisfied at\nevery point in the stream. We verify the correctness of an executable\nmonitor for specifications given as formulas in metric first-order\ntemporal logic (MFOTL), an expressive extension of linear temporal\nlogic with real-time constraints and first-order quantification. The\nverified monitor implements a simplified variant of the algorithm used\nin the efficient MonPoly monitoring tool. The formalization is\npresented in a <a href=\"https://doi.org/10.1007/978-3-030-32079-9_18\">RV\n2019 paper</a>, which also compares the output of the verified\nmonitor to that of other monitoring tools on randomly generated\ninputs. This case study revealed several errors in the optimized but\nunverified tools.",
        "extra": {
            "Change history": "[2020-08-13]\nadded the formalization of the abstract slicing framework and joint data\nslicer (revision b1639ed541b7)<br>"
        },
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-07-05"
            }
        ],
        "dependencies": [
            "Containers"
        ],
        "theories": [
            "Trace",
            "Table",
            "Abstract_Monitor",
            "Interval",
            "MFOTL",
            "Monitor",
            "Slicing",
            "Monitor_Code",
            "Examples"
        ]
    },
    {
        "session": "Dijkstra_Shortest_Path",
        "title": "Dijkstra's Shortest Path Algorithm",
        "authors": [
            "Benedikt Nordhoff",
            "Peter Lammich"
        ],
        "topics": [
            "Computer science/Algorithms/Graph"
        ],
        "date": "2012-01-30",
        "abstract": "We implement and prove correct Dijkstra's algorithm for the\nsingle source shortest path problem, conceived in 1956 by E. Dijkstra.\nThe algorithm is implemented using the data refinement framework for monadic,\nnondeterministic programs. An efficient implementation is derived using data\nstructures from the Isabelle Collection Framework.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-03-08"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-24"
            },
            {
                "2011-1": "2012-03-15"
            },
            {
                "2011-1": "2012-02-10"
            }
        ],
        "dependencies": [
            "Collections"
        ],
        "theories": [
            "Introduction",
            "Dijkstra_Misc",
            "Graph",
            "Weight",
            "Dijkstra",
            "GraphSpec",
            "GraphGA",
            "GraphByMap",
            "HashGraphImpl",
            "Dijkstra_Impl",
            "Dijkstra_Impl_Adet",
            "Test"
        ]
    },
    {
        "session": "Nominal2",
        "title": "Nominal 2",
        "authors": [
            "Christian Urban",
            "Stefan Berghofer",
            "Cezary Kaliszyk"
        ],
        "date": "2013-02-21",
        "topics": [
            "Tools"
        ],
        "abstract": "\n<p>Dealing with binders, renaming of bound variables, capture-avoiding\nsubstitution, etc., is very often a major problem in formal\nproofs, especially in proofs by structural and rule\ninduction. Nominal Isabelle is designed to make such proofs easy to\nformalise: it provides an infrastructure for declaring nominal\ndatatypes (that is alpha-equivalence classes) and for defining\nfunctions over them by structural recursion. It also provides\ninduction principles that have Barendregt’s variable convention\nalready built in.\n</p><p>\nThis entry can be used as a more advanced replacement for\nHOL/Nominal in the Isabelle distribution.\n</p>",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-04-24"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-02-24"
            }
        ],
        "dependencies": [
            "FinFun"
        ],
        "theories": [
            "Nominal2_Base",
            "files/nominal_basics.ML",
            "files/nominal_thmdecls.ML",
            "files/nominal_permeq.ML",
            "files/nominal_library.ML",
            "files/nominal_atoms.ML",
            "files/nominal_eqvt.ML",
            "Nominal2_Abs",
            "Nominal2_FCB",
            "Nominal2",
            "files/nominal_dt_data.ML",
            "files/nominal_dt_rawfuns.ML",
            "files/nominal_dt_alpha.ML",
            "files/nominal_dt_quot.ML",
            "files/nominal_induct.ML",
            "files/nominal_inductive.ML",
            "files/nominal_function_common.ML",
            "files/nominal_function_core.ML",
            "files/nominal_mutual.ML",
            "files/nominal_function.ML",
            "files/nominal_termination.ML",
            "Atoms",
            "Eqvt"
        ]
    },
    {
        "session": "RSAPSS",
        "title": "SHA1, RSA, PSS and more",
        "authors": [
            "Christina Lindenberg",
            "Kai Wirt"
        ],
        "date": "2005-05-02",
        "topics": [
            "Computer science/Security/Cryptography"
        ],
        "abstract": "Formal verification is getting more and more important in computer science. However the state of the art formal verification methods in cryptography are very rudimentary. These theories are one step to provide a tool box allowing the use of formal methods in every aspect of cryptography. Moreover we present a proof of concept for the feasibility of verification techniques to a standard signature algorithm.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-03-02"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-24"
            },
            {
                "2011-1": "2011-10-11"
            },
            {
                "2011": "2011-02-11"
            },
            {
                "2009-2": "2010-07-01"
            },
            {
                "2009-1": "2009-12-12"
            },
            {
                "2009": "2009-04-29"
            },
            {
                "2008": "2008-06-10"
            },
            {
                "2007": "2007-11-27"
            },
            {
                "2005": "2005-10-14"
            }
        ],
        "theories": [
            "Word",
            "WordOperations",
            "SHA1Padding",
            "SHA1",
            "Wordarith",
            "EMSAPSS",
            "Mod",
            "Crypt",
            "Pdifference",
            "Productdivides",
            "Cryptinverts",
            "RSAPSS"
        ]
    },
    {
        "session": "Aristotles_Assertoric_Syllogistic",
        "title": "Aristotle's Assertoric Syllogistic",
        "authors": [
            "Angeliki Koutsoukou-Argyraki"
        ],
        "topics": [
            "Logic/Philosophical aspects"
        ],
        "date": "2019-10-08",
        "abstract": "\nWe formalise with Isabelle/HOL some basic elements of Aristotle's\nassertoric syllogistic following the <a\nhref=\"https://plato.stanford.edu/entries/aristotle-logic/\">article from the Stanford Encyclopedia of Philosophy by Robin Smith.</a> To\nthis end, we use a set theoretic formulation (covering both individual\nand general predication). In particular, we formalise the deductions\nin the Figures and after that we present Aristotle's\nmetatheoretical observation that all deductions in the Figures can in\nfact be reduced to either Barbara or Celarent. As the formal proofs\nprove to be straightforward, the interest of this entry lies in\nillustrating the functionality of Isabelle and high efficiency of\nSledgehammer for simple exercises in philosophy.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-10-17"
            }
        ],
        "theories": [
            "AristotlesAssertoric"
        ]
    },
    {
        "session": "Median_Of_Medians_Selection",
        "title": "The Median-of-Medians Selection Algorithm",
        "authors": [
            "Manuel Eberl"
        ],
        "topics": [
            "Computer science/Algorithms"
        ],
        "date": "2017-12-21",
        "abstract": "\n<p>This entry provides an executable functional implementation\nof the Median-of-Medians algorithm for selecting the\n<em>k</em>-th smallest element of an unsorted list\ndeterministically in linear time. The size bounds for the recursive\ncall that lead to the linear upper bound on the run-time of the\nalgorithm are also proven. </p>",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-12-22"
            }
        ],
        "theories": [
            "Median_Of_Medians_Selection"
        ]
    },
    {
        "session": "Priority_Search_Trees",
        "title": "Priority Search Trees",
        "authors": [
            "Peter Lammich",
            "Tobias Nipkow"
        ],
        "topics": [
            "Computer science/Data structures"
        ],
        "date": "2019-06-25",
        "abstract": "\nWe present a new, purely functional, simple and efficient data\nstructure combining a search tree and a priority queue, which we call\na <em>priority search tree</em>. The salient feature of priority search\ntrees is that they offer a decrease-key operation, something that is\nmissing from other simple, purely functional priority queue\nimplementations. Priority search trees can be implemented on top of\nany search tree. This entry does the implementation for red-black\ntrees.  This entry formalizes the first part of our ITP-2019 proof\npearl <em>Purely Functional, Simple and Efficient Priority\nSearch Trees and Applications to Prim and Dijkstra</em>.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-29"
            }
        ],
        "theories": [
            "Prio_Map_Specs",
            "PST_General",
            "PST_RBT"
        ]
    },
    {
        "session": "Coinductive",
        "title": "Coinductive",
        "topics": [
            "Computer science/Functional programming"
        ],
        "authors": [
            "Andreas Lochbihler"
        ],
        "contributors": [
            "Johannes Hölzl"
        ],
        "date": "2010-02-12",
        "abstract": "This article collects formalisations of general-purpose coinductive data types and sets. Currently, it contains coinductive natural numbers, coinductive lists, i.e. lazy lists or streams, infinite streams, coinductive terminated lists, coinductive resumptions, a library of operations on coinductive lists, and a version of König's lemma as an application for coinductive lists.<br>The initial theory was contributed by Paulson and Wenzel. Extensions and other coinductive formalisations of general interest are welcome.",
        "extra": {
            "Change history": "[2010-06-10]\ncoinductive lists setup for quotient package\n(revision 015574f3bf3c)<br>\n[2010-06-28]\nnew codatatype terminated lazy lists\n(revision e12de475c558)<br>\n[2010-08-04]\nterminated lazy lists setup for quotient package;\nmore lemmas\n(revision 6ead626f1d01)<br>\n[2010-08-17]\nKoenig's lemma as an example application for coinductive lists\n(revision f81ce373fa96)<br>\n[2011-02-01]\nlazy implementation of coinductive (terminated) lists for the code generator\n(revision 6034973dce83)<br>\n[2011-07-20]\nnew codatatype resumption\n(revision 811364c776c7)<br>\n[2012-06-27]\nnew codatatype stream with operations (with contributions by Peter Gammie)\n(revision dd789a56473c)<br>\n[2013-03-13]\nconstruct codatatypes with the BNF package and adjust the definitions and proofs,\nsetup for lifting and transfer packages\n(revision f593eda5b2c0)<br>\n[2013-09-20]\nstream theory uses type and operations from HOL/BNF/Examples/Stream\n(revision 692809b2b262)<br>\n[2014-04-03]\nccpo structure on codatatypes used to define ldrop, ldropWhile, lfilter, lconcat as least fixpoint;\nccpo topology on coinductive lists contributed by Johannes Hölzl;\nadded examples\n(revision 23cd8156bd42)<br>"
        },
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-03-02"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-24"
            },
            {
                "2011-1": "2011-10-11"
            },
            {
                "2011": "2011-02-11"
            },
            {
                "2009-2": "2010-06-30"
            },
            {
                "2009-1": "2010-02-15"
            },
            {
                "2009-1": "2010-02-14"
            }
        ],
        "theories": [
            "Coinductive_Nat",
            "Coinductive_List",
            "Coinductive_List_Prefix",
            "Coinductive_Stream",
            "TLList",
            "Quotient_Coinductive_List",
            "Quotient_TLList",
            "Coinductive",
            "Lazy_LList",
            "Lazy_TLList",
            "CCPO_Topology",
            "LList_CCPO_Topology",
            "TLList_CCPO",
            "TLList_CCPO_Examples",
            "Koenigslemma",
            "LMirror",
            "Hamming_Stream",
            "Resumption",
            "Coinductive_Examples"
        ]
    },
    {
        "session": "Cayley_Hamilton",
        "title": "The Cayley-Hamilton Theorem",
        "authors": [
            "Stephan Adelsberger",
            "Stefan Hetzl",
            "Florian Pollak"
        ],
        "date": "2014-09-15",
        "topics": [
            "Mathematics/Algebra"
        ],
        "abstract": "\nThis document contains a proof of the Cayley-Hamilton theorem\nbased on the development of matrices in HOL/Multivariate Analysis.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-09-16"
            }
        ],
        "theories": [
            "Square_Matrix",
            "Cayley_Hamilton"
        ]
    },
    {
        "session": "Certification_Monads",
        "title": "Certification Monads",
        "authors": [
            "Christian Sternagel",
            "René Thiemann"
        ],
        "date": "2014-10-03",
        "topics": [
            "Computer science/Functional programming"
        ],
        "abstract": "This entry provides several monads intended for the development of stand-alone certifiers via code generation from Isabelle/HOL. More specifically, there are three flavors of error monads (the sum type, for the case where all monadic functions are total; an instance of the former, the so called check monad, yielding either success without any further information or an error message; as well as a variant of the sum type that accommodates partial functions by providing an explicit bottom element) and a parser monad built on top. All of this monads are heavily used in the IsaFoR/CeTA project which thus provides many examples of their usage.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-10-08"
            }
        ],
        "dependencies": [
            "Partial_Function_MR",
            "Show"
        ],
        "theories": [
            "Error_Syntax",
            "Error_Monad",
            "Check_Monad",
            "Strict_Sum",
            "Parser_Monad",
            "Misc"
        ]
    },
    {
        "session": "Lucas_Theorem",
        "title": "Lucas's Theorem",
        "authors": [
            "Chelsea Edmonds"
        ],
        "topics": [
            "Mathematics/Number theory"
        ],
        "date": "2020-04-07",
        "abstract": "\nThis work presents a formalisation of a generating function proof for\nLucas's theorem. We first outline extensions to the existing\nFormal Power Series (FPS) library, including an equivalence relation\nfor coefficients modulo <em>n</em>, an alternate binomial theorem statement,\nand a formalised proof of the Freshman's dream (mod <em>p</em>) lemma.\nThe second part of the work presents the formal proof of Lucas's\nTheorem. Working backwards, the formalisation first proves a well\nknown corollary of the theorem which is easier to formalise, and then\napplies induction to prove the original theorem statement. The proof\nof the corollary aims to provide a good example of a formalised\ngenerating function equivalence proof using the FPS library. The final\ntheorem statement is intended to be integrated into the formalised\nproof of Hilbert's 10th Problem.",
        "licence": "BSD",
        "theories": [
            "Lucas_Theorem"
        ]
    },
    {
        "session": "Binomial-Queues",
        "title": "Functional Binomial Queues",
        "authors": [
            "René Neumann"
        ],
        "date": "2010-10-28",
        "topics": [
            "Computer science/Data structures"
        ],
        "abstract": "Priority queues are an important data structure and efficient implementations of them are crucial. We implement a functional variant of binomial queues in Isabelle/HOL and show its functional correctness. A verification against an abstract reference specification of priority queues has also been attempted, but could not be achieved to the full extent.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-24"
            },
            {
                "2011-1": "2011-10-11"
            },
            {
                "2011": "2011-02-11"
            }
        ],
        "theories": [
            "PQ",
            "Binomial_Queue",
            "PQ_Implementation"
        ]
    },
    {
        "session": "ZFC_in_HOL",
        "title": "Zermelo Fraenkel Set Theory in Higher-Order Logic",
        "authors": [
            "Lawrence C. Paulson"
        ],
        "topics": [
            "Logic/Set theory"
        ],
        "date": "2019-10-24",
        "abstract": "\n<p>This entry is a new formalisation of ZFC set theory in Isabelle/HOL. It is\nlogically equivalent to Obua's HOLZF; the point is to have the closest\npossible integration with the rest of Isabelle/HOL, minimising the amount of\nnew notations and exploiting type classes.</p>\n<p>There is a type <em>V</em> of sets and a function <em>elts :: V =&gt; V\nset</em> mapping a set to its elements. Classes simply have type <em>V\nset</em>, and a predicate identifies the small classes: those that correspond\nto actual sets. Type classes connected with orders and lattices are used to\nminimise the amount of new notation for concepts such as the subset relation,\nunion and intersection. Basic concepts — Cartesian products, disjoint sums,\nnatural numbers, functions, etc. — are formalised.</p>\n<p>More advanced set-theoretic concepts, such as transfinite induction,\nordinals, cardinals and the transitive closure of a set, are also provided.\nThe definition of addition and multiplication for general sets (not just\nordinals) follows Kirby.</p>\n<p>The theory provides two type classes with the aim of facilitating\ndevelopments that combine <em>V</em> with other Isabelle/HOL types:\n<em>embeddable</em>, the class of types that can be injected into <em>V</em>\n(including <em>V</em> itself as well as <em>V*V</em>, etc.), and\n<em>small</em>, the class of types that correspond to some ZF set.</p>\nextra-history =\nChange history:\n[2020-01-28]:  Generalisation of the \"small\" predicate and order types to arbitrary sets;\nordinal exponentiation;\nintroduction of the coercion ord_of_nat :: \"nat => V\";\nnumerous new lemmas. (revision 6081d5be8d08)",
        "licence": "BSD",
        "theories": [
            "ZFC_Library",
            "ZFC_in_HOL",
            "ZFC_Cardinals",
            "Kirby",
            "Ordinal_Exp",
            "Cantor_NF",
            "ZFC_Typeclasses"
        ]
    },
    {
        "session": "Group-Ring-Module",
        "title": "Groups, Rings and Modules",
        "authors": [
            "Hidetsune Kobayashi",
            "L. Chen",
            "H. Murao"
        ],
        "date": "2004-05-18",
        "topics": [
            "Mathematics/Algebra"
        ],
        "abstract": "The theory of groups, rings and modules is developed to a great depth. Group theory results include Zassenhaus's theorem and the Jordan-Hoelder theorem. The ring theory development includes ideals, quotient rings and the Chinese remainder theorem. The module development includes the Nakayama lemma, exact sequences and Tensor products.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-03-02"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-24"
            },
            {
                "2011-1": "2011-10-11"
            },
            {
                "2011": "2011-02-11"
            },
            {
                "2009-2": "2010-07-01"
            },
            {
                "2009-1": "2009-12-12"
            },
            {
                "2009": "2009-04-30"
            },
            {
                "2009": "2009-04-29"
            },
            {
                "2008": "2008-06-10"
            },
            {
                "2007": "2007-11-27"
            },
            {
                "2005": "2005-10-14"
            },
            {
                "2004": "2004-05-20"
            },
            {
                "2004": "2004-05-19"
            }
        ],
        "theories": [
            "Algebra1",
            "Algebra2",
            "Algebra3",
            "Algebra4",
            "Algebra5",
            "Algebra6",
            "Algebra7",
            "Algebra8",
            "Algebra9"
        ]
    },
    {
        "session": "Bondy",
        "title": "Bondy's Theorem",
        "authors": [
            "Jeremy Avigad",
            "Stefan Hetzl"
        ],
        "date": "2012-10-27",
        "topics": [
            "Mathematics/Combinatorics"
        ],
        "abstract": "A proof of Bondy's theorem following B. Bollabas, Combinatorics, 1986, Cambridge University Press.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-10-27"
            }
        ],
        "theories": [
            "Bondy"
        ]
    },
    {
        "session": "Subset_Boolean_Algebras",
        "title": "A Hierarchy of Algebras for Boolean Subsets",
        "authors": [
            "Walter Guttmann",
            "Bernhard Möller"
        ],
        "topics": [
            "Mathematics/Algebra"
        ],
        "date": "2020-01-31",
        "abstract": "\nWe present a collection of axiom systems for the construction of\nBoolean subalgebras of larger overall algebras. The subalgebras are\ndefined as the range of a complement-like operation on a semilattice.\nThis technique has been used, for example, with the antidomain\noperation, dynamic negation and Stone algebras. We present a common\nground for these constructions based on a new equational\naxiomatisation of Boolean algebras.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2020-01-31"
            }
        ],
        "dependencies": [
            "Stone_Algebras"
        ],
        "theories": [
            "Subset_Boolean_Algebras"
        ]
    },
    {
        "session": "Deep_Learning",
        "title": "Expressiveness of Deep Learning",
        "authors": [
            "Alexander Bentkamp"
        ],
        "date": "2016-11-10",
        "topics": [
            "Computer science/Machine learning",
            "Mathematics/Analysis"
        ],
        "abstract": "\nDeep learning has had a profound impact on computer science in recent years, with applications to search engines, image recognition and language processing, bioinformatics, and more. Recently, Cohen et al. provided theoretical evidence for the superiority of deep learning over shallow learning. This formalization of their work simplifies and generalizes the original proof, while working around the limitations of the Isabelle type system. To support the formalization, I developed reusable libraries of formalized mathematics, including results about the matrix rank, the Lebesgue measure, and multivariate polynomials, as well as a library for tensor analysis.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-11-10"
            }
        ],
        "dependencies": [
            "Jordan_Normal_Form",
            "Polynomial_Interpolation",
            "Polynomials",
            "VectorSpace"
        ],
        "theories": [
            "Tensor",
            "Tensor_Subtensor",
            "Tensor_Plus",
            "Tensor_Scalar_Mult",
            "Tensor_Product",
            "Tensor_Unit_Vec",
            "Tensor_Rank",
            "Tensor_Matricization",
            "DL_Rank_CP_Rank",
            "DL_Flatten_Matrix",
            "DL_Network",
            "DL_Concrete_Matrices",
            "DL_Missing_Finite_Set",
            "DL_Deep_Model",
            "DL_Deep_Model_Poly",
            "Lebesgue_Functional",
            "Lebesgue_Zero_Set",
            "DL_Shallow_Model",
            "DL_Fundamental_Theorem_Network_Capacity"
        ]
    },
    {
        "session": "Latin_Square",
        "title": "Latin Square",
        "authors": [
            "Alexander Bentkamp"
        ],
        "date": "2015-12-02",
        "topics": [
            "Mathematics/Combinatorics"
        ],
        "abstract": "\nA Latin Square is a n x n table filled with integers from 1 to n where each number appears exactly once in each row and each column. A Latin Rectangle is a partially filled n x n table with r filled rows and n-r empty rows, such that each number appears at most once in each row and each column. The main result of this theory is that any Latin Rectangle can be completed to a Latin Square.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-12-03"
            }
        ],
        "dependencies": [
            "Marriage"
        ],
        "theories": [
            "Latin_Square"
        ]
    },
    {
        "session": "FOL_Seq_Calc1",
        "title": "A Sequent Calculus for First-Order Logic",
        "authors": [
            "Asta Halkjær From"
        ],
        "contributors": [
            "Alexander Birch Jensen",
            "Anders Schlichtkrull",
            "Jørgen Villadsen"
        ],
        "topics": [
            "Logic/Proof theory"
        ],
        "date": "2019-07-18",
        "abstract": "\nThis work formalizes soundness and completeness of a one-sided sequent\ncalculus for first-order logic. The completeness is shown via a\ntranslation from a complete semantic tableau calculus, the proof of\nwhich is based on the First-Order Logic According to Fitting theory.\nThe calculi and proof techniques are taken from Ben-Ari's\nMathematical Logic for Computer Science.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-07-18"
            }
        ],
        "dependencies": [
            "FOL-Fitting"
        ],
        "theories": [
            "Common",
            "Tableau",
            "Sequent"
        ]
    },
    {
        "session": "CSP_RefTK",
        "title": "The HOL-CSP Refinement Toolkit",
        "authors": [
            "Safouan Taha",
            "Burkhart Wolff",
            "Lina Ye"
        ],
        "topics": [
            "Computer science/Concurrency/Process calculi",
            "Computer science/Semantics"
        ],
        "date": "2020-11-19",
        "abstract": "\nWe use a formal development for CSP, called HOL-CSP2.0, to analyse a\nfamily of refinement notions, comprising classic and new ones. This\nanalysis enables to derive a number of properties that allow to deepen\nthe understanding of these notions, in particular with respect to\nspecification decomposition principles for the case of infinite sets\nof events. The established relations between the refinement relations\nhelp to clarify some obscure points in the CSP literature, but also\nprovide a weapon for shorter refinement proofs. Furthermore, we\nprovide a framework for state-normalisation allowing to formally\nreason on parameterised process architectures. As a result, we have a\nmodern environment for formal proofs of concurrent systems that allow\nfor the combination of general infinite processes with locally finite\nones in a logically safe way. We demonstrate these\nverification-techniques for classical, generalised examples: The\nCopyBuffer for arbitrary data and the Dijkstra's Dining\nPhilosopher Problem of arbitrary size.",
        "licence": "BSD",
        "theories": [
            "Introduction",
            "Assertions_ext",
            "Properties",
            "Fix_ind_ext",
            "Process_norm",
            "CopyBuffer_props",
            "DiningPhilosophers",
            "Conclusion"
        ]
    },
    {
        "session": "IP_Addresses",
        "title": "IP Addresses",
        "authors": [
            "Cornelius Diekmann",
            "Julius Michaelis",
            "Lars Hupel"
        ],
        "date": "2016-06-28",
        "topics": [
            "Computer science/Networks"
        ],
        "abstract": "\nThis entry contains a definition of IP addresses and a library to work\nwith them.  Generic IP addresses are modeled as machine words of\narbitrary length. Derived from this generic definition, IPv4 addresses\nare 32bit machine words, IPv6 addresses are 128bit words.\nAdditionally, IPv4 addresses can be represented in dot-decimal\nnotation and IPv6 addresses in (compressed) colon-separated notation.\nWe support toString functions and parsers for both notations. Sets of\nIP addresses can be represented with a netmask (e.g.\n192.168.0.0/255.255.0.0) or in CIDR notation (e.g. 192.168.0.0/16). To\nprovide executable code for set operations on IP address ranges, the\nlibrary includes a datatype to work on arbitrary intervals of machine\nwords.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-06-28"
            }
        ],
        "dependencies": [
            "Word_Lib",
            "Automatic_Refinement"
        ],
        "theories": [
            "NumberWang_IPv4",
            "NumberWang_IPv6",
            "WordInterval",
            "Hs_Compat",
            "IP_Address",
            "IPv4",
            "IPv6",
            "Prefix_Match",
            "CIDR_Split",
            "WordInterval_Sorted",
            "IP_Address_Parser",
            "Lib_Numbers_toString",
            "Lib_Word_toString",
            "Lib_List_toString",
            "IP_Address_toString",
            "Prefix_Match_toString"
        ]
    },
    {
        "session": "Relational-Incorrectness-Logic",
        "title": "An Under-Approximate Relational Logic",
        "authors": [
            "Toby Murray"
        ],
        "topics": [
            "Computer science/Programming languages/Logics",
            "Computer science/Security"
        ],
        "date": "2020-03-12",
        "abstract": "\nRecently, authors have proposed under-approximate logics for reasoning\nabout programs. So far, all such logics have been confined to\nreasoning about individual program behaviours. Yet there exist many\nover-approximate relational logics for reasoning about pairs of\nprograms and relating their behaviours. We present the first\nunder-approximate relational logic, for the simple imperative language\nIMP. We prove our logic is both sound and complete. Additionally, we\nshow how reasoning in this logic can be decomposed into non-relational\nreasoning in an under-approximate Hoare logic, mirroring Beringer’s\nresult for over-approximate relational logics. We illustrate the\napplication of our logic on some small examples in which we provably\ndemonstrate the presence of insecurity.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2020-03-26"
            }
        ],
        "theories": [
            "RelationalIncorrectness"
        ]
    },
    {
        "session": "Functional-Automata",
        "title": "Functional Automata",
        "authors": [
            "Tobias Nipkow"
        ],
        "date": "2004-03-30",
        "topics": [
            "Computer science/Automata and formal languages"
        ],
        "abstract": "This theory defines deterministic and nondeterministic automata in a functional representation: the transition function/relation and the finality predicate are just functions. Hence the state space may be infinite. It is shown how to convert regular expressions into such automata. A scanner (generator) is implemented with the help of functional automata: the scanner chops the input up into longest recognized substrings. Finally we also show how to convert a certain subclass of functional automata (essentially the finite deterministic ones) into regular sets.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-03-02"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-24"
            },
            {
                "2011-1": "2011-10-11"
            },
            {
                "2011": "2011-02-11"
            },
            {
                "2009-2": "2010-07-01"
            },
            {
                "2009-1": "2009-12-12"
            },
            {
                "2009": "2009-04-29"
            },
            {
                "2008": "2008-06-10"
            },
            {
                "2007": "2007-11-27"
            },
            {
                "2005": "2005-10-14"
            },
            {
                "2004": "2004-05-21"
            },
            {
                "2004": "2004-04-20"
            },
            {
                "2003": "2004-03-30"
            }
        ],
        "dependencies": [
            "Regular-Sets"
        ],
        "theories": [
            "AutoProj",
            "DA",
            "NA",
            "NAe",
            "Automata",
            "RegExp2NA",
            "RegExp2NAe",
            "AutoRegExp",
            "MaxPrefix",
            "MaxChop",
            "AutoMaxChop",
            "RegSet_of_nat_DA",
            "Execute",
            "Functional_Automata"
        ]
    },
    {
        "session": "SuperCalc",
        "title": "A Variant of the Superposition Calculus",
        "authors": [
            "Nicolas Peltier"
        ],
        "date": "2016-09-06",
        "topics": [
            "Logic/Proof theory"
        ],
        "abstract": "\nWe provide a formalization of a variant of the superposition\ncalculus, together with formal proofs of soundness and refutational\ncompleteness (w.r.t. the usual redundancy criteria based on clause\nordering). This version of the calculus uses all the standard\nrestrictions of the superposition rules, together with the following\nrefinement, inspired by the basic superposition calculus: each clause\nis associated with a set of terms which are assumed to be in normal\nform -- thus any application of the replacement rule on these terms is\nblocked. The set is initially empty and terms may be added or removed\nat each inference step. The set of terms that are assumed to be in\nnormal form includes any term introduced by previous unifiers as well\nas any term occurring in the parent clauses at a position that is\nsmaller (according to some given ordering on positions) than a\npreviously replaced term. The standard superposition calculus\ncorresponds to the case where the set of irreducible terms is always\nempty.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-09-06"
            }
        ],
        "theories": [
            "multisets_continued",
            "well_founded_continued",
            "terms",
            "equational_clausal_logic",
            "superposition"
        ]
    },
    {
        "session": "Hybrid_Logic",
        "title": "Formalizing a Seligman-Style Tableau System for Hybrid Logic",
        "authors": [
            "Asta Halkjær From"
        ],
        "topics": [
            "Logic/General logic/Modal logic"
        ],
        "date": "2019-12-20",
        "abstract": "\nThis work is a formalization of soundness and completeness proofs\nfor a Seligman-style tableau system for hybrid logic. The completeness\nresult is obtained via a synthetic approach using maximally\nconsistent sets of tableau blocks. The formalization differs from\nprevious work in a few ways. First, to avoid the need to backtrack in\nthe construction of a tableau, the formalized system has no unnamed\ninitial segment, and therefore no Name rule. Second, I show that the\nfull Bridge rule is admissible in the system. Third, I start from rules\nrestricted to only extend the branch with new formulas, including only\nwitnessing diamonds that are not already witnessed, and show that\nthe unrestricted rules are admissible. Similarly, I start from simpler\nversions of the @-rules and show that these are sufficient.\nThe GoTo rule is restricted using a notion of potential such that each\napplication consumes potential and potential is earned through applications of\nthe remaining rules. I show that if a branch can be closed then it can\nbe closed starting from a single unit. Finally, Nom is restricted by\na fixed set of allowed nominals. The resulting system should be terminating.",
        "extra": {
            "Change history": "[2020-06-03] The fully restricted system has been shown complete by updating the synthetic completeness proof."
        },
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2020-01-07"
            }
        ],
        "theories": [
            "Hybrid_Logic"
        ]
    },
    {
        "session": "WHATandWHERE_Security",
        "title": "A Formalization of Declassification with WHAT-and-WHERE-Security",
        "authors": [
            "Sylvia Grewe",
            "Alexander Lux",
            "Heiko Mantel",
            "Jens Sauer"
        ],
        "date": "2014-04-23",
        "topics": [
            "Computer science/Security",
            "Computer science/Programming languages/Type systems"
        ],
        "abstract": "Research in information-flow security aims at developing methods to\nidentify undesired information leaks within programs from private\nsources to public sinks. Noninterference captures this intuition by\nrequiring that no information whatsoever flows from private sources\nto public sinks. However, in practice this definition is often too\nstrict: Depending on the intuitive desired security policy, the\ncontrolled declassification of certain private information (WHAT) at\ncertain points in the program (WHERE) might not result in an\nundesired information leak.\n<p>\nWe present an Isabelle/HOL formalization of such a security property\nfor controlled declassification, namely WHAT&WHERE-security from\n\"Scheduler-Independent Declassification\" by Lux, Mantel, and Perner.\nThe formalization includes\ncompositionality proofs for and a soundness proof for a security\ntype system that checks for programs in a simple while language with\ndynamic thread creation.\n<p>\nOur formalization of the security type system is abstract in the\nlanguage for expressions and in the semantic side conditions for\nexpressions. It can easily be instantiated with different syntactic\napproximations for these side conditions. The soundness proof of\nsuch an instantiation boils down to showing that these syntactic\napproximations imply the semantic side conditions.\n<p>\nThis Isabelle/HOL formalization uses theories from the entry\nStrong Security.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2014-04-24"
            }
        ],
        "dependencies": [
            "Strong_Security"
        ],
        "theories": [
            "WHATWHERE_Security",
            "Up_To_Technique",
            "MWLs",
            "Parallel_Composition",
            "WHATWHERE_Secure_Skip_Assign",
            "Language_Composition",
            "Type_System",
            "Type_System_example"
        ]
    },
    {
        "session": "LambdaAuth",
        "title": "Formalization of Generic Authenticated Data Structures",
        "authors": [
            "Matthias Brun",
            "Dmitriy Traytel"
        ],
        "topics": [
            "Computer science/Security",
            "Computer science/Programming languages/Lambda calculi"
        ],
        "date": "2019-05-14",
        "abstract": "\nAuthenticated data structures are a technique for outsourcing data\nstorage and maintenance to an untrusted server. The server is required\nto produce an efficiently checkable and cryptographically secure proof\nthat it carried out precisely the requested computation. <a\nhref=\"https://doi.org/10.1145/2535838.2535851\">Miller et\nal.</a> introduced &lambda;&bull; (pronounced\n<i>lambda auth</i>)&mdash;a functional programming\nlanguage with a built-in primitive authentication construct, which\nsupports a wide range of user-specified authenticated data structures\nwhile guaranteeing certain correctness and security properties for all\nwell-typed programs. We formalize &lambda;&bull; and prove its\ncorrectness and security properties. With Isabelle's help, we\nuncover and repair several mistakes in the informal proofs and lemma\nstatements. Our findings are summarized in a <a\nhref=\"http://people.inf.ethz.ch/trayteld/papers/lambdaauth/lambdaauth.pdf\">paper\ndraft</a>.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2019-05-15"
            }
        ],
        "dependencies": [
            "Nominal2"
        ],
        "theories": [
            "Nominal2_Lemmas",
            "FMap_Lemmas",
            "Syntax",
            "Semantics",
            "Agreement",
            "Results"
        ]
    },
    {
        "session": "Paraconsistency",
        "title": "Paraconsistency",
        "authors": [
            "Anders Schlichtkrull",
            "Jørgen Villadsen"
        ],
        "topics": [
            "Logic/General logic/Paraconsistent logics"
        ],
        "date": "2016-12-07",
        "abstract": "\nParaconsistency is about handling inconsistency in a coherent way. In\nclassical and intuitionistic logic everything follows from an\ninconsistent theory. A paraconsistent logic avoids the explosion.\nQuite a few applications in computer science and engineering are\ndiscussed in the Intelligent Systems Reference Library Volume 110:\nTowards Paraconsistent Engineering (Springer 2016). We formalize a\nparaconsistent many-valued logic that we motivated and described in a\nspecial issue on logical approaches to paraconsistency (Journal of\nApplied Non-Classical Logics 2005). We limit ourselves to the\npropositional fragment of the higher-order logic. The logic is based\non so-called key equalities and has a countably infinite number of\ntruth values. We prove theorems in the logic using the definition of\nvalidity. We verify truth tables and also counterexamples for\nnon-theorems. We prove meta-theorems about the logic and finally we\ninvestigate a case study.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-12-08"
            }
        ],
        "theories": [
            "Paraconsistency"
        ]
    },
    {
        "session": "Winding_Number_Eval",
        "title": "Evaluate Winding Numbers through Cauchy Indices",
        "authors": [
            "Wenda Li"
        ],
        "topics": [
            "Mathematics/Analysis"
        ],
        "date": "2017-10-17",
        "abstract": "\nIn complex analysis, the winding number measures the number of times a\npath (counterclockwise) winds around a point, while the Cauchy index\ncan approximate how the path winds. This entry provides a\nformalisation of the Cauchy index, which is then shown to be related\nto the winding number. In addition, this entry also offers a tactic\nthat enables users to evaluate the winding number by calculating\nCauchy indices.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-18"
            }
        ],
        "dependencies": [
            "Sturm_Tarski",
            "Budan_Fourier"
        ],
        "theories": [
            "Missing_Topology",
            "Missing_Algebraic",
            "Missing_Transcendental",
            "Missing_Analysis",
            "Cauchy_Index_Theorem",
            "Winding_Number_Eval",
            "Winding_Number_Eval_Examples"
        ]
    },
    {
        "session": "Depth-First-Search",
        "title": "Depth First Search",
        "authors": [
            "Toshiaki Nishihara",
            "Yasuhiko Minamide"
        ],
        "date": "2004-06-24",
        "topics": [
            "Computer science/Algorithms/Graph"
        ],
        "abstract": "Depth-first search of a graph is formalized with recdef. It is shown that it visits all of the reachable nodes from a given list of nodes. Executable ML code of depth-first search is obtained using the code generation feature of Isabelle/HOL.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-24"
            },
            {
                "2011-1": "2011-10-11"
            },
            {
                "2011": "2011-02-11"
            },
            {
                "2009-2": "2010-06-30"
            },
            {
                "2009-1": "2009-12-12"
            },
            {
                "2009": "2009-04-29"
            },
            {
                "2008": "2008-06-10"
            },
            {
                "2007": "2007-11-27"
            },
            {
                "2005": "2005-10-14"
            },
            {
                "2004": "2004-06-24"
            }
        ],
        "theories": [
            "DFS"
        ]
    },
    {
        "session": "LTL_Normal_Form",
        "title": "An Efficient Normalisation Procedure for Linear Temporal Logic: Isabelle/HOL Formalisation",
        "authors": [
            "Salomon Sickert"
        ],
        "topics": [
            "Computer science/Automata and formal languages",
            "Logic/General logic/Temporal logic"
        ],
        "date": "2020-05-08",
        "abstract": "\nIn the mid 80s, Lichtenstein, Pnueli, and Zuck proved a classical\ntheorem stating that every formula of Past LTL (the extension of LTL\nwith past operators) is equivalent to a formula of the form\n$\\bigwedge_{i=1}^n \\mathbf{G}\\mathbf{F} \\varphi_i \\vee\n\\mathbf{F}\\mathbf{G} \\psi_i$,  where $\\varphi_i$ and $\\psi_i$ contain\nonly past operators. Some years later, Chang, Manna, and Pnueli built\non this result to derive a similar normal form for LTL. Both\nnormalisation procedures have a non-elementary worst-case blow-up, and\nfollow an involved path from formulas to counter-free automata to\nstar-free regular expressions and back to formulas. We improve on both\npoints. We present an executable formalisation of a direct and purely\nsyntactic normalisation procedure for LTL yielding a normal form,\ncomparable to the one by Chang, Manna, and Pnueli, that has only a\nsingle exponential blow-up.",
        "licence": "BSD",
        "dependencies": [
            "LTL",
            "LTL_Master_Theorem"
        ],
        "theories": [
            "Normal_Form",
            "Normal_Form_Complexity",
            "Normal_Form_Code_Export"
        ]
    },
    {
        "session": "E_Transcendental",
        "title": "The Transcendence of e",
        "authors": [
            "Manuel Eberl"
        ],
        "topics": [
            "Mathematics/Analysis",
            "Mathematics/Number theory"
        ],
        "date": "2017-01-12",
        "abstract": "\n<p>This work contains a proof that Euler's number e is transcendental. The\nproof follows the standard approach of assuming that e is algebraic and\nthen using a specific integer polynomial to derive two inconsistent bounds,\nleading to a contradiction.</p> <p>This kind of approach can be found in\nmany different sources; this formalisation mostly follows a <a  href=\"http://planetmath.org/proofoflindemannweierstrasstheoremandthateandpiaretranscendental\">PlanetMath article</a> by Roger Lipsett.</p>",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2017-01-13"
            }
        ],
        "theories": [
            "E_Transcendental"
        ]
    },
    {
        "session": "TLA",
        "title": "A Definitional Encoding of TLA* in Isabelle/HOL",
        "authors": [
            "Gudmund Grov",
            "Stephan Merz"
        ],
        "date": "2011-11-19",
        "topics": [
            "Computer science/Programming languages/Logics"
        ],
        "abstract": "We mechanise the logic TLA*\n<a href=\"http://www.springerlink.com/content/ax3qk557qkdyt7n6/\">[Merz 1999]</a>,\nan extension of Lamport's  Temporal Logic of Actions (TLA)\n<a href=\"http://dl.acm.org/citation.cfm?doid=177492.177726\">[Lamport 1994]</a>\nfor specifying and reasoning\nabout concurrent and reactive systems. Aiming at a framework for mechanising]  the verification of TLA (or TLA*) specifications, this contribution reuses\nsome elements from a previous axiomatic encoding of TLA in Isabelle/HOL\nby the second author [Merz 1998], which has been part of the Isabelle\ndistribution. In contrast to that previous work, we give here a shallow,\ndefinitional embedding, with the following highlights:\n<ul>\n<li>a theory of infinite sequences, including a formalisation of the concepts of stuttering invariance central to TLA and TLA*;\n<li>a definition of the semantics of TLA*, which extends TLA by a mutually-recursive definition of formulas and pre-formulas, generalising TLA action formulas;\n<li>a substantial set of derived proof rules, including the TLA* axioms and Lamport's proof rules for system verification;\n<li>a set of examples illustrating the usage of Isabelle/TLA* for reasoning about systems.\n</ul>\nNote that this work is unrelated to the ongoing development of a proof system\nfor the specification language TLA+, which includes an encoding of TLA+ as a\nnew Isabelle object logic <a href=\"http://www.springerlink.com/content/354026160p14j175/\">[Chaudhuri et al 2010]</a>.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-03-02"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-24"
            },
            {
                "2011-1": "2011-11-27"
            }
        ],
        "theories": [
            "Sequence",
            "Intensional",
            "Semantics",
            "PreFormulas",
            "Rules",
            "Liveness",
            "State",
            "Even",
            "Inc",
            "Buffer"
        ]
    },
    {
        "session": "DOM_Components",
        "title": "A Formalization of Web Components",
        "authors": [
            "Achim D. Brucker",
            "Michael Herzberg"
        ],
        "topics": [
            "Computer science/Data structures"
        ],
        "date": "2020-09-28",
        "abstract": "\nWhile the DOM with shadow trees provide the technical basis for\ndefining web components, the DOM standard neither defines the concept\nof web components nor specifies the safety properties that web\ncomponents should guarantee. Consequently, the standard also does not\ndiscuss how or even if the methods for modifying the DOM respect\ncomponent boundaries.  In AFP entry, we present a formally verified\nmodel of web components and define safety properties which ensure that\ndifferent web components can only interact with each other using\nwell-defined interfaces. Moreover, our verification of the application\nprogramming interface (API) of the DOM revealed numerous invariants\nthat implementations of the DOM API need to preserve to ensure the\nintegrity of components.",
        "licence": "BSD",
        "dependencies": [
            "Shadow_DOM"
        ],
        "theories": [
            "Core_DOM_Components",
            "Shadow_DOM_Components",
            "fancy_tabs"
        ]
    },
    {
        "session": "Amortized_Complexity",
        "title": "Amortized Complexity Verified",
        "authors": [
            "Tobias Nipkow"
        ],
        "date": "2014-07-07",
        "topics": [
            "Computer science/Data structures"
        ],
        "abstract": "\nA framework for the analysis of the amortized complexity of functional\ndata structures is formalized in Isabelle/HOL and applied to a number of\nstandard examples and to the folowing non-trivial ones: skew heaps,\nsplay trees, splay heaps and pairing heaps.\n<p>\nA preliminary version of this work (without pairing heaps) is described\nin a <a href=\"http://www21.in.tum.de/~nipkow/pubs/itp15.html\">paper</a>\npublished in the proceedings of the conference on Interactive\nTheorem Proving ITP 2015. An extended version of this publication\nis available <a href=\"http://www21.in.tum.de/~nipkow/pubs/jfp16.html\">here</a>.",
        "extra": {
            "Change history": "[2015-03-17] Added pairing heaps by Hauke Brinkop.<br>\n[2016-07-12] Moved splay heaps from here to Splay_Tree<br>\n[2016-07-14] Moved pairing heaps from here to the new Pairing_Heap"
        },
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-28"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2015-05-19"
            },
            {
                "2014": "2014-08-28"
            }
        ],
        "dependencies": [
            "Pairing_Heap",
            "Skew_Heap",
            "Splay_Tree"
        ],
        "theories": [
            "Amortized_Framework0",
            "Amortized_Framework",
            "Amortized_Examples",
            "Priority_Queue_ops_merge",
            "Skew_Heap_Analysis",
            "Lemmas_log",
            "Splay_Tree_Analysis_Base",
            "Splay_Tree_Analysis",
            "Splay_Tree_Analysis_Optimal",
            "Priority_Queue_ops",
            "Splay_Heap_Analysis",
            "Pairing_Heap_Tree_Analysis",
            "Pairing_Heap_Tree_Analysis2",
            "Pairing_Heap_List1_Analysis",
            "Pairing_Heap_List1_Analysis2",
            "Pairing_Heap_List2_Analysis"
        ]
    },
    {
        "session": "Open_Induction",
        "title": "Open Induction",
        "authors": [
            "Mizuhito Ogawa",
            "Christian Sternagel"
        ],
        "date": "2012-11-02",
        "topics": [
            "Mathematics/Combinatorics"
        ],
        "abstract": "\nA proof of the open induction schema based on J.-C. Raoult, Proving open properties by induction, <i>Information Processing Letters</i> 29, 1988, pp.19-23.\n<p>This research was supported by the Austrian Science Fund (FWF): J3202.</p>",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-03-02"
            },
            {
                "2013": "2013-02-16"
            }
        ],
        "theories": [
            "Restricted_Predicates",
            "Open_Induction"
        ]
    },
    {
        "session": "Kruskal",
        "title": "Kruskal's Algorithm for Minimum Spanning Forest",
        "authors": [
            "Maximilian P. L. Haslbeck",
            "Peter Lammich",
            "Julian Biendarra"
        ],
        "topics": [
            "Computer science/Algorithms/Graph"
        ],
        "date": "2019-02-14",
        "abstract": "\nThis Isabelle/HOL formalization defines a greedy algorithm for finding\na minimum weight basis on a weighted matroid and proves its\ncorrectness. This algorithm is an abstract version of Kruskal's\nalgorithm.  We interpret the abstract algorithm for the cycle matroid\n(i.e. forests in a graph) and refine it to imperative executable code\nusing an efficient union-find data structure.  Our formalization can\nbe instantiated for different graph representations. We provide\ninstantiations for undirected graphs and symmetric directed graphs.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2019-02-19"
            }
        ],
        "dependencies": [
            "Sepref_IICF",
            "Refine_Monadic",
            "Matroids",
            "Collections",
            "Refine_Imperative_HOL"
        ],
        "theories": [
            "Kruskal_Misc",
            "SeprefUF",
            "MinWeightBasis",
            "Kruskal",
            "Kruskal_Refine",
            "Kruskal_Impl",
            "UGraph",
            "UGraph_Impl",
            "Graph_Definition",
            "Graph_Definition_Aux",
            "Graph_Definition_Impl"
        ]
    },
    {
        "session": "SIFUM_Type_Systems",
        "title": "A Formalization of Assumptions and Guarantees for Compositional Noninterference",
        "authors": [
            "Sylvia Grewe",
            "Heiko Mantel",
            "Daniel Schoepe"
        ],
        "date": "2014-04-23",
        "topics": [
            "Computer science/Security",
            "Computer science/Programming languages/Type systems"
        ],
        "abstract": "Research in information-flow security aims at developing methods to\nidentify undesired information leaks within programs from private\n(high) sources to public (low) sinks. For a concurrent system, it is\ndesirable to have compositional analysis methods that allow for\nanalyzing each thread independently and that nevertheless guarantee\nthat the parallel composition of successfully analyzed threads\nsatisfies a global security guarantee. However, such a compositional\nanalysis should not be overly pessimistic about what an environment\nmight do with shared resources. Otherwise, the analysis will reject\nmany intuitively secure programs.\n<p>\nThe paper \"Assumptions and Guarantees for Compositional\nNoninterference\" by Mantel et. al. presents one solution for this problem:\nan approach for compositionally reasoning about non-interference in\nconcurrent programs via rely-guarantee-style reasoning.  We present an\nIsabelle/HOL formalization of the concepts and proofs of this approach.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2014-04-25"
            },
            {
                "2013-2": "2014-04-24"
            }
        ],
        "theories": [
            "Preliminaries",
            "Security",
            "Compositionality",
            "Language",
            "TypeSystem",
            "LocallySoundModeUse"
        ]
    },
    {
        "session": "Lazy_Case",
        "title": "Lazifying case constants",
        "authors": [
            "Lars Hupel"
        ],
        "topics": [
            "Tools"
        ],
        "date": "2017-04-18",
        "abstract": "\nIsabelle's code generator performs various adaptations for target\nlanguages. Among others, case statements are printed as match\nexpressions. Internally, this is a sophisticated procedure, because in\nHOL, case statements are represented as nested calls to the case\ncombinators as generated by the datatype package. Furthermore, the\nprocedure relies on laziness of match expressions in the target\nlanguage, i.e., that branches guarded by patterns that fail to match\nare not evaluated. Similarly, <tt>if-then-else</tt> is\nprinted to the corresponding construct in the target language. This\nentry provides tooling to replace these special cases in the code\ngenerator by ignoring these target language features, instead printing\ncase expressions and <tt>if-then-else</tt> as functions.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2017-04-20"
            }
        ],
        "theories": [
            "Lazy_Case",
            "files/lazy_case.ML",
            "Test_Lazy_Case"
        ]
    },
    {
        "session": "Regular_Algebras",
        "title": "Regular Algebras",
        "authors": [
            "Simon Foster",
            "Georg Struth"
        ],
        "date": "2014-05-21",
        "topics": [
            "Computer science/Automata and formal languages",
            "Mathematics/Algebra"
        ],
        "abstract": "\nRegular algebras axiomatise the equational theory of regular expressions as induced by\nregular language identity. We use Isabelle/HOL for a detailed systematic study of regular\nalgebras given by Boffa, Conway, Kozen and Salomaa. We investigate the relationships between\nthese classes, formalise a soundness proof for the smallest class (Salomaa's) and obtain\ncompleteness of the largest one (Boffa's) relative to a deep result by Krob. In addition\nwe provide a large collection of regular identities in the general setting of Boffa's axiom.\nOur regular algebra hierarchy is orthogonal to the Kleene algebra hierarchy in the Archive\nof Formal Proofs; we have not aimed at an integration for pragmatic reasons.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2014-05-22"
            }
        ],
        "dependencies": [
            "Kleene_Algebra"
        ],
        "theories": [
            "Dioid_Power_Sum",
            "Regular_Algebras",
            "Regular_Algebra_Models",
            "Pratts_Counterexamples",
            "Regular_Algebra_Variants"
        ]
    },
    {
        "session": "Comparison_Sort_Lower_Bound",
        "title": "Lower bound on comparison-based sorting algorithms",
        "authors": [
            "Manuel Eberl"
        ],
        "topics": [
            "Computer science/Algorithms"
        ],
        "date": "2017-03-15",
        "abstract": "\n<p>This article contains a formal proof of the well-known fact\nthat number of comparisons that a comparison-based sorting algorithm\nneeds to perform to sort a list of length <em>n</em> is at\nleast <em>log<sub>2</sub>&nbsp;(n!)</em>\nin the worst case, i.&thinsp;e.&nbsp;<em>Ω(n log\nn)</em>.</p>  <p>For this purpose, a shallow\nembedding for comparison-based sorting algorithms is defined: a\nsorting algorithm is a recursive datatype containing either a HOL\nfunction or a query of a comparison oracle with a continuation\ncontaining the remaining computation. This makes it possible to force\nthe algorithm to use only comparisons and to track the number of\ncomparisons made.</p>",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2017-03-16"
            }
        ],
        "dependencies": [
            "Stirling_Formula",
            "List-Index",
            "Landau_Symbols"
        ],
        "theories": [
            "Linorder_Relations",
            "Comparison_Sort_Lower_Bound"
        ]
    },
    {
        "session": "Derangements",
        "title": "Derangements Formula",
        "authors": [
            "Lukas Bulwahn"
        ],
        "date": "2015-06-27",
        "topics": [
            "Mathematics/Combinatorics"
        ],
        "abstract": "\nThe Derangements Formula describes the number of fixpoint-free permutations\nas a closed formula. This theorem is the 88th theorem in a list of the\n``<a href=\"http://www.cs.ru.nl/~freek/100/\">Top 100 Mathematical Theorems</a>''.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-11-20"
            },
            {
                "2015": "2015-06-28"
            }
        ],
        "theories": [
            "Derangements"
        ]
    },
    {
        "session": "Combinatorics_Words_Graph_Lemma",
        "title": "Graph Lemma",
        "authors": [
            "Štěpán Holub",
            "Štěpán Starosta"
        ],
        "topics": [
            "Computer science/Automata and formal languages"
        ],
        "date": "2021-05-24",
        "abstract": "\nGraph lemma quantifies the defect effect of a system of word\nequations. That is, it provides an upper bound on the rank of the\nsystem. We formalize the proof based on the decomposition of a\nsolution into its free basis. A direct application is an alternative\nproof of the fact that two noncommuting words form a code.",
        "licence": "BSD",
        "dependencies": [
            "Combinatorics_Words"
        ],
        "theories": [
            "Graph_Lemma"
        ]
    },
    {
        "session": "VerifyThis2019",
        "title": "VerifyThis 2019 -- Polished Isabelle Solutions",
        "authors": [
            "Peter Lammich",
            "Simon Wimmer"
        ],
        "topics": [
            "Computer science/Algorithms"
        ],
        "date": "2019-10-16",
        "abstract": "\nVerifyThis 2019 (http://www.pm.inf.ethz.ch/research/verifythis.html)\nwas a program verification competition associated with ETAPS 2019. It\nwas the 8th event in the VerifyThis competition series. In this entry,\nwe present polished and completed versions of our solutions that we\ncreated during the competition.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-10-25"
            }
        ],
        "dependencies": [
            "Sepref_IICF"
        ],
        "theories": [
            "Exc_Nres_Monad",
            "VTcomp",
            "Challenge1A",
            "Challenge1B",
            "Challenge2A",
            "Challenge2B",
            "Parallel_Multiset_Fold",
            "Challenge3"
        ]
    },
    {
        "session": "IMP2",
        "title": "IMP2 – Simple Program Verification in Isabelle/HOL",
        "authors": [
            "Peter Lammich",
            "Simon Wimmer"
        ],
        "topics": [
            "Computer science/Programming languages/Logics",
            "Computer science/Algorithms"
        ],
        "date": "2019-01-15",
        "abstract": "\nIMP2 is a simple imperative language together with Isabelle tooling to\ncreate a program verification environment in Isabelle/HOL. The tools\ninclude a C-like syntax, a verification condition generator, and\nIsabelle commands for the specification of programs. The framework is\nmodular, i.e., it allows easy reuse of already proved programs within\nlarger programs.  This entry comes with a quickstart guide and a large\ncollection of examples, spanning basic algorithms with simple proofs\nto more advanced algorithms and proof techniques like data refinement.\nSome highlights from the examples are: <ul> <li>Bisection\nSquare Root, </li> <li>Extended Euclid,  </li>\n<li>Exponentiation by Squaring,  </li> <li>Binary\nSearch,  </li> <li>Insertion Sort,  </li>\n<li>Quicksort,  </li> <li>Depth First Search.\n</li> </ul>  The abstract syntax and semantics are very\nsimple and well-documented. They are suitable to be used in a course,\nas extension to the IMP language which comes with the Isabelle\ndistribution.  While this entry is limited to a simple imperative\nlanguage, the ideas could be extended to more sophisticated languages.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2019-01-15"
            }
        ],
        "theories": [
            "IMP2_Aux_Lemmas",
            "IMP2_Utils",
            "Named_Simpsets",
            "files/named_simpsets.ML",
            "Subgoal_Focus_Some",
            "files/subgoal_focus_some.ML",
            "Syntax",
            "Semantics",
            "Annotated_Syntax",
            "IMP2_Basic_Simpset",
            "Parser",
            "IMP2_Basic_Decls",
            "IMP2_Program_Analysis",
            "IMP2_Var_Postprocessor",
            "IMP2_Var_Abs",
            "IMP2_VCG",
            "IMP2_Specification",
            "IMP2",
            "Quickstart_Guide",
            "IMP2_from_IMP",
            "Examples"
        ]
    },
    {
        "session": "Signature_Groebner",
        "title": "Signature-Based Gröbner Basis Algorithms",
        "authors": [
            "Alexander Maletzky"
        ],
        "topics": [
            "Mathematics/Algebra",
            "Computer science/Algorithms/Mathematical"
        ],
        "date": "2018-09-20",
        "abstract": "\n<p>This article formalizes signature-based algorithms for computing\nGr&ouml;bner bases. Such algorithms are, in general, superior to\nother algorithms in terms of efficiency, and have not been formalized\nin any proof assistant so far. The present development is both\ngeneric, in the sense that most known variants of signature-based\nalgorithms are covered by it, and effectively executable on concrete\ninput thanks to Isabelle's code generator. Sample computations of\nbenchmark problems show that the verified implementation of\nsignature-based algorithms indeed outperforms the existing\nimplementation of Buchberger's algorithm in Isabelle/HOL.</p>\n<p>Besides total correctness of the algorithms, the article also proves\nthat under certain conditions they a-priori detect and avoid all\nuseless zero-reductions, and always return 'minimal' (in\nsome sense) Gr&ouml;bner bases if an input parameter is chosen in\nthe right way.</p><p>The formalization follows the recent survey article by\nEder and Faug&egrave;re.</p>",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-09-20"
            }
        ],
        "dependencies": [
            "Groebner_Bases"
        ],
        "theories": [
            "Prelims",
            "More_MPoly",
            "Signature_Groebner",
            "Signature_Examples"
        ]
    },
    {
        "session": "RefinementReactive",
        "title": "Formalization of Refinement Calculus for Reactive Systems",
        "authors": [
            "Viorel Preoteasa"
        ],
        "date": "2014-10-08",
        "topics": [
            "Computer science/Programming languages/Logics"
        ],
        "abstract": "\nWe present a formalization of refinement calculus for reactive systems.\nRefinement calculus is based on monotonic predicate transformers\n(monotonic functions from sets of post-states to sets of pre-states),\nand it is a powerful formalism for reasoning about imperative programs.\nWe model reactive systems as monotonic property transformers\nthat transform sets of output infinite sequences into sets of input\ninfinite sequences. Within this semantics we can model\nrefinement of reactive systems, (unbounded) angelic and\ndemonic nondeterminism, sequential composition, and\nother semantic properties. We can model systems that may\nfail for some inputs, and we can model compatibility of systems.\nWe can specify systems that have liveness properties using\nlinear temporal logic, and we can refine system specifications\ninto systems based on symbolic transitions systems, suitable\nfor implementations.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-10-08"
            }
        ],
        "theories": [
            "Temporal",
            "Refinement",
            "Reactive"
        ]
    },
    {
        "session": "Formal_Puiseux_Series",
        "title": "Formal Puiseux Series",
        "authors": [
            "Manuel Eberl"
        ],
        "topics": [
            "Mathematics/Algebra"
        ],
        "date": "2021-02-17",
        "abstract": "\n<p>Formal Puiseux series are generalisations of formal power\nseries and formal Laurent series that also allow for fractional\nexponents. They have the following general form: \\[\\sum_{i=N}^\\infty\na_{i/d} X^{i/d}\\] where <em>N</em> is an integer and\n<em>d</em> is a positive integer.</p> <p>This\nentry defines these series including their basic algebraic properties.\nFurthermore, it proves the Newton–Puiseux Theorem, namely that the\nPuiseux series over an algebraically closed field of characteristic 0\nare also algebraically closed.</p>",
        "licence": "BSD",
        "dependencies": [
            "Polynomial_Interpolation"
        ],
        "theories": [
            "Puiseux_Polynomial_Library",
            "Puiseux_Laurent_Library",
            "FPS_Hensel",
            "Formal_Puiseux_Series"
        ]
    },
    {
        "session": "Vickrey_Clarke_Groves",
        "title": "VCG - Combinatorial Vickrey-Clarke-Groves Auctions",
        "authors": [
            "Marco B. Caminati",
            "Manfred Kerber",
            "Christoph Lange",
            "Colin Rowat"
        ],
        "date": "2015-04-30",
        "topics": [
            "Mathematics/Games and economics"
        ],
        "abstract": "\nA VCG auction (named after their inventors Vickrey, Clarke, and\nGroves) is a generalization of the single-good, second price Vickrey\nauction to the case of a combinatorial auction (multiple goods, from\nwhich any participant can bid on each possible combination). We\nformalize in this entry VCG auctions, including tie-breaking and prove\nthat the functions for the allocation and the price determination are\nwell-defined. Furthermore we show that the allocation function\nallocates goods only to participants, only goods in the auction are\nallocated, and no good is allocated twice. We also show that the price\nfunction is non-negative. These properties also hold for the\nautomatically extracted Scala code.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2015-05-09"
            },
            {
                "2014": "2015-04-30"
            }
        ],
        "theories": [
            "SetUtils",
            "Partitions",
            "RelationOperators",
            "RelationProperties",
            "Argmax",
            "MiscTools",
            "StrictCombinatorialAuction",
            "Universes",
            "UniformTieBreaking",
            "CombinatorialAuction",
            "CombinatorialAuctionCodeExtraction",
            "FirstPrice",
            "CombinatorialAuctionExamples"
        ]
    },
    {
        "session": "Linear_Inequalities",
        "title": "Linear Inequalities",
        "authors": [
            "Ralph Bottesch",
            "Alban Reynaud",
            "René Thiemann"
        ],
        "topics": [
            "Mathematics/Algebra"
        ],
        "date": "2019-06-21",
        "abstract": "\nWe formalize results about linear inqualities, mainly from\nSchrijver's book. The main results are the proof of the\nfundamental theorem on linear inequalities, Farkas' lemma,\nCarathéodory's theorem, the Farkas-Minkowsky-Weyl theorem, the\ndecomposition theorem of polyhedra, and Meyer's result that the\ninteger hull of a polyhedron is a polyhedron itself. Several theorems\ninclude bounds on the appearing numbers, and in particular we provide\nan a-priori bound on mixed-integer solutions of linear inequalities.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-24"
            }
        ],
        "dependencies": [
            "LLL_Basis_Reduction"
        ],
        "theories": [
            "Missing_Matrix",
            "Missing_VS_Connect",
            "Basis_Extension",
            "Sum_Vec_Set",
            "Integral_Bounded_Vectors",
            "Cone",
            "Convex_Hull",
            "Normal_Vector",
            "Dim_Span",
            "Fundamental_Theorem_Linear_Inequalities",
            "Farkas_Lemma",
            "Farkas_Minkowsky_Weyl",
            "Decomposition_Theorem",
            "Mixed_Integer_Solutions",
            "Integer_Hull"
        ]
    },
    {
        "session": "Verified_SAT_Based_AI_Planning",
        "title": "Verified SAT-Based AI Planning",
        "authors": [
            "Mohammad Abdulaziz",
            "Friedrich Kurz"
        ],
        "topics": [
            "Computer science/Artificial intelligence"
        ],
        "date": "2020-10-29",
        "abstract": "\nWe present an executable formally verified SAT encoding of classical\nAI planning that is based on the encodings by Kautz and Selman and the\none by Rintanen et al. The encoding was experimentally tested and\nshown to be usable for reasonably sized standard AI planning\nbenchmarks. We also use it as a reference to test a state-of-the-art\nSAT-based planner, showing that it sometimes falsely claims that\nproblems have no solutions of certain lengths. The formalisation in\nthis submission was described in an independent publication.",
        "licence": "BSD",
        "dependencies": [
            "AI_Planning_Languages_Semantics",
            "Propositional_Proof_Systems",
            "List-Index"
        ],
        "theories": [
            "List_Supplement",
            "Map_Supplement",
            "CNF_Supplement",
            "CNF_Semantics_Supplement",
            "State_Variable_Representation",
            "STRIPS_Representation",
            "STRIPS_Semantics",
            "SAS_Plus_Representation",
            "SAS_Plus_Semantics",
            "SAS_Plus_STRIPS",
            "SAT_Plan_Base",
            "SAT_Plan_Extensions",
            "SAT_Solve_SAS_Plus",
            "AST_SAS_Plus_Equivalence",
            "Set2_Join_RBT",
            "Solve_SASP"
        ]
    },
    {
        "session": "IFC_Tracking",
        "title": "Information Flow Control via Dependency Tracking",
        "authors": [
            "Benedikt Nordhoff"
        ],
        "topics": [
            "Computer science/Security"
        ],
        "date": "2021-04-01",
        "abstract": "\nWe provide a characterisation of how information is propagated by\nprogram executions based on the tracking data and control dependencies\nwithin executions themselves.  The characterisation might be used for\nderiving approximative safety properties to be targeted by static\nanalyses or checked at runtime.  We utilise a simple yet versatile\ncontrol flow graph model as a program representation.  As our model is\nnot assumed to be finite it can be instantiated for a broad class of\nprograms.  The targeted security property is indistinguishable\nsecurity where executions produce sequences of observations and only\nnon-terminating executions are allowed to drop a tail of those.  A\nvery crude approximation of our characterisation is slicing based on\nprogram dependence graphs, which we use as a minimal example and\nderive a corresponding soundness result.  For further details and\napplications refer to the authors upcoming dissertation.",
        "licence": "BSD",
        "theories": [
            "IFC",
            "PDG"
        ]
    },
    {
        "session": "Tarskis_Geometry",
        "title": "The independence of Tarski's Euclidean axiom",
        "authors": [
            "T. J. M. Makarios"
        ],
        "topics": [
            "Mathematics/Geometry"
        ],
        "date": "2012-10-30",
        "abstract": "\nTarski's axioms of plane geometry are formalized and, using the standard\nreal Cartesian model, shown to be consistent. A substantial theory of\nthe projective plane is developed. Building on this theory, the\nKlein-Beltrami model of the hyperbolic plane is defined and shown to\nsatisfy all of Tarski's axioms except his Euclidean axiom; thus Tarski's\nEuclidean axiom is shown to be independent of his other axioms of plane\ngeometry.\n<p>\nAn earlier version of this work was the subject of the author's\n<a href=\"http://researcharchive.vuw.ac.nz/handle/10063/2315\">MSc thesis</a>,\nwhich contains natural-language explanations of some of the\nmore interesting proofs.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-11-09"
            },
            {
                "2012": "2012-11-08"
            }
        ],
        "theories": [
            "Metric",
            "Miscellany",
            "Tarski",
            "Euclid_Tarski",
            "Linear_Algebra2",
            "Action",
            "Projective",
            "Hyperbolic_Tarski"
        ]
    },
    {
        "session": "Fisher_Yates",
        "title": "Fisher–Yates shuffle",
        "authors": [
            "Manuel Eberl"
        ],
        "date": "2016-09-30",
        "topics": [
            "Computer science/Algorithms"
        ],
        "abstract": "\n<p>This work defines and proves the correctness of the Fisher–Yates\nalgorithm for shuffling – i.e. producing a random permutation – of a\nlist. The algorithm proceeds by traversing the list and in\neach step swapping the current element with a random element from the\nremaining list.</p>",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            }
        ],
        "theories": [
            "Fisher_Yates"
        ]
    },
    {
        "session": "Blue_Eyes",
        "title": "Solution to the xkcd Blue Eyes puzzle",
        "authors": [
            "Jakub Kądziołka"
        ],
        "topics": [
            "Logic/General logic/Logics of knowledge and belief"
        ],
        "date": "2021-01-30",
        "abstract": "\nIn a <a href=\"https://xkcd.com/blue_eyes.html\">puzzle published by\nRandall Munroe</a>, perfect logicians forbidden\nfrom communicating are stranded on an island, and may only leave once\nthey have figured out their own eye color. We present a method of\nmodeling the behavior of perfect logicians and formalize a solution of\nthe puzzle.",
        "licence": "BSD",
        "theories": [
            "Blue_Eyes"
        ]
    },
    {
        "session": "Category2",
        "title": "Category Theory",
        "authors": [
            "Alexander Katovsky"
        ],
        "date": "2010-06-20",
        "topics": [
            "Mathematics/Category theory"
        ],
        "abstract": "This article presents a development of Category Theory in Isabelle/HOL. A Category is defined using records and locales. Functors and Natural Transformations are also defined. The main result that has been formalized is that the Yoneda functor is a full and faithful embedding. We also formalize the completeness of many sorted monadic equational logic. Extensive use is made of the HOLZF theory in both cases. For an informal description see <a href=\"http://www.srcf.ucam.org/~apk32/Isabelle/Category/Cat.pdf\">here [pdf]</a>.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-03-02"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-24"
            },
            {
                "2011-1": "2011-10-11"
            },
            {
                "2011": "2011-02-11"
            },
            {
                "2009-2": "2010-06-30"
            },
            {
                "2009-1": "2010-06-21"
            }
        ],
        "theories": [
            "Category",
            "Universe",
            "MonadicEquationalTheory",
            "Functors",
            "NatTrans",
            "SetCat",
            "Yoneda"
        ]
    },
    {
        "session": "Octonions",
        "title": "Octonions",
        "authors": [
            "Angeliki Koutsoukou-Argyraki"
        ],
        "topics": [
            "Mathematics/Algebra",
            "Mathematics/Geometry"
        ],
        "date": "2018-09-14",
        "abstract": "\nWe develop the basic theory of Octonions, including various identities\nand properties of the octonions and of the octonionic product, a\ndescription of 7D isometries and representations of orthogonal\ntransformations. To this end we first develop the theory of the vector\ncross product in 7 dimensions. The development of the theory of\nOctonions is inspired by that of the theory of Quaternions by Lawrence\nPaulson. However, we do not work within the type class real_algebra_1\nbecause the octonionic product is not associative.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-09-16"
            }
        ],
        "theories": [
            "Cross_Product_7",
            "Octonions"
        ]
    },
    {
        "session": "OpSets",
        "title": "OpSets: Sequential Specifications for Replicated Datatypes",
        "authors": [
            "Martin Kleppmann",
            "Victor B. F. Gomes",
            "Dominic P. Mulligan",
            "Alastair R. Beresford"
        ],
        "topics": [
            "Computer science/Algorithms/Distributed",
            "Computer science/Data structures"
        ],
        "date": "2018-05-10",
        "abstract": "\nWe introduce OpSets, an executable framework for specifying and\nreasoning about the semantics of replicated datatypes that provide\neventual consistency in a distributed system, and for mechanically\nverifying algorithms that implement these datatypes. Our approach is\nsimple but expressive, allowing us to succinctly specify a variety of\nabstract datatypes, including maps, sets, lists, text, graphs, trees,\nand registers. Our datatypes are also composable, enabling the\nconstruction of complex data structures. To demonstrate the utility of\nOpSets for analysing replication algorithms, we highlight an important\ncorrectness property for collaborative text editing that has\ntraditionally been overlooked; algorithms that do not satisfy this\nproperty can exhibit awkward interleaving of text. We use OpSets to\nspecify this correctness property and prove that although one existing\nreplication algorithm satisfies this property, several other published\nalgorithms do not.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2018-05-25"
            }
        ],
        "theories": [
            "OpSet",
            "Insert_Spec",
            "List_Spec",
            "Interleaving",
            "RGA"
        ]
    },
    {
        "session": "Lambda_Free_EPO",
        "title": "Formalization of the Embedding Path Order for Lambda-Free Higher-Order Terms",
        "authors": [
            "Alexander Bentkamp"
        ],
        "topics": [
            "Logic/Rewriting"
        ],
        "date": "2018-10-19",
        "abstract": "\nThis Isabelle/HOL formalization defines the Embedding Path Order (EPO)\nfor higher-order terms without lambda-abstraction and proves many\nuseful properties about it. In contrast to the lambda-free recursive\npath orders, it does not fully coincide with RPO on first-order terms,\nbut it is compatible with arbitrary higher-order contexts.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-10-21"
            }
        ],
        "dependencies": [
            "Lambda_Free_RPOs"
        ],
        "theories": [
            "Embeddings",
            "Chop",
            "Lambda_Free_EPO"
        ]
    },
    {
        "session": "IMAP-CRDT",
        "title": "The IMAP CmRDT",
        "authors": [
            "Tim Jungnickel",
            "Lennart Oldenburg",
            "Matthias Loibl"
        ],
        "topics": [
            "Computer science/Algorithms/Distributed",
            "Computer science/Data structures"
        ],
        "date": "2017-11-09",
        "abstract": "\nWe provide our Isabelle/HOL formalization of a Conflict-free\nReplicated Datatype for Internet Message Access Protocol commands.\nWe show that Strong Eventual Consistency (SEC) is guaranteed\nby proving the commutativity of concurrent operations. We base our\nformalization on the recently proposed \"framework for\nestablishing Strong Eventual Consistency for Conflict-free Replicated\nDatatypes\" (AFP.CRDT) from Gomes et al. Hence, we provide an\nadditional example of how the recently proposed framework can be used\nto design and prove CRDTs.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2020-01-14"
            },
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-11-10"
            }
        ],
        "dependencies": [
            "CRDT"
        ],
        "theories": [
            "IMAP-def",
            "IMAP-proof-commute",
            "IMAP-proof-helpers",
            "IMAP-proof-independent",
            "IMAP-proof"
        ]
    },
    {
        "session": "Lehmer",
        "title": "Lehmer's Theorem",
        "authors": [
            "Simon Wimmer",
            "Lars Noschinski"
        ],
        "date": "2013-07-22",
        "topics": [
            "Mathematics/Number theory"
        ],
        "abstract": "In 1927, Lehmer presented criterions for primality, based on the converse of Fermat's litte theorem. This work formalizes the second criterion from Lehmer's paper, a necessary and sufficient condition for primality.\n<p>\nAs a side product we formalize some properties of Euler's phi-function,\nthe notion of the order of an element of a group, and the cyclicity of the multiplicative group of a finite field.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            }
        ],
        "theories": [
            "Lehmer"
        ]
    },
    {
        "session": "C2KA_DistributedSystems",
        "title": "Communicating Concurrent Kleene Algebra for Distributed Systems Specification",
        "authors": [
            "Maxime Buyse",
            "Jason Jaskolka"
        ],
        "topics": [
            "Computer science/Automata and formal languages",
            "Mathematics/Algebra"
        ],
        "date": "2019-08-06",
        "abstract": "\nCommunicating Concurrent Kleene Algebra (C²KA) is a mathematical\nframework for capturing the communicating and concurrent behaviour of\nagents in distributed systems. It extends Hoare et al.'s\nConcurrent Kleene Algebra (CKA) with communication actions through the\nnotions of stimuli and shared environments. C²KA has applications in\nstudying system-level properties of distributed systems such as\nsafety, security, and reliability. In this work, we formalize results\nabout C²KA and its application for distributed systems specification.\nWe first formalize the stimulus structure and behaviour structure\n(CKA). Next, we combine them to formalize C²KA and its properties.\nThen, we formalize notions and properties related to the topology of\ndistributed systems and the potential for communication via stimuli\nand via shared environments of agents, all within the algebraic\nsetting of C²KA.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-08-06"
            }
        ],
        "theories": [
            "Stimuli",
            "CKA",
            "C2KA",
            "Topology_C2KA",
            "Communication_C2KA"
        ]
    },
    {
        "session": "QR_Decomposition",
        "title": "QR Decomposition",
        "authors": [
            "Jose Divasón",
            "Jesús Aransay"
        ],
        "topics": [
            "Computer science/Algorithms/Mathematical",
            "Mathematics/Algebra"
        ],
        "date": "2015-02-12",
        "abstract": "QR decomposition is an algorithm to decompose a real matrix A into the product of two other matrices Q and R, where Q is orthogonal and R is invertible and upper triangular. The algorithm is useful for the least squares problem; i.e., the computation of the best approximation of an unsolvable system of linear equations. As a side-product, the Gram-Schmidt process has also been formalized. A refinement using immutable arrays is presented as well. The development relies, among others, on the AFP entry \"Implementing field extensions of the form Q[sqrt(b)]\" by René Thiemann, which allows execution of the algorithm using symbolic computations. Verified code can be generated and executed using floats as well.",
        "extra": {
            "Change history": "[2015-06-18] The second part of the Fundamental Theorem of Linear Algebra has been generalized to more general inner product spaces."
        },
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2015-02-13"
            }
        ],
        "dependencies": [
            "Gauss_Jordan",
            "Rank_Nullity_Theorem",
            "Real_Impl",
            "Sqrt_Babylonian"
        ],
        "theories": [
            "Miscellaneous_QR",
            "Projections",
            "Gram_Schmidt",
            "QR_Decomposition",
            "Least_Squares_Approximation",
            "Examples_QR_Abstract_Float",
            "Examples_QR_Abstract_Symbolic",
            "IArray_Addenda_QR",
            "Matrix_To_IArray_QR",
            "Gram_Schmidt_IArrays",
            "QR_Decomposition_IArrays",
            "Examples_QR_IArrays_Float",
            "Examples_QR_IArrays_Symbolic",
            "Generalizations2",
            "QR_Efficient"
        ]
    },
    {
        "session": "Differential_Game_Logic",
        "title": "Differential Game Logic",
        "authors": [
            "André Platzer"
        ],
        "topics": [
            "Computer science/Programming languages/Logics"
        ],
        "date": "2019-06-03",
        "abstract": "\nThis formalization provides differential game logic (dGL), a logic for\nproving properties of hybrid game. In addition to the syntax and\nsemantics, it formalizes a uniform substitution calculus for dGL.\nChurch's uniform substitutions substitute a term or formula for a\nfunction or predicate symbol everywhere. The uniform substitutions for\ndGL also substitute hybrid games for a game symbol everywhere. We\nprove soundness of one-pass uniform substitutions and the axioms of\ndifferential game logic with respect to their denotational semantics.\nOne-pass uniform substitutions are faster by postponing\nsoundness-critical admissibility checks with a linear pass homomorphic\napplication and regain soundness by a variable condition at the\nreplacements.  The formalization is based on prior non-mechanized\nsoundness proofs for dGL.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-24"
            }
        ],
        "theories": [
            "Lib",
            "Identifiers",
            "Syntax",
            "Denotational_Semantics",
            "Static_Semantics",
            "Coincidence",
            "USubst",
            "Ids",
            "Axioms",
            "Differential_Game_Logic"
        ]
    },
    {
        "session": "PAC_Checker",
        "title": "Practical Algebraic Calculus Checker",
        "authors": [
            "Mathias Fleury",
            "Daniela Kaufmann"
        ],
        "topics": [
            "Computer science/Algorithms"
        ],
        "date": "2020-08-31",
        "abstract": "\nGenerating and checking proof certificates is important to increase\nthe trust in automated reasoning tools. In recent years formal\nverification using computer algebra became more important and is\nheavily used in automated circuit verification.  An existing proof\nformat which covers algebraic reasoning and allows efficient proof\nchecking is the practical algebraic calculus (PAC). In this\ndevelopment, we present the verified checker Pastèque that is obtained\nby synthesis via the Refinement Framework.  This is the formalization\ngoing with our FMCAD'20 tool presentation.",
        "licence": "BSD",
        "dependencies": [
            "Sepref_IICF",
            "Polynomials",
            "Nested_Multisets_Ordinals"
        ],
        "theories": [
            "PAC_More_Poly",
            "Finite_Map_Multiset",
            "WB_Sort",
            "More_Loops",
            "PAC_Specification",
            "PAC_Map_Rel",
            "PAC_Checker_Specification",
            "PAC_Polynomials",
            "PAC_Polynomials_Term",
            "PAC_Polynomials_Operations",
            "PAC_Misc",
            "PAC_Checker",
            "PAC_Checker_Relation",
            "PAC_Assoc_Map_Rel",
            "PAC_Checker_Init",
            "PAC_Version",
            "PAC_Checker_Synthesis",
            "PAC_Checker_MLton"
        ]
    },
    {
        "session": "Mereology",
        "title": "Mereology",
        "authors": [
            "Ben Blumson"
        ],
        "topics": [
            "Logic/Philosophical aspects"
        ],
        "date": "2021-03-01",
        "abstract": "\nWe use Isabelle/HOL to verify elementary theorems and alternative\naxiomatizations of classical extensional mereology.",
        "licence": "BSD",
        "theories": [
            "PM",
            "M",
            "MM",
            "EM",
            "CM",
            "CEM",
            "GM",
            "GMM",
            "GEM"
        ]
    },
    {
        "session": "PseudoHoops",
        "title": "Pseudo Hoops",
        "authors": [
            "George Georgescu",
            "Laurentiu Leustean",
            "Viorel Preoteasa"
        ],
        "topics": [
            "Mathematics/Algebra"
        ],
        "date": "2011-09-22",
        "abstract": "Pseudo-hoops are algebraic structures introduced by B. Bosbach under the name of complementary semigroups. In this formalization we prove some properties of pseudo-hoops and we define the basic concepts of filter and normal filter. The lattice of normal filters is isomorphic with the lattice of congruences of a pseudo-hoop. We also study some important classes of pseudo-hoops. Bounded Wajsberg pseudo-hoops are equivalent to pseudo-Wajsberg algebras and bounded basic pseudo-hoops are equivalent to pseudo-BL algebras. Some examples of pseudo-hoops are given in the last section of the formalization.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-24"
            },
            {
                "2011-1": "2011-10-11"
            },
            {
                "2011": "2011-09-27"
            }
        ],
        "dependencies": [
            "LatticeProperties"
        ],
        "theories": [
            "Operations",
            "LeftComplementedMonoid",
            "RightComplementedMonoid",
            "PseudoHoops",
            "PseudoHoopFilters",
            "PseudoWaisbergAlgebra",
            "SpecialPseudoHoops",
            "Examples"
        ]
    },
    {
        "session": "Trie",
        "title": "Trie",
        "authors": [
            "Andreas Lochbihler",
            "Tobias Nipkow"
        ],
        "date": "2015-03-30",
        "topics": [
            "Computer science/Data structures"
        ],
        "abstract": "\nThis article formalizes the ``trie'' data structure invented by\nFredkin [CACM 1960]. It also provides a specialization where the entries\nin the trie are lists.",
        "extra": {
            "Origin": "This article was extracted from existing articles by the authors."
        },
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2015-03-30"
            }
        ],
        "theories": [
            "Trie",
            "Tries"
        ]
    },
    {
        "session": "Probabilistic_While",
        "title": "Probabilistic while loop",
        "authors": [
            "Andreas Lochbihler"
        ],
        "topics": [
            "Computer science/Functional programming",
            "Mathematics/Probability theory",
            "Computer science/Algorithms"
        ],
        "date": "2017-05-05",
        "abstract": "\nThis AFP entry defines a probabilistic while operator based on\nsub-probability mass functions and formalises zero-one laws and variant\nrules for probabilistic loop termination. As applications, we\nimplement probabilistic algorithms for the Bernoulli, geometric and\narbitrary uniform distributions that only use fair coin flips, and\nprove them correct and terminating with probability 1.",
        "extra": {
            "Change history": "[2018-02-02]\nAdded a proof that probabilistic conditioning can be implemented by repeated sampling.\n(revision 305867c4e911)<br>"
        },
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2017-05-11"
            }
        ],
        "dependencies": [
            "MFMC_Countable",
            "Pre_BZ"
        ],
        "theories": [
            "While_SPMF",
            "Bernoulli",
            "Geometric",
            "Fast_Dice_Roll",
            "Resampling"
        ]
    },
    {
        "session": "Core_DOM",
        "title": "A Formal Model of the Document Object Model",
        "authors": [
            "Achim D. Brucker",
            "Michael Herzberg"
        ],
        "topics": [
            "Computer science/Data structures"
        ],
        "date": "2018-12-26",
        "abstract": "\nIn this AFP entry, we formalize the core of the Document Object Model\n(DOM).  At its core, the DOM defines a tree-like data structure for\nrepresenting documents in general and HTML documents in particular. It\nis the heart of any modern web browser.  Formalizing the key concepts\nof the DOM is a prerequisite for the formal reasoning over client-side\nJavaScript programs and for the analysis of security concepts in\nmodern web browsers.  We present a formalization of the core DOM, with\nfocus on the node-tree and the operations defined on node-trees, in\nIsabelle/HOL. We use the formalization to verify the functional\ncorrectness of the most important functions defined in the DOM\nstandard. Moreover, our formalization is 1) extensible, i.e., can be\nextended without the need of re-proving already proven properties and\n2) executable, i.e., we can generate executable code from our\nspecification.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2019-01-07"
            }
        ],
        "theories": [
            "Hiding_Type_Variables",
            "Ref",
            "Core_DOM_Basic_Datatypes",
            "BaseClass",
            "Heap_Error_Monad",
            "BaseMonad",
            "ObjectPointer",
            "ObjectClass",
            "ObjectMonad",
            "NodePointer",
            "NodeClass",
            "NodeMonad",
            "ElementPointer",
            "CharacterDataPointer",
            "DocumentPointer",
            "ShadowRootPointer",
            "ElementClass",
            "ElementMonad",
            "CharacterDataClass",
            "CharacterDataMonad",
            "DocumentClass",
            "DocumentMonad",
            "Core_DOM_Functions",
            "Core_DOM_Heap_WF",
            "Core_DOM",
            "Testing_Utils",
            "Core_DOM_BaseTest",
            "Document_adoptNode",
            "Document_getElementById",
            "Node_insertBefore",
            "Node_removeChild",
            "Core_DOM_Tests"
        ]
    },
    {
        "session": "Euler_Partition",
        "title": "Euler's Partition Theorem",
        "authors": [
            "Lukas Bulwahn"
        ],
        "date": "2015-11-19",
        "topics": [
            "Mathematics/Combinatorics"
        ],
        "abstract": "\nEuler's Partition Theorem states that the number of partitions with only\ndistinct parts is equal to the number of partitions with only odd parts.\nThe combinatorial proof follows John Harrison's HOL Light formalization.\nThis theorem is the 45th theorem of the Top 100 Theorems list.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-11-20"
            }
        ],
        "dependencies": [
            "Card_Number_Partitions"
        ],
        "theories": [
            "Euler_Partition"
        ]
    },
    {
        "session": "Matrix",
        "title": "Executable Matrix Operations on Matrices of Arbitrary Dimensions",
        "topics": [
            "Computer science/Data structures"
        ],
        "date": "2010-06-17",
        "authors": [
            "Christian Sternagel",
            "René Thiemann"
        ],
        "license": "LGPL",
        "abstract": "\nWe provide the operations of matrix addition, multiplication,\ntransposition, and matrix comparisons as executable functions over\nordered semirings. Moreover, it is proven that strongly normalizing\n(monotone) orders can be lifted to strongly normalizing (monotone) orders\nover matrices. We further show that the standard semirings over the\nnaturals, integers, and rationals, as well as the arctic semirings\nsatisfy the axioms that are required by our matrix theory. Our\nformalization is part of the <a\nhref=\"http://cl-informatik.uibk.ac.at/software/ceta\">CeTA</a> system\nwhich contains several termination techniques. The provided theories have\nbeen essential to formalize matrix-interpretations and arctic\ninterpretations.",
        "extra": {
            "Change history": "[2010-09-17] Moved theory on arbitrary (ordered) semirings to Abstract Rewriting."
        },
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-24"
            },
            {
                "2011-1": "2011-10-11"
            },
            {
                "2011": "2011-02-11"
            },
            {
                "2009-2": "2010-07-01"
            },
            {
                "2009-1": "2010-06-17"
            }
        ],
        "dependencies": [
            "Abstract-Rewriting"
        ],
        "theories": [
            "Utility",
            "Ordered_Semiring",
            "Matrix_Legacy"
        ]
    },
    {
        "session": "Ordered_Resolution_Prover",
        "title": "Formalization of Bachmair and Ganzinger's Ordered Resolution Prover",
        "authors": [
            "Anders Schlichtkrull",
            "Jasmin Christian Blanchette",
            "Dmitriy Traytel",
            "Uwe Waldmann"
        ],
        "topics": [
            "Logic/General logic/Mechanization of proofs"
        ],
        "date": "2018-01-18",
        "abstract": "\nThis Isabelle/HOL formalization covers Sections 2 to 4 of Bachmair and\nGanzinger's \"Resolution Theorem Proving\" chapter in the\n<em>Handbook of Automated Reasoning</em>. This includes\nsoundness and completeness of unordered and ordered variants of ground\nresolution with and without literal selection, the standard redundancy\ncriterion, a general framework for refutational theorem proving, and\nsoundness and completeness of an abstract first-order prover.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2018-01-22"
            }
        ],
        "dependencies": [
            "Coinductive",
            "Nested_Multisets_Ordinals"
        ],
        "theories": [
            "Map2",
            "Lazy_List_Liminf",
            "Lazy_List_Chain",
            "Clausal_Logic",
            "Herbrand_Interpretation",
            "Abstract_Substitution",
            "Inference_System",
            "Ground_Resolution_Model",
            "Unordered_Ground_Resolution",
            "Ordered_Ground_Resolution",
            "Proving_Process",
            "Standard_Redundancy",
            "FO_Ordered_Resolution",
            "FO_Ordered_Resolution_Prover"
        ]
    },
    {
        "session": "Hermite",
        "title": "Hermite Normal Form",
        "authors": [
            "Jose Divasón",
            "Jesús Aransay"
        ],
        "topics": [
            "Computer science/Algorithms/Mathematical",
            "Mathematics/Algebra"
        ],
        "date": "2015-07-07",
        "abstract": "Hermite Normal Form is a canonical matrix analogue of Reduced Echelon Form, but involving matrices over more general rings. In this work we formalise an algorithm to compute the Hermite Normal Form of a matrix by means of elementary row operations, taking advantage of the Echelon Form AFP entry. We have proven the correctness of such an algorithm and refined it to immutable arrays. Furthermore, we have also formalised the uniqueness of the Hermite Normal Form of a matrix. Code can be exported and some examples of execution involving integer matrices and polynomial matrices are presented as well.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-07-07"
            }
        ],
        "dependencies": [
            "Echelon_Form"
        ],
        "theories": [
            "Hermite",
            "Hermite_IArrays"
        ]
    },
    {
        "session": "KAD",
        "title": "Kleene Algebras with Domain",
        "authors": [
            "Victor B. F. Gomes",
            "Walter Guttmann",
            "Peter Höfner",
            "Georg Struth",
            "Tjark Weber"
        ],
        "date": "2016-04-12",
        "topics": [
            "Computer science/Programming languages/Logics",
            "Computer science/Automata and formal languages",
            "Mathematics/Algebra"
        ],
        "abstract": "\nKleene algebras with domain are Kleene algebras endowed with an\noperation that maps each element of the algebra to its domain of\ndefinition (or its complement) in abstract fashion. They form a simple\nalgebraic basis for Hoare logics, dynamic logics or predicate\ntransformer semantics. We formalise a modular hierarchy of algebras\nwith domain and antidomain (domain complement) operations in\nIsabelle/HOL that ranges from domain and antidomain semigroups to\nmodal Kleene algebras and divergence Kleene algebras. We link these\nalgebras with models of binary relations and program traces. We\ninclude some examples from modal logics, termination and program\nanalysis.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-04-12"
            }
        ],
        "dependencies": [
            "Kleene_Algebra"
        ],
        "theories": [
            "Domain_Semiring",
            "Antidomain_Semiring",
            "Range_Semiring",
            "Modal_Kleene_Algebra",
            "Modal_Kleene_Algebra_Models",
            "Modal_Kleene_Algebra_Applications"
        ]
    },
    {
        "session": "MuchAdoAboutTwo",
        "title": "Much Ado About Two",
        "authors": [
            "Sascha Böhme"
        ],
        "date": "2007-11-06",
        "topics": [
            "Computer science/Algorithms"
        ],
        "abstract": "This article is an Isabelle formalisation of a paper with the same title. In a similar way as Knuth's 0-1-principle for sorting algorithms, that paper develops a 0-1-2-principle for parallel prefix computations.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-03-02"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-24"
            },
            {
                "2011-1": "2011-10-11"
            },
            {
                "2011": "2011-02-11"
            },
            {
                "2009-2": "2010-07-01"
            },
            {
                "2009-1": "2009-12-12"
            },
            {
                "2009": "2009-04-29"
            },
            {
                "2008": "2008-06-10"
            },
            {
                "2007": "2007-11-27"
            }
        ],
        "theories": [
            "MuchAdoAboutTwo"
        ]
    },
    {
        "session": "Goedel_Incompleteness",
        "title": "An Abstract Formalization of G&ouml;del's Incompleteness Theorems",
        "authors": [
            "Andrei Popescu",
            "Dmitriy Traytel"
        ],
        "topics": [
            "Logic/Proof theory"
        ],
        "date": "2020-09-16",
        "abstract": "\nWe present an abstract formalization of G&ouml;del's\nincompleteness theorems. We analyze sufficient conditions for the\ntheorems' applicability to a partially specified logic. Our\nabstract perspective enables a comparison between alternative\napproaches from the literature. These include Rosser's variation\nof the first theorem, Jeroslow's variation of the second theorem,\nand the Swierczkowski&ndash;Paulson semantics-based approach. This\nAFP entry is the main entry point to the results described in our\nCADE-27 paper <a\nhref=\"https://dx.doi.org/10.1007/978-3-030-29436-6_26\">A\nFormally Verified Abstract Account of Gödel's Incompleteness\nTheorems</a>.  As part of our abstract formalization's\nvalidation, we instantiate our locales twice in the separate AFP\nentries <a\nhref=\"https://www.isa-afp.org/entries/Goedel_HFSet_Semantic.html\">Goedel_HFSet_Semantic</a>\nand <a\nhref=\"https://www.isa-afp.org/entries/Goedel_HFSet_Semanticless.html\">Goedel_HFSet_Semanticless</a>.",
        "licence": "BSD",
        "dependencies": [
            "Syntax_Independent_Logic"
        ],
        "theories": [
            "Deduction2",
            "Abstract_Encoding",
            "Abstract_Representability",
            "Diagonalization",
            "Derivability_Conditions",
            "Goedel_Formula",
            "Standard_Model_More",
            "Abstract_First_Goedel",
            "Rosser_Formula",
            "Abstract_First_Goedel_Rosser",
            "Abstract_Second_Goedel",
            "Abstract_Jeroslow_Encoding",
            "Jeroslow_Original",
            "Jeroslow_Simplified",
            "Loeb_Formula",
            "Loeb",
            "Tarski",
            "All_Abstract"
        ]
    },
    {
        "session": "Hermite_Lindemann",
        "title": "The Hermite–Lindemann–Weierstraß Transcendence Theorem",
        "authors": [
            "Manuel Eberl"
        ],
        "topics": [
            "Mathematics/Number theory"
        ],
        "date": "2021-03-03",
        "abstract": "\n<p>This article provides a formalisation of the\nHermite-Lindemann-Weierstraß Theorem (also known as simply\nHermite-Lindemann or Lindemann-Weierstraß). This theorem is one of the\ncrowning achievements of 19th century number theory.</p>\n<p>The theorem states that if $\\alpha_1, \\ldots,\n\\alpha_n\\in\\mathbb{C}$ are algebraic numbers that are linearly\nindependent over $\\mathbb{Z}$, then $e^{\\alpha_1},\\ldots,e^{\\alpha_n}$\nare algebraically independent over $\\mathbb{Q}$.</p>\n<p>Like the <a\nhref=\"https://doi.org/10.1007/978-3-319-66107-0_5\">previous\nformalisation in Coq by Bernard</a>, I proceeded by formalising\n<a\nhref=\"https://doi.org/10.1017/CBO9780511565977\">Baker's\nversion of the theorem and proof</a> and then deriving the\noriginal one from that. Baker's version states that for any\nalgebraic numbers $\\beta_1, \\ldots, \\beta_n\\in\\mathbb{C}$ and distinct\nalgebraic numbers $\\alpha_i, \\ldots, \\alpha_n\\in\\mathbb{C}$, we have\n$\\beta_1 e^{\\alpha_1} + \\ldots + \\beta_n e^{\\alpha_n} = 0$ if and only\nif all the $\\beta_i$ are zero.</p> <p>This has a number of\ndirect corollaries, e.g.:</p> <ul> <li>$e$ and $\\pi$\nare transcendental</li> <li>$e^z$, $\\sin z$, $\\tan z$,\netc. are transcendental for algebraic\n$z\\in\\mathbb{C}\\setminus\\{0\\}$</li> <li>$\\ln z$ is\ntranscendental for algebraic $z\\in\\mathbb{C}\\setminus\\{0,\n1\\}$</li> </ul>",
        "licence": "BSD",
        "dependencies": [
            "Pi_Transcendental",
            "Algebraic_Numbers",
            "Power_Sum_Polynomials"
        ],
        "theories": [
            "Algebraic_Integer_Divisibility",
            "More_Polynomial_HLW",
            "Min_Int_Poly",
            "Complex_Lexorder",
            "More_Multivariate_Polynomial_HLW",
            "More_Algebraic_Numbers_HLW",
            "Misc_HLW",
            "Hermite_Lindemann"
        ]
    },
    {
        "session": "GPU_Kernel_PL",
        "title": "Syntax and semantics of a GPU kernel programming language",
        "authors": [
            "John Wickerson"
        ],
        "date": "2014-04-03",
        "topics": [
            "Computer science/Programming languages/Language definitions"
        ],
        "abstract": "\nThis document accompanies the article \"The Design and\nImplementation of a Verification Technique for GPU Kernels\"\nby Adam Betts, Nathan Chong, Alastair F. Donaldson, Jeroen\nKetema, Shaz Qadeer, Paul Thomson and John Wickerson. It\nformalises all of the definitions provided in Sections 3\nand 4 of the article.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2014-04-06"
            }
        ],
        "theories": [
            "Misc",
            "KPL_syntax",
            "KPL_wellformedness",
            "KPL_state",
            "KPL_execution_thread",
            "KPL_execution_group",
            "KPL_execution_kernel",
            "Kernel_programming_language"
        ]
    },
    {
        "session": "Fourier",
        "title": "Fourier Series",
        "authors": [
            "Lawrence C. Paulson"
        ],
        "topics": [
            "Mathematics/Analysis"
        ],
        "date": "2019-09-06",
        "abstract": "\nThis development formalises the square integrable functions over the\nreals and the basics of Fourier series. It culminates with a proof\nthat every well-behaved periodic function can be approximated by a\nFourier series. The material is ported from HOL Light:\nhttps://github.com/jrh13/hol-light/blob/master/100/fourier.ml",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-09-11"
            }
        ],
        "dependencies": [
            "Lp"
        ],
        "theories": [
            "Periodic",
            "Lspace",
            "Square_Integrable",
            "Confine",
            "Fourier_Aux2",
            "Fourier"
        ]
    },
    {
        "session": "JinjaDCI",
        "title": "JinjaDCI: a Java semantics with dynamic class initialization",
        "authors": [
            "Susannah Mansky"
        ],
        "topics": [
            "Computer science/Programming languages/Language definitions"
        ],
        "date": "2021-01-11",
        "abstract": "\nWe extend Jinja to include static fields, methods, and instructions,\nand dynamic class initialization, based on the Java SE 8\nspecification. This includes extension of definitions and proofs. This\nwork is partially described in Mansky and Gunter's paper at CPP\n2019 and Mansky's doctoral thesis (UIUC, 2020).",
        "licence": "BSD",
        "dependencies": [
            "List-Index",
            "Jinja"
        ],
        "theories": [
            "Auxiliary",
            "Type",
            "Decl",
            "TypeRel",
            "Value",
            "Objects",
            "Exceptions",
            "Expr",
            "WellType",
            "WellTypeRT",
            "State",
            "SystemClasses",
            "WellForm",
            "WWellForm",
            "BigStep",
            "DefAss",
            "Conform",
            "SmallStep",
            "EConform",
            "Progress",
            "JWellForm",
            "TypeSafe",
            "Equivalence",
            "Annotate",
            "JVMState",
            "JVMInstructions",
            "JVMExceptions",
            "JVMExecInstr",
            "JVMExec",
            "JVMDefensive",
            "SemiType",
            "JVM_SemiType",
            "Effect",
            "EffectMono",
            "BVSpec",
            "TF_JVM",
            "BVExec",
            "LBVJVM",
            "BVConform",
            "ClassAdd",
            "StartProg",
            "BVSpecTypeSafe",
            "BVNoTypeError",
            "J1",
            "J1WellForm",
            "PCompiler",
            "Hidden",
            "Compiler1",
            "Correctness1",
            "Compiler2",
            "Correctness2",
            "Compiler",
            "TypeComp",
            "JinjaDCI"
        ]
    },
    {
        "session": "Automatic_Refinement",
        "title": "Automatic Data Refinement",
        "authors": [
            "Peter Lammich"
        ],
        "topics": [
            "Computer science/Programming languages/Logics"
        ],
        "date": "2013-10-02",
        "abstract": "We present the Autoref tool for Isabelle/HOL, which automatically\nrefines algorithms specified over abstract concepts like maps\nand sets to algorithms over concrete implementations like red-black-trees,\nand produces a refinement theorem. It is based on ideas borrowed from\nrelational parametricity due to Reynolds and Wadler.\nThe tool allows for rapid prototyping of verified, executable algorithms.\nMoreover, it can be configured to fine-tune the result to the user~s needs.\nOur tool is able to automatically instantiate generic algorithms, which\ngreatly simplifies the implementation of executable data structures.\n<p>\nThis AFP-entry provides the basic tool, which is then used by the\nRefinement and Collection Framework to provide automatic data refinement for\nthe nondeterminism monad and various collection datastructures.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            }
        ],
        "theories": [
            "Refine_Util_Bootstrap1",
            "Mpat_Antiquot",
            "Mk_Term_Antiquot",
            "Refine_Util",
            "Attr_Comb",
            "Named_Sorted_Thms",
            "Prio_List",
            "Tagged_Solver",
            "Anti_Unification",
            "Misc",
            "Foldi",
            "Indep_Vars",
            "Select_Solve",
            "Mk_Record_Simp",
            "Refine_Lib",
            "files/Cond_Rewr_Conv.ML",
            "files/Revert_Abbrev.ML",
            "Param_Chapter",
            "Relators",
            "Param_Tool",
            "Param_HOL",
            "Parametricity",
            "Autoref_Phases",
            "Autoref_Data",
            "Autoref_Tagging",
            "Autoref_Id_Ops",
            "Autoref_Fix_Rel",
            "Autoref_Relator_Interface",
            "Autoref_Translate",
            "Autoref_Gen_Algo",
            "Autoref_Chapter",
            "Autoref_Tool",
            "Autoref_Bindings_HOL",
            "Automatic_Refinement"
        ]
    },
    {
        "session": "Stern_Brocot",
        "title": "The Stern-Brocot Tree",
        "authors": [
            "Peter Gammie",
            "Andreas Lochbihler"
        ],
        "date": "2015-12-22",
        "topics": [
            "Mathematics/Number theory"
        ],
        "abstract": "The Stern-Brocot tree contains all rational numbers exactly once and in their lowest terms.  We formalise the Stern-Brocot tree as a coinductive tree using recursive and iterative specifications, which we have proven equivalent, and show that it indeed contains all the numbers as stated.  Following Hinze, we prove that the Stern-Brocot tree can be linearised looplessly into Stern's diatonic sequence (also known as Dijkstra's fusc function) and that it is a permutation of the Bird tree.\n</p><p>\nThe reasoning stays at an abstract level by appealing to the uniqueness of solutions of guarded recursive equations and lifting algebraic laws point-wise to trees and streams using applicative functors.\n</p>",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-12-22"
            }
        ],
        "dependencies": [
            "Applicative_Lifting"
        ],
        "theories": [
            "Cotree",
            "Cotree_Algebra",
            "Stern_Brocot_Tree",
            "Bird_Tree"
        ]
    },
    {
        "session": "Stellar_Quorums",
        "title": "Stellar Quorum Systems",
        "authors": [
            "Giuliano Losa"
        ],
        "topics": [
            "Computer science/Algorithms/Distributed"
        ],
        "date": "2019-08-01",
        "abstract": "\nWe formalize the static properties of personal Byzantine quorum\nsystems (PBQSs) and Stellar quorum systems, as described in the paper\n``Stellar Consensus by Reduction'' (to appear at DISC 2019).",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-08-03"
            }
        ],
        "theories": [
            "Stellar_Quorums"
        ]
    },
    {
        "session": "TortoiseHare",
        "title": "The Tortoise and Hare Algorithm",
        "authors": [
            "Peter Gammie"
        ],
        "date": "2015-11-18",
        "topics": [
            "Computer science/Algorithms"
        ],
        "abstract": "We formalize the Tortoise and Hare cycle-finding algorithm ascribed to Floyd by Knuth, and an improved version due to Brent.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            }
        ],
        "theories": [
            "Basis",
            "TortoiseHare",
            "Brent"
        ]
    },
    {
        "session": "Show",
        "title": "Haskell's Show Class in Isabelle/HOL",
        "authors": [
            "Christian Sternagel",
            "René Thiemann"
        ],
        "date": "2014-07-29",
        "topics": [
            "Computer science/Functional programming"
        ],
        "license": "LGPL",
        "abstract": "\nWe implemented a type class for \"to-string\" functions, similar to\nHaskell's Show class. Moreover, we provide instantiations for Isabelle/HOL's\nstandard types like bool, prod, sum, nats, ints, and rats. It is further\npossible, to automatically derive show functions for arbitrary user defined\ndatatypes similar to Haskell's \"deriving Show\".",
        "extra": {
            "Change history": "[2015-03-11] Adapted development to new-style (BNF-based) datatypes.<br>\n[2015-04-10] Moved development for old-style datatypes into subdirectory\n\"Old_Datatype\".<br>"
        },
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-29"
            },
            {
                "2014": "2014-08-28"
            }
        ],
        "dependencies": [
            "Deriving"
        ],
        "theories": [
            "Show",
            "files/show_generator.ML",
            "Show_Instances",
            "Show_Poly",
            "Show_Real",
            "Show_Complex",
            "Show_Real_Impl"
        ]
    },
    {
        "session": "Knot_Theory",
        "title": "Knot Theory",
        "authors": [
            "T.V.H. Prathamesh"
        ],
        "date": "2016-01-20",
        "topics": [
            "Mathematics/Topology"
        ],
        "abstract": "\nThis work contains a formalization of some topics in knot theory.\nThe concepts that were formalized include definitions of tangles, links,\nframed links and link/tangle equivalence. The formalization is based on a\nformulation of links in terms of tangles. We further construct and prove the\ninvariance of the Bracket polynomial. Bracket polynomial is an invariant of\nframed links closely linked to the Jones polynomial. This is perhaps the first\nattempt to formalize any aspect of knot theory in an interactive proof assistant.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2016-01-20"
            }
        ],
        "dependencies": [
            "Matrix_Tensor"
        ],
        "theories": [
            "Preliminaries",
            "Tangles",
            "Tangle_Algebra",
            "Tangle_Relation",
            "Tangle_Moves",
            "Link_Algebra",
            "Example",
            "Kauffman_Matrix",
            "Computations",
            "Linkrel_Kauffman",
            "Kauffman_Invariance",
            "Knot_Theory"
        ]
    },
    {
        "session": "Budan_Fourier",
        "title": "The Budan-Fourier Theorem and Counting Real Roots with Multiplicity",
        "authors": [
            "Wenda Li"
        ],
        "topics": [
            "Mathematics/Analysis"
        ],
        "date": "2018-09-02",
        "abstract": "\nThis entry is mainly about counting and approximating real roots (of a\npolynomial) with multiplicity. We have first formalised the\nBudan-Fourier theorem: given a polynomial with real coefficients, we\ncan calculate sign variations on Fourier sequences to over-approximate\nthe number of real roots (counting multiplicity) within an interval.\nWhen all roots are known to be real, the over-approximation becomes\ntight: we can utilise this theorem to count real roots exactly. It is\nalso worth noting that Descartes' rule of sign is a direct\nconsequence of the Budan-Fourier theorem, and has been included in\nthis entry. In addition, we have extended previous formalised\nSturm's theorem to count real roots with multiplicity, while the\noriginal Sturm's theorem only counts distinct real roots.\nCompared to the Budan-Fourier theorem, our extended Sturm's\ntheorem always counts roots exactly but may suffer from greater\ncomputational cost.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-09-04"
            }
        ],
        "dependencies": [
            "Sturm_Tarski"
        ],
        "theories": [
            "BF_Misc",
            "Budan_Fourier",
            "Sturm_Multiple_Roots",
            "Descartes_Roots_Test"
        ]
    },
    {
        "session": "Incredible_Proof_Machine",
        "title": "The meta theory of the Incredible Proof Machine",
        "authors": [
            "Joachim Breitner",
            "Denis Lohner"
        ],
        "date": "2016-05-20",
        "topics": [
            "Logic/Proof theory"
        ],
        "abstract": "\nThe <a href=\"http://incredible.pm\">Incredible Proof Machine</a> is an\ninteractive visual theorem prover which represents proofs as port\ngraphs. We model this proof representation in Isabelle, and prove that\nit is just as powerful as natural deduction.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-05-20"
            }
        ],
        "dependencies": [
            "Abstract_Completeness"
        ],
        "theories": [
            "Indexed_FSet",
            "Abstract_Formula",
            "Incredible_Signatures",
            "Incredible_Deduction",
            "Abstract_Rules",
            "Abstract_Rules_To_Incredible",
            "Entailment",
            "Natural_Deduction",
            "Incredible_Correctness",
            "Rose_Tree",
            "Incredible_Trees",
            "Build_Incredible_Tree",
            "Incredible_Completeness",
            "Incredible_Everything",
            "Propositional_Formulas",
            "Incredible_Propositional",
            "Incredible_Propositional_Tasks",
            "Predicate_Formulas",
            "Incredible_Predicate",
            "Incredible_Predicate_Tasks"
        ]
    },
    {
        "session": "CofGroups",
        "title": "An Example of a Cofinitary Group in Isabelle/HOL",
        "authors": [
            "Bart Kastermans"
        ],
        "date": "2009-08-04",
        "topics": [
            "Mathematics/Algebra"
        ],
        "abstract": "We formalize the usual proof that the group generated by the function k -> k + 1 on the integers gives rise to a cofinitary group.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-03-02"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-24"
            },
            {
                "2011-1": "2011-10-11"
            },
            {
                "2011": "2011-02-11"
            },
            {
                "2009-2": "2010-06-30"
            },
            {
                "2009-1": "2009-12-12"
            },
            {
                "2009": "2009-09-05"
            },
            {
                "2009": "2009-08-09"
            }
        ],
        "theories": [
            "CofGroups"
        ]
    },
    {
        "session": "PLM",
        "title": "Representation and Partial Automation of the Principia Logico-Metaphysica in Isabelle/HOL",
        "authors": [
            "Daniel Kirchner"
        ],
        "topics": [
            "Logic/Philosophical aspects"
        ],
        "date": "2017-09-17",
        "abstract": "\n<p> We present an embedding of the second-order fragment of the\nTheory of Abstract Objects as described in Edward Zalta's\nupcoming work <a\nhref=\"https://mally.stanford.edu/principia.pdf\">Principia\nLogico-Metaphysica (PLM)</a> in the automated reasoning\nframework Isabelle/HOL. The Theory of Abstract Objects is a\nmetaphysical theory that reifies property patterns, as they for\nexample occur in the abstract reasoning of mathematics, as\n<b>abstract objects</b> and provides an axiomatic\nframework that allows to reason about these objects. It thereby serves\nas a fundamental metaphysical theory that can be used to axiomatize\nand describe a wide range of philosophical objects, such as Platonic\nforms or Leibniz' concepts, and has the ambition to function as a\nfoundational theory of mathematics. The target theory of our embedding\nas described in chapters 7-9 of PLM employs a modal relational type\ntheory as logical foundation for which a representation in functional\ntype theory is <a\nhref=\"https://mally.stanford.edu/Papers/rtt.pdf\">known to\nbe challenging</a>. </p> <p> Nevertheless we arrive\nat a functioning representation of the theory in the functional logic\nof Isabelle/HOL based on a semantical representation of an Aczel-model\nof the theory. Based on this representation we construct an\nimplementation of the deductive system of PLM which allows to\nautomatically and interactively find and verify theorems of PLM.\n</p> <p> Our work thereby supports the concept of shallow\nsemantical embeddings of logical systems in HOL as a universal tool\nfor logical reasoning <a\nhref=\"http://www.mi.fu-berlin.de/inf/groups/ag-ki/publications/Universal-Reasoning/1703_09620_pd.pdf\">as\npromoted by Christoph Benzm&uuml;ller</a>. </p>\n<p> The most notable result of the presented work is the\ndiscovery of a previously unknown paradox in the formulation of the\nTheory of Abstract Objects. The embedding of the theory in\nIsabelle/HOL played a vital part in this discovery. Furthermore it was\npossible to immediately offer several options to modify the theory to\nguarantee its consistency. Thereby our work could provide a\nsignificant contribution to the development of a proper grounding for\nobject theory. </p>",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2017-09-19"
            }
        ],
        "theories": [
            "TAO_1_Embedding",
            "TAO_2_Semantics",
            "TAO_3_Quantifiable",
            "TAO_4_BasicDefinitions",
            "TAO_5_MetaSolver",
            "TAO_6_Identifiable",
            "TAO_7_Axioms",
            "TAO_8_Definitions",
            "TAO_9_PLM",
            "TAO_10_PossibleWorlds",
            "TAO_98_ArtificialTheorems",
            "TAO_99_SanityTests",
            "TAO_99_Paradox",
            "Thesis"
        ]
    },
    {
        "session": "Minsky_Machines",
        "title": "Minsky Machines",
        "authors": [
            "Bertram Felgenhauer"
        ],
        "topics": [
            "Logic/Computability"
        ],
        "date": "2018-08-14",
        "abstract": "\n<p> We formalize undecidablity results for Minsky machines. To\nthis end, we also formalize recursive inseparability.\n</p><p> We start by proving that Minsky machines can\ncompute arbitrary primitive recursive and recursive functions. We then\nshow that there is a deterministic Minsky machine with one argument\nand two final states such that the set of inputs that are accepted in\none state is recursively inseparable from the set of inputs that are\naccepted in the other state. </p><p> As a corollary, the\nset of Minsky configurations that reach the first state but not the\nsecond recursively inseparable from the set of Minsky configurations\nthat reach the second state but not the first. In particular both\nthese sets are undecidable. </p><p> We do\n<em>not</em> prove that recursive functions can simulate\nMinsky machines. </p>",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2018-08-14"
            }
        ],
        "dependencies": [
            "Abstract-Rewriting",
            "Recursion-Theory-I"
        ],
        "theories": [
            "Recursive_Inseparability",
            "Minsky"
        ]
    },
    {
        "session": "Modular_Assembly_Kit_Security",
        "title": "An Isabelle/HOL Formalization of the Modular Assembly Kit for Security Properties",
        "authors": [
            "Oliver Bračevac",
            "Richard Gay",
            "Sylvia Grewe",
            "Heiko Mantel",
            "Henning Sudbrock",
            "Markus Tasch"
        ],
        "topics": [
            "Computer science/Security"
        ],
        "date": "2018-05-07",
        "abstract": "\nThe \"Modular Assembly Kit for Security Properties\" (MAKS) is\na framework for both the definition and verification of possibilistic\ninformation-flow security properties at the specification-level. MAKS\nsupports the uniform representation of a wide range of possibilistic\ninformation-flow properties and provides support for the verification\nof such properties via unwinding results and compositionality results.\nWe provide a formalization of this framework in Isabelle/HOL.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2018-05-09"
            }
        ],
        "theories": [
            "Projection",
            "Prefix",
            "EventSystems",
            "StateEventSystems",
            "Views",
            "FlowPolicies",
            "BasicSecurityPredicates",
            "InformationFlowProperties",
            "BSPTaxonomy",
            "PropertyLibrary",
            "SecureSystems",
            "UnwindingConditions",
            "AuxiliaryLemmas",
            "UnwindingResults",
            "CompositionBase",
            "CompositionSupport",
            "GeneralizedZippingLemma",
            "CompositionalityResults"
        ]
    },
    {
        "session": "JinjaThreads",
        "title": "Jinja with Threads",
        "authors": [
            "Andreas Lochbihler"
        ],
        "date": "2007-12-03",
        "topics": [
            "Computer science/Programming languages/Language definitions"
        ],
        "abstract": "We extend the Jinja source code semantics by Klein and Nipkow with Java-style arrays and threads. Concurrency is captured in a generic framework semantics for adding concurrency through interleaving to a sequential semantics, which features dynamic thread creation, inter-thread communication via shared memory, lock synchronisation and joins. Also, threads can suspend themselves and be notified by others. We instantiate the framework with the adapted versions of both Jinja source and byte code and show type safety for the multithreaded case. Equally, the compiler from source to byte code is extended, for which we prove weak bisimilarity between the source code small step semantics and the defensive Jinja virtual machine. On top of this, we formalise the JMM and show the DRF guarantee and consistency. For description of the different parts, see Lochbihler's papers at FOOL 2008, ESOP 2010, ITP 2011, and ESOP 2012.",
        "extra": {
            "Change history": "[2008-04-23]\nadded bytecode formalisation with arrays and threads, added thread joins\n(revision f74a8be156a7)<br>\n[2009-04-27]\nadded verified compiler from source code to bytecode;\nencapsulate native methods in separate semantics\n(revision e4f26541e58a)<br>\n[2009-11-30]\nextended compiler correctness proof to infinite and deadlocking computations\n(revision e50282397435)<br>\n[2010-06-08]\nadded thread interruption;\nnew abstract memory model with sequential consistency as implementation\n(revision 0cb9e8dbd78d)<br>\n[2010-06-28]\nnew thread interruption model\n(revision c0440d0a1177)<br>\n[2010-10-15]\npreliminary version of the Java memory model for source code\n(revision 02fee0ef3ca2)<br>\n[2010-12-16]\nimproved version of the Java memory model, also for bytecode\nexecutable scheduler for source code semantics\n(revision 1f41c1842f5a)<br>\n[2011-02-02]\nsimplified code generator setup\nnew random scheduler\n(revision 3059dafd013f)<br>\n[2011-07-21]\nnew interruption model,\ngeneralized JMM proof of DRF guarantee,\nallow class Object to declare methods and fields,\nsimplified subtyping relation,\ncorrected division and modulo implementation\n(revision 46e4181ed142)<br>\n[2012-02-16]\nadded example programs\n(revision bf0b06c8913d)<br>\n[2012-11-21]\ntype safety proof for the Java memory model,\nallow spurious wake-ups\n(revision 76063d860ae0)<br>\n[2013-05-16]\nsupport for non-deterministic memory allocators\n(revision cc3344a49ced)<br>\n[2017-10-20]\nadd an atomic compare-and-swap operation for volatile fields\n(revision a6189b1d6b30)<br>"
        },
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-17"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-26"
            },
            {
                "2011-1": "2011-10-12"
            },
            {
                "2011-1": "2011-10-11"
            },
            {
                "2011": "2011-02-11"
            },
            {
                "2009-2": "2010-07-02"
            },
            {
                "2009-1": "2009-12-12"
            },
            {
                "2009": "2009-04-30"
            },
            {
                "2009": "2009-04-29"
            },
            {
                "2008": "2008-06-10"
            },
            {
                "2007": "2007-12-03"
            }
        ],
        "dependencies": [
            "Binomial-Heaps",
            "Finger-Trees",
            "Automatic_Refinement",
            "Coinductive",
            "Collections",
            "FinFun",
            "Native_Word",
            "Refine_Monadic",
            "Trie"
        ],
        "theories": [
            "Set_without_equal",
            "Set_Monad",
            "JT_ICF",
            "Auxiliary",
            "Basic_Main",
            "FWState",
            "FWLock",
            "FWLocking",
            "FWThread",
            "FWWait",
            "FWCondAction",
            "FWWellform",
            "FWLockingThread",
            "FWInterrupt",
            "FWSemantics",
            "FWProgressAux",
            "FWDeadlock",
            "FWProgress",
            "FWLifting",
            "LTS",
            "FWLTS",
            "Bisimulation",
            "FWBisimulation",
            "FWBisimDeadlock",
            "FWLiftingSem",
            "FWInitFinLift",
            "FWBisimLift",
            "Semilat",
            "Err",
            "Opt",
            "Product",
            "Listn",
            "Semilattices",
            "Typing_Framework",
            "SemilatAlg",
            "Typing_Framework_err",
            "Kildall",
            "LBVSpec",
            "LBVCorrect",
            "LBVComplete",
            "Abstract_BV",
            "Type",
            "Decl",
            "TypeRel",
            "Value",
            "Exceptions",
            "SystemClasses",
            "Heap",
            "Observable_Events",
            "StartConfig",
            "Conform",
            "ExternalCall",
            "WellForm",
            "ExternalCallWF",
            "ConformThreaded",
            "BinOp",
            "SemiType",
            "Common_Main",
            "State",
            "Expr",
            "JHeap",
            "SmallStep",
            "WWellForm",
            "WellType",
            "DefAss",
            "JWellForm",
            "Threaded",
            "WellTypeRT",
            "Progress",
            "DefAssPreservation",
            "TypeSafe",
            "ProgressThreaded",
            "Deadlocked",
            "Annotate",
            "J_Main",
            "JVMState",
            "JVMInstructions",
            "JVMHeap",
            "JVMExecInstr",
            "JVMExceptions",
            "JVMExec",
            "JVMDefensive",
            "JVMThreaded",
            "JVM_Main",
            "JVM_SemiType",
            "Effect",
            "BVSpec",
            "BVConform",
            "BVSpecTypeSafe",
            "BVNoTypeError",
            "BVProgressThreaded",
            "JVMDeadlocked",
            "EffectMono",
            "TF_JVM",
            "LBVJVM",
            "BVExec",
            "BCVExec",
            "BV_Main",
            "CallExpr",
            "J0",
            "J0Bisim",
            "J1State",
            "J1Heap",
            "J1",
            "J1Deadlock",
            "PCompiler",
            "Compiler2",
            "Exception_Tables",
            "J1WellType",
            "J1WellForm",
            "TypeComp",
            "JVMTau",
            "Execs",
            "J1JVMBisim",
            "J1JVM",
            "JVMJ1",
            "Correctness2",
            "ListIndex",
            "Compiler1",
            "J0J1Bisim",
            "Correctness1Threaded",
            "Correctness1",
            "JJ1WellForm",
            "Compiler",
            "Correctness",
            "Preprocessor",
            "Compiler_Main",
            "MM",
            "SC",
            "SC_Interp",
            "SC_Collections",
            "Orders",
            "JMM_Spec",
            "JMM_DRF",
            "SC_Legal",
            "Non_Speculative",
            "SC_Completion",
            "HB_Completion",
            "JMM_Heap",
            "JMM_Framework",
            "JMM_Typesafe",
            "JMM_Common",
            "JMM_J",
            "DRF_J",
            "JMM_JVM",
            "DRF_JVM",
            "JMM_Type",
            "JMM_Compiler",
            "JMM_Type2",
            "JMM_Interp",
            "JMM_Typesafe2",
            "JMM_J_Typesafe",
            "JMM_JVM_Typesafe",
            "JMM_Compiler_Type2",
            "JMM",
            "MM_Main",
            "State_Refinement",
            "Scheduler",
            "Random_Scheduler",
            "Round_Robin",
            "SC_Schedulers",
            "TypeRelRefine",
            "PCompilerRefine",
            "J_Execute",
            "ExternalCall_Execute",
            "JVMExec_Execute2",
            "JVM_Execute2",
            "Code_Generation",
            "JVMExec_Execute",
            "JVM_Execute",
            "ToString",
            "Java2Jinja",
            "Execute_Main",
            "ApprenticeChallenge",
            "BufferExample",
            "Examples_Main",
            "JinjaThreads"
        ]
    },
    {
        "session": "Nullstellensatz",
        "title": "Hilbert's Nullstellensatz",
        "authors": [
            "Alexander Maletzky"
        ],
        "topics": [
            "Mathematics/Algebra",
            "Mathematics/Geometry"
        ],
        "date": "2019-06-16",
        "abstract": "\nThis entry formalizes Hilbert's Nullstellensatz, an important\ntheorem in algebraic geometry that can be viewed as the generalization\nof the Fundamental Theorem of Algebra to multivariate polynomials: If\na set of (multivariate) polynomials over an algebraically closed field\nhas no common zero, then the ideal it generates is the entire\npolynomial ring. The formalization proves several equivalent versions\nof this celebrated theorem: the weak Nullstellensatz, the strong\nNullstellensatz (connecting algebraic varieties and radical ideals),\nand the field-theoretic Nullstellensatz. The formalization follows\nChapter 4.1. of <a\nhref=\"https://link.springer.com/book/10.1007/978-0-387-35651-8\">Ideals,\nVarieties, and Algorithms</a> by Cox, Little and O'Shea.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-17"
            }
        ],
        "dependencies": [
            "Groebner_Bases"
        ],
        "theories": [
            "Algebraically_Closed_Fields",
            "Lex_Order_PP",
            "Univariate_PM",
            "Nullstellensatz",
            "Nullstellensatz_Field"
        ]
    },
    {
        "session": "BNF_CC",
        "title": "Bounded Natural Functors with Covariance and Contravariance",
        "authors": [
            "Andreas Lochbihler",
            "Joshua Schneider"
        ],
        "topics": [
            "Computer science/Functional programming",
            "Tools"
        ],
        "date": "2018-04-24",
        "abstract": "\nBounded natural functors (BNFs) provide a modular framework for the\nconstruction of (co)datatypes in higher-order logic.  Their functorial\noperations, the mapper and relator, are restricted to a subset of the\nparameters, namely those where recursion can take place.  For certain\napplications, such as free theorems, data refinement, quotients, and\ngeneralised rewriting, it is desirable that these operations do not\nignore the other parameters.  In this article, we formalise the\ngeneralisation BNF<sub>CC</sub> that extends the mapper\nand relator to covariant and contravariant parameters.  We show that\n<ol> <li> BNF<sub>CC</sub>s are closed under\nfunctor composition and least and greatest fixpoints,</li>\n<li> subtypes inherit the BNF<sub>CC</sub> structure\nunder conditions that generalise those for the BNF case,\nand</li> <li> BNF<sub>CC</sub>s preserve\nquotients under mild conditions.</li> </ol> These proofs\nare carried out for abstract BNF<sub>CC</sub>s similar to\nthe AFP entry BNF Operations.  In addition, we apply the\nBNF<sub>CC</sub> theory to several concrete functors.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2018-04-25"
            }
        ],
        "theories": [
            "Preliminaries",
            "Axiomatised_BNF_CC",
            "Composition",
            "Fixpoints",
            "Subtypes",
            "Quotient_Preservation",
            "Operation_Examples",
            "Concrete_Examples",
            "DDS"
        ]
    },
    {
        "session": "Polynomials",
        "title": "Executable Multivariate Polynomials",
        "authors": [
            "Christian Sternagel",
            "René Thiemann",
            "Alexander Maletzky",
            "Fabian Immler",
            "Florian Haftmann",
            "Andreas Lochbihler",
            "Alexander Bentkamp"
        ],
        "date": "2010-08-10",
        "topics": [
            "Mathematics/Analysis",
            "Mathematics/Algebra",
            "Computer science/Algorithms/Mathematical"
        ],
        "license": "LGPL",
        "abstract": "\nWe define multivariate polynomials over arbitrary (ordered) semirings in\ncombination with (executable) operations like addition, multiplication,\nand substitution. We also define (weak) monotonicity of polynomials and\ncomparison of polynomials where we provide standard estimations like\nabsolute positiveness or the more recent approach of Neurauter, Zankl,\nand Middeldorp. Moreover, it is proven that strongly normalizing\n(monotone) orders can be lifted to strongly normalizing (monotone) orders\nover polynomials. Our formalization was performed as part of the <a\nhref=\"http://cl-informatik.uibk.ac.at/software/ceta\">IsaFoR/CeTA-system</a>\nwhich contains several termination techniques. The provided theories have\nbeen essential to  formalize polynomial interpretations.\n<p>\nThis formalization also contains an abstract representation as coefficient functions with finite\nsupport and a type of power-products. If this type is ordered by a linear (term) ordering, various\nadditional notions, such as leading power-product, leading coefficient etc., are introduced as\nwell. Furthermore, a lot of generic properties of, and functions on, multivariate polynomials are\nformalized, including the substitution and evaluation homomorphisms, embeddings of polynomial rings\ninto larger rings (i.e. with one additional indeterminate), homogenization and dehomogenization of\npolynomials, and the canonical isomorphism between R[X,Y] and R[X][Y].",
        "extra": {
            "Change history": "[2010-09-17] Moved theories on arbitrary (ordered) semirings to Abstract Rewriting.<br>\n[2016-10-28] Added abstract representation of polynomials and authors Maletzky/Immler.<br>\n[2018-01-23] Added authors Haftmann, Lochbihler after incorporating\ntheir formalization of multivariate polynomials based on Polynomial mappings.\nMoved material from Bentkamp's entry \"Deep Learning\".<br>\n[2019-04-18] Added material about polynomials whose power-products are represented themselves\nby polynomial mappings."
        },
        "olderReleases": [
            {
                "2019": "2020-01-14"
            },
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-24"
            },
            {
                "2011-1": "2011-10-11"
            },
            {
                "2011": "2011-02-11"
            },
            {
                "2009-2": "2010-08-11"
            }
        ],
        "dependencies": [
            "Abstract-Rewriting",
            "Matrix",
            "Show",
            "Well_Quasi_Orders"
        ],
        "theories": [
            "Utils",
            "MPoly_Type",
            "More_MPoly_Type",
            "Power_Products",
            "More_Modules",
            "MPoly_Type_Class",
            "MPoly_Type_Class_Ordered",
            "Poly_Mapping_Finite_Map",
            "MPoly_Type_Class_FMap",
            "PP_Type",
            "OAlist",
            "OAlist_Poly_Mapping",
            "Term_Order",
            "MPoly_Type_Class_OAlist",
            "Quasi_PM_Power_Products",
            "MPoly_PM",
            "MPoly_Type_Univariate",
            "Polynomials",
            "Show_Polynomials",
            "NZM"
        ]
    },
    {
        "session": "FOL_Harrison",
        "title": "First-Order Logic According to Harrison",
        "authors": [
            "Alexander Birch Jensen",
            "Anders Schlichtkrull",
            "Jørgen Villadsen"
        ],
        "topics": [
            "Logic/General logic/Mechanization of proofs"
        ],
        "date": "2017-01-01",
        "abstract": "\n<p>We present a certified declarative first-order prover with equality\nbased on John Harrison's Handbook of Practical Logic and\nAutomated Reasoning, Cambridge University Press, 2009. ML code\nreflection is used such that the entire prover can be executed within\nIsabelle as a very simple interactive proof assistant. As examples we\nconsider Pelletier's problems 1-46.</p>\n<p>Reference: Programming and Verifying a Declarative First-Order\nProver in Isabelle/HOL. Alexander Birch Jensen, John Bruntse Larsen,\nAnders Schlichtkrull & Jørgen Villadsen. AI Communications 31:281-299\n2018. <a href=\"https://content.iospress.com/articles/ai-communications/aic764\">\nhttps://content.iospress.com/articles/ai-communications/aic764</a></p>\n<p>See also: Students' Proof Assistant (SPA).\n<a href=https://github.com/logic-tools/spa>\nhttps://github.com/logic-tools/spa</a></p>",
        "extra": {
            "Change history": "[2018-07-21] Proof of Pelletier's problem 34 (Andrews's Challenge) thanks to Asta Halkjær From."
        },
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2017-01-04"
            }
        ],
        "theories": [
            "FOL_Harrison"
        ]
    },
    {
        "session": "Akra_Bazzi",
        "title": "The Akra-Bazzi theorem and the Master theorem",
        "authors": [
            "Manuel Eberl"
        ],
        "date": "2015-07-14",
        "topics": [
            "Mathematics/Analysis"
        ],
        "abstract": "This article contains a formalisation of the Akra-Bazzi method\nbased on a proof by Leighton. It is a generalisation of the well-known\nMaster Theorem for analysing the complexity of Divide & Conquer algorithms.\nWe also include a generalised version of the Master theorem based on the\nAkra-Bazzi theorem, which is easier to apply than the Akra-Bazzi theorem\nitself.\n<p>\nSome proof methods that facilitate applying the Master theorem are also\nincluded. For a more detailed explanation of the formalisation and the\nproof methods, see the accompanying paper (publication forthcoming).",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-07-24"
            },
            {
                "2015": "2015-07-15"
            }
        ],
        "dependencies": [
            "Landau_Symbols"
        ],
        "theories": [
            "Akra_Bazzi_Library",
            "Akra_Bazzi_Asymptotics",
            "Akra_Bazzi_Real",
            "Akra_Bazzi",
            "Master_Theorem",
            "Eval_Numeral",
            "Akra_Bazzi_Method",
            "files/akra_bazzi.ML",
            "Akra_Bazzi_Approximation",
            "Master_Theorem_Examples"
        ]
    },
    {
        "session": "Relational_Minimum_Spanning_Trees",
        "title": "Relational Minimum Spanning Tree Algorithms",
        "authors": [
            "Walter Guttmann",
            "Nicolas Robinson-O'Brien"
        ],
        "topics": [
            "Computer science/Algorithms/Graph"
        ],
        "date": "2020-12-08",
        "abstract": "\nWe verify the correctness of Prim's, Kruskal's and\nBorůvka's minimum spanning tree algorithms based on algebras for\naggregation and minimisation.",
        "licence": "BSD",
        "dependencies": [
            "Aggregation_Algebras",
            "Relational_Disjoint_Set_Forests"
        ],
        "theories": [
            "Kruskal",
            "Prim",
            "Boruvka"
        ]
    },
    {
        "session": "Finite-Map-Extras",
        "title": "Finite Map Extras",
        "authors": [
            "Javier Díaz"
        ],
        "topics": [
            "Computer science/Data structures"
        ],
        "date": "2020-10-12",
        "abstract": "\nThis entry includes useful syntactic sugar, new operators and functions, and\ntheir associated lemmas for finite maps which currently are not\npresent in the standard Finite_Map theory.",
        "licence": "BSD",
        "theories": [
            "Finite_Map_Extras"
        ]
    },
    {
        "session": "Symmetric_Polynomials",
        "title": "Symmetric Polynomials",
        "authors": [
            "Manuel Eberl"
        ],
        "topics": [
            "Mathematics/Algebra"
        ],
        "date": "2018-09-25",
        "abstract": "\n<p>A symmetric polynomial is a polynomial in variables\n<em>X</em><sub>1</sub>,&hellip;,<em>X</em><sub>n</sub>\nthat does not discriminate between its variables, i.&thinsp;e. it\nis invariant under any permutation of them. These polynomials are\nimportant in the study of the relationship between the coefficients of\na univariate polynomial and its roots in its algebraic\nclosure.</p> <p>This article provides a definition of\nsymmetric polynomials and the elementary symmetric polynomials\ne<sub>1</sub>,&hellip;,e<sub>n</sub> and\nproofs of their basic properties, including three notable\nones:</p> <ul> <li> Vieta's formula, which\ngives an explicit expression for the <em>k</em>-th\ncoefficient of a univariate monic polynomial in terms of its roots\n<em>x</em><sub>1</sub>,&hellip;,<em>x</em><sub>n</sub>,\nnamely\n<em>c</em><sub><em>k</em></sub> = (-1)<sup><em>n</em>-<em>k</em></sup>&thinsp;e<sub><em>n</em>-<em>k</em></sub>(<em>x</em><sub>1</sub>,&hellip;,<em>x</em><sub>n</sub>).</li>\n<li>Second, the Fundamental Theorem of Symmetric Polynomials,\nwhich states that any symmetric polynomial is itself a uniquely\ndetermined polynomial combination of the elementary symmetric\npolynomials.</li> <li>Third, as a corollary of the\nprevious two, that given a polynomial over some ring\n<em>R</em>, any symmetric polynomial combination of its\nroots is also in <em>R</em> even when the roots are not.\n</ul> <p> Both the symmetry property itself and the\nwitness for the Fundamental Theorem are executable. </p>",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-09-26"
            }
        ],
        "dependencies": [
            "Polynomials"
        ],
        "theories": [
            "Vieta",
            "Symmetric_Polynomials",
            "Symmetric_Polynomials_Code"
        ]
    },
    {
        "session": "Gaussian_Integers",
        "title": "Gaussian Integers",
        "authors": [
            "Manuel Eberl"
        ],
        "topics": [
            "Mathematics/Number theory"
        ],
        "date": "2020-04-24",
        "abstract": "\n<p>The Gaussian integers are the subring &#8484;[i] of the\ncomplex numbers, i. e. the ring of all complex numbers with integral\nreal and imaginary part. This article provides a definition of this\nring as well as proofs of various basic properties, such as that they\nform a Euclidean ring and a full classification of their primes. An\nexecutable (albeit not very efficient) factorisation algorithm is also\nprovided.</p> <p>Lastly, this Gaussian integer\nformalisation is used in two short applications:</p> <ol>\n<li> The characterisation of all positive integers that can be\nwritten as sums of two squares</li> <li> Euclid's\nformula for primitive Pythagorean triples</li> </ol>\n<p>While elementary proofs for both of these are already\navailable in the AFP, the theory of Gaussian integers provides more\nconcise proofs and a more high-level view.</p>",
        "licence": "BSD",
        "dependencies": [
            "Polynomial_Factorization"
        ],
        "theories": [
            "Gaussian_Integers",
            "Gaussian_Integers_Test",
            "Gaussian_Integers_Sums_Of_Two_Squares",
            "Gaussian_Integers_Pythagorean_Triples",
            "Gaussian_Integers_Everything"
        ]
    },
    {
        "session": "Pop_Refinement",
        "title": "Pop-Refinement",
        "authors": [
            "Alessandro Coglio"
        ],
        "date": "2014-07-03",
        "topics": [
            "Computer science/Programming languages/Misc"
        ],
        "abstract": "Pop-refinement is an approach to stepwise refinement, carried out inside an interactive theorem prover by constructing a monotonically decreasing sequence of predicates over deeply embedded target programs. The sequence starts with a predicate that characterizes the possible implementations, and ends with a predicate that characterizes a unique program in explicit syntactic form. Pop-refinement enables more requirements (e.g. program-level and non-functional) to be captured in the initial specification and preserved through refinement. Security requirements expressed as hyperproperties (i.e. predicates over sets of traces) are always preserved by pop-refinement, unlike the popular notion of refinement as trace set inclusion. Two simple examples in Isabelle/HOL are presented, featuring program-level requirements, non-functional requirements, and hyperproperties.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2014-07-03"
            }
        ],
        "theories": [
            "Definition",
            "First_Example",
            "Second_Example",
            "General_Remarks",
            "Related_Work",
            "Future_Work"
        ]
    },
    {
        "session": "Constructive_Cryptography_CM",
        "title": "Constructive Cryptography in HOL: the Communication Modeling Aspect",
        "authors": [
            "Andreas Lochbihler",
            "S. Reza Sefidgar"
        ],
        "topics": [
            "Computer science/Security/Cryptography",
            "Mathematics/Probability theory"
        ],
        "date": "2021-03-17",
        "abstract": "\nConstructive Cryptography (CC) [<a\nhref=\"https://conference.iiis.tsinghua.edu.cn/ICS2011/content/papers/14.html\">ICS\n2011</a>, <a\nhref=\"https://doi.org/10.1007/978-3-642-27375-9_3\">TOSCA\n2011</a>, <a\nhref=\"https://doi.org/10.1007/978-3-662-53641-4_1\">TCC\n2016</a>] introduces an abstract approach to composable security\nstatements that allows one to focus on a particular aspect of security\nproofs at a time. Instead of proving the properties of concrete\nsystems, CC studies system classes, i.e., the shared behavior of\nsimilar systems, and their transformations.  Modeling of systems\ncommunication plays a crucial role in composability and reusability of\nsecurity statements; yet, this aspect has not been studied in any of\nthe existing CC results. We extend our previous CC formalization\n[<a href=\"https://isa-afp.org/entries/Constructive_Cryptography.html\">Constructive_Cryptography</a>,\n<a href=\"https://doi.org/10.1109/CSF.2019.00018\">CSF\n2019</a>] with a new semantic domain called Fused Resource\nTemplates (FRT) that abstracts over the systems communication patterns\nin CC proofs. This widens the scope of cryptography proof\nformalizations in the CryptHOL library [<a\nhref=\"https://isa-afp.org/entries/CryptHOL.html\">CryptHOL</a>,\n<a\nhref=\"https://doi.org/10.1007/978-3-662-49498-1_20\">ESOP\n2016</a>, <a\nhref=\"https://doi.org/10.1007/s00145-019-09341-z\">J\nCryptol 2020</a>].  This formalization is described in <a\nhref=\"http://www.andreas-lochbihler.de/pub/basin2021.pdf\">Abstract\nModeling of Systems Communication in Constructive Cryptography using\nCryptHOL</a>.",
        "licence": "BSD",
        "dependencies": [
            "Constructive_Cryptography",
            "Game_Based_Crypto",
            "Sigma_Commit_Crypto"
        ],
        "theories": [
            "More_CC",
            "Observe_Failure",
            "Fold_Spmf",
            "Fused_Resource",
            "State_Isomorphism",
            "Construction_Utility",
            "Concrete_Security",
            "Asymptotic_Security",
            "Key",
            "Channel",
            "One_Time_Pad",
            "Diffie_Hellman_CC",
            "DH_OTP"
        ]
    },
    {
        "session": "Pi_Transcendental",
        "title": "The Transcendence of π",
        "authors": [
            "Manuel Eberl"
        ],
        "topics": [
            "Mathematics/Number theory"
        ],
        "date": "2018-09-28",
        "abstract": "\n<p>This entry shows the transcendence of &pi; based on the\nclassic proof using the fundamental theorem of symmetric polynomials\nfirst given by von Lindemann in 1882, but the formalisation mostly\nfollows the version by Niven. The proof reuses much of the machinery\ndeveloped in the AFP entry on the transcendence of\n<em>e</em>.</p>",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-10-02"
            }
        ],
        "dependencies": [
            "E_Transcendental",
            "Symmetric_Polynomials"
        ],
        "theories": [
            "Pi_Transcendental_Polynomial_Library",
            "Pi_Transcendental"
        ]
    },
    {
        "session": "Compiling-Exceptions-Correctly",
        "title": "Compiling Exceptions Correctly",
        "authors": [
            "Tobias Nipkow"
        ],
        "date": "2004-07-09",
        "topics": [
            "Computer science/Programming languages/Compiling"
        ],
        "abstract": "An exception compilation scheme that dynamically creates and removes exception handler entries on the stack. A formalization of an article of the same name by <a href=\"http://www.cs.nott.ac.uk/~gmh/\">Hutton</a> and Wright.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-24"
            },
            {
                "2011-1": "2011-10-11"
            },
            {
                "2011": "2011-02-11"
            },
            {
                "2009-2": "2010-06-30"
            },
            {
                "2009-1": "2009-12-12"
            },
            {
                "2009": "2009-04-29"
            },
            {
                "2008": "2008-06-10"
            },
            {
                "2007": "2007-11-27"
            },
            {
                "2005": "2005-10-14"
            },
            {
                "2004": "2004-07-09"
            }
        ],
        "theories": [
            "Exceptions"
        ]
    },
    {
        "session": "Sturm_Tarski",
        "title": "The Sturm-Tarski Theorem",
        "authors": [
            "Wenda Li"
        ],
        "date": "2014-09-19",
        "topics": [
            "Mathematics/Analysis"
        ],
        "abstract": "We have formalized the Sturm-Tarski theorem (also referred as the Tarski theorem), which generalizes Sturm's theorem. Sturm's theorem is usually used as a way to count distinct real roots, while the Sturm-Tarksi theorem forms the basis for Tarski's classic quantifier elimination for real closed field.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-12-05"
            },
            {
                "2014": "2014-09-20"
            }
        ],
        "theories": [
            "PolyMisc",
            "Sturm_Tarski"
        ]
    },
    {
        "session": "Randomised_BSTs",
        "title": "Randomised Binary Search Trees",
        "authors": [
            "Manuel Eberl"
        ],
        "topics": [
            "Computer science/Data structures"
        ],
        "date": "2018-10-19",
        "abstract": "\n<p>This work is a formalisation of the Randomised Binary Search\nTrees introduced by Martínez and Roura, including definitions and\ncorrectness proofs.</p> <p>Like randomised treaps, they\nare a probabilistic data structure that behaves exactly as if elements\nwere inserted into a non-balancing BST in random order. However,\nunlike treaps, they only use discrete probability distributions, but\ntheir use of randomness is more complicated.</p>",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-10-19"
            }
        ],
        "dependencies": [
            "Random_BSTs",
            "Monad_Normalisation"
        ],
        "theories": [
            "Randomised_BSTs"
        ]
    },
    {
        "session": "Buildings",
        "title": "Chamber Complexes, Coxeter Systems, and Buildings",
        "authors": [
            "Jeremy Sylvestre"
        ],
        "date": "2016-07-01",
        "topics": [
            "Mathematics/Algebra",
            "Mathematics/Geometry"
        ],
        "abstract": "\nWe provide a basic formal framework for the theory of chamber\ncomplexes and Coxeter systems, and for buildings as thick chamber\ncomplexes endowed with a system of apartments. Along the way, we\ndevelop some of the general theory of abstract simplicial complexes\nand of groups (relying on the <i>group_add</i> class for the basics),\nincluding free groups and group presentations, and their universal\nproperties. The main results verified are that the deletion condition\nis both necessary and sufficient for a group with a set of generators\nof order two to be a Coxeter system, and that the apartments in a\n(thick) building are all uniformly Coxeter.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-07-01"
            }
        ],
        "theories": [
            "Prelim",
            "Algebra",
            "Simplicial",
            "Chamber",
            "Coxeter",
            "Building"
        ]
    },
    {
        "session": "DataRefinementIBP",
        "title": "Semantics and Data Refinement of Invariant Based Programs",
        "authors": [
            "Viorel Preoteasa",
            "Ralph-Johan Back"
        ],
        "date": "2010-05-28",
        "topics": [
            "Computer science/Programming languages/Logics"
        ],
        "abstract": "The invariant based programming is a technique of constructing correct programs by first identifying the basic situations (pre- and post-conditions and invariants) that can occur during the execution of the program, and then defining the transitions and proving that they preserve the invariants. Data refinement is a technique of building correct programs working on concrete datatypes as refinements of more abstract programs. In the theories presented here we formalize the predicate transformer semantics for invariant based programs and their data refinement.",
        "extra": {
            "Change history": "[2012-01-05] Moved some general complete lattice properties to the AFP entry Lattice Properties.\nChanged the definition of the data refinement relation to be more general and updated all corresponding theorems.\nAdded new syntax for demonic and angelic update statements."
        },
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-24"
            },
            {
                "2011-1": "2011-10-11"
            },
            {
                "2011": "2011-02-11"
            },
            {
                "2009-2": "2010-06-30"
            },
            {
                "2009-1": "2010-05-28"
            }
        ],
        "dependencies": [
            "LatticeProperties"
        ],
        "theories": [
            "Preliminaries",
            "Statements",
            "Hoare",
            "Diagram",
            "DataRefinement"
        ]
    },
    {
        "session": "Topology",
        "title": "Topology",
        "authors": [
            "Stefan Friedrich"
        ],
        "date": "2004-04-26",
        "topics": [
            "Mathematics/Topology"
        ],
        "abstract": "This entry contains two theories. The first, <tt>Topology</tt>, develops the basic notions of general topology. The second, which can be viewed as a demonstration of the first, is called <tt>LList_Topology</tt>. It develops the topology of lazy lists.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-03-02"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-24"
            },
            {
                "2011-1": "2011-10-11"
            },
            {
                "2011": "2011-02-11"
            },
            {
                "2009-2": "2010-07-01"
            },
            {
                "2009-1": "2009-12-12"
            },
            {
                "2009": "2009-04-29"
            },
            {
                "2008": "2008-06-10"
            },
            {
                "2007": "2007-11-27"
            },
            {
                "2005": "2005-10-14"
            },
            {
                "2004": "2004-05-21"
            },
            {
                "2004": "2004-04-27"
            }
        ],
        "dependencies": [
            "Coinductive",
            "Lazy-Lists-II"
        ],
        "theories": [
            "Topology",
            "LList_Topology"
        ]
    },
    {
        "session": "Tail_Recursive_Functions",
        "title": "A General Method for the Proof of Theorems on Tail-recursive Functions",
        "authors": [
            "Pasquale Noce"
        ],
        "date": "2013-12-01",
        "topics": [
            "Computer science/Functional programming"
        ],
        "abstract": "\n<p>\nTail-recursive function definitions are sometimes more straightforward than\nalternatives, but proving theorems on them may be roundabout because of the\npeculiar form of the resulting recursion induction rules.\n</p><p>\nThis paper describes a proof method that provides a general solution to\nthis problem by means of suitable invariants over inductive sets, and\nillustrates the application of such method by examining two case studies.\n</p>",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-06-13"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-12-02"
            }
        ],
        "theories": [
            "Method",
            "CaseStudy1",
            "CaseStudy2"
        ]
    },
    {
        "session": "Hood_Melville_Queue",
        "title": "Hood-Melville Queue",
        "authors": [
            "Alejandro Gómez-Londoño"
        ],
        "topics": [
            "Computer science/Data structures"
        ],
        "date": "2021-01-18",
        "abstract": "\nThis is a verified implementation of a constant time queue. The\noriginal design is due to <a\nhref=\"https://doi.org/10.1016/0020-0190(81)90030-2\">Hood\nand Melville</a>. This formalization follows the presentation in\n<em>Purely Functional Data Structures</em>by Okasaki.",
        "licence": "BSD",
        "theories": [
            "Hood_Melville_Queue"
        ]
    },
    {
        "session": "FunWithFunctions",
        "title": "Fun With Functions",
        "authors": [
            "Tobias Nipkow"
        ],
        "date": "2008-08-26",
        "topics": [
            "Mathematics/Misc"
        ],
        "abstract": "This is a collection of cute puzzles of the form ``Show that if a function satisfies the following constraints, it must be ...'' Please add further examples to this collection!",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-24"
            },
            {
                "2011-1": "2011-10-11"
            },
            {
                "2011": "2011-02-11"
            },
            {
                "2009-2": "2010-07-01"
            },
            {
                "2009-1": "2009-12-12"
            },
            {
                "2009": "2009-04-29"
            }
        ],
        "theories": [
            "FunWithFunctions"
        ]
    },
    {
        "session": "Interval_Arithmetic_Word32",
        "title": "Interval Arithmetic on 32-bit Words",
        "authors": [
            "Brandon Bohrer"
        ],
        "topics": [
            "Computer science/Data structures"
        ],
        "date": "2019-11-27",
        "abstract": "\nInterval_Arithmetic implements conservative interval arithmetic\ncomputations, then uses this interval arithmetic to implement a simple\nprogramming language where all terms have 32-bit signed word values,\nwith explicit infinities for terms outside the representable bounds.\nOur target use case is interpreters for languages that must have a\nwell-understood low-level behavior.  We include a formalization of\nbounded-length strings which are used for the identifiers of our\nlanguage. Bounded-length identifiers are useful in some applications,\nfor example the <a href=\"https://www.isa-afp.org/entries/Differential_Dynamic_Logic.html\">Differential_Dynamic_Logic</a> article,\nwhere a Euclidean space indexed by identifiers demands that identifiers\nare finitely many.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-11-28"
            }
        ],
        "dependencies": [
            "Word_Lib"
        ],
        "theories": [
            "Interval_Word32",
            "Finite_String",
            "Interpreter"
        ]
    },
    {
        "session": "Perfect-Number-Thm",
        "title": "Perfect Number Theorem",
        "authors": [
            "Mark Ijbema"
        ],
        "date": "2009-11-22",
        "topics": [
            "Mathematics/Number theory"
        ],
        "abstract": "These theories present the mechanised proof of the Perfect Number Theorem.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-24"
            },
            {
                "2011-1": "2011-10-11"
            },
            {
                "2011": "2011-02-11"
            },
            {
                "2009-2": "2010-07-01"
            },
            {
                "2009-1": "2009-12-12"
            },
            {
                "2009": "2009-11-24"
            }
        ],
        "theories": [
            "PerfectBasics",
            "Sigma",
            "Perfect"
        ]
    },
    {
        "session": "Inductive_Inference",
        "title": "Some classical results in inductive inference of recursive functions",
        "authors": [
            "Frank J. Balbach"
        ],
        "topics": [
            "Logic/Computability",
            "Computer science/Machine learning"
        ],
        "date": "2020-08-31",
        "abstract": "\n<p> This entry formalizes some classical concepts and results\nfrom inductive inference of recursive functions. In the basic setting\na partial recursive function (\"strategy\") must identify\n(\"learn\") all functions from a set (\"class\") of\nrecursive functions. To that end the strategy receives more and more\nvalues $f(0), f(1), f(2), \\ldots$ of some function $f$ from the given\nclass and in turn outputs descriptions of partial recursive functions,\nfor example, Gödel numbers. The strategy is considered successful if\nthe sequence of outputs (\"hypotheses\") converges to a\ndescription of $f$. A class of functions learnable in this sense is\ncalled \"learnable in the limit\". The set of all these\nclasses is denoted by LIM. </p>  <p> Other types of\ninference considered are finite learning (FIN), behaviorally correct\nlearning in the limit (BC), and some variants of LIM with restrictions\non the hypotheses: total learning (TOTAL), consistent learning (CONS),\nand class-preserving learning (CP). The main results formalized are\nthe proper inclusions $\\mathrm{FIN} \\subset \\mathrm{CP} \\subset\n\\mathrm{TOTAL} \\subset \\mathrm{CONS} \\subset \\mathrm{LIM} \\subset\n\\mathrm{BC} \\subset 2^{\\mathcal{R}}$, where $\\mathcal{R}$ is the set\nof all total recursive functions.  Further results show that for all\nthese inference types except CONS, strategies can be assumed to be\ntotal recursive functions; that all inference types but CP are closed\nunder the subset relation between classes; and that no inference type\nis closed under the union of classes. </p>  <p> The above\nis based on a formalization of recursive functions heavily inspired by\nthe <a\nhref=\"https://www.isa-afp.org/entries/Universal_Turing_Machine.html\">Universal\nTuring Machine</a> entry by Xu et al., but different in that it\nmodels partial functions with codomain <em>nat\noption</em>. The formalization contains a construction of a\nuniversal partial recursive function, without resorting to Turing\nmachines, introduces decidability and recursive enumerability, and\nproves some standard results: existence of a Kleene normal form, the\n<em>s-m-n</em> theorem, Rice's theorem, and assorted\nfixed-point theorems (recursion theorems) by Kleene, Rogers, and\nSmullyan. </p>",
        "licence": "BSD",
        "theories": [
            "Partial_Recursive",
            "Universal",
            "Standard_Results",
            "Inductive_Inference_Basics",
            "CP_FIN_NUM",
            "CONS_LIM",
            "Lemma_R",
            "LIM_BC",
            "TOTAL_CONS",
            "R1_BC",
            "Union"
        ]
    },
    {
        "session": "Berlekamp_Zassenhaus",
        "title": "The Factorization Algorithm of Berlekamp and Zassenhaus",
        "authors": [
            "Jose Divasón",
            "Sebastiaan J. C. Joosten",
            "René Thiemann",
            "Akihisa Yamada"
        ],
        "date": "2016-10-14",
        "topics": [
            "Mathematics/Algebra"
        ],
        "abstract": "\n<p>We formalize the Berlekamp-Zassenhaus algorithm for factoring\nsquare-free integer polynomials in Isabelle/HOL. We further adapt an\nexisting formalization of Yun’s square-free factorization algorithm to\ninteger polynomials, and thus provide an efficient and certified\nfactorization algorithm for arbitrary univariate polynomials.\n</p>\n<p>The algorithm first performs a factorization in the prime field GF(p) and\nthen performs computations in the integer ring modulo p^k, where both\np and k are determined at runtime. Since a natural modeling of these\nstructures via dependent types is not possible in Isabelle/HOL, we\nformalize the whole algorithm using Isabelle’s recent addition of\nlocal type definitions.\n</p>\n<p>Through experiments we verify that our algorithm factors polynomials of degree\n100 within seconds.\n</p>",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-09-07"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            }
        ],
        "dependencies": [
            "Pre_BZ"
        ],
        "theories": [
            "Finite_Field",
            "Arithmetic_Record_Based",
            "Finite_Field_Record_Based",
            "Matrix_Record_Based",
            "More_Missing_Multiset",
            "Unique_Factorization",
            "Unique_Factorization_Poly",
            "Poly_Mod",
            "Poly_Mod_Finite_Field",
            "Karatsuba_Multiplication",
            "Polynomial_Record_Based",
            "Poly_Mod_Finite_Field_Record_Based",
            "Chinese_Remainder_Poly",
            "Berlekamp_Type_Based",
            "Distinct_Degree_Factorization",
            "Finite_Field_Factorization",
            "Finite_Field_Factorization_Record_Based",
            "Hensel_Lifting",
            "Hensel_Lifting_Type_Based",
            "Berlekamp_Hensel",
            "Square_Free_Int_To_Square_Free_GFp",
            "Suitable_Prime",
            "Degree_Bound",
            "Mahler_Measure",
            "Factor_Bound",
            "Sublist_Iteration",
            "Reconstruction",
            "Code_Abort_Gcd",
            "Berlekamp_Zassenhaus",
            "Gcd_Finite_Field_Impl",
            "Square_Free_Factorization_Int",
            "Factorize_Int_Poly",
            "Factorize_Rat_Poly"
        ]
    },
    {
        "session": "Smith_Normal_Form",
        "title": "A verified algorithm for computing the Smith normal form of a matrix",
        "authors": [
            "Jose Divasón"
        ],
        "topics": [
            "Mathematics/Algebra",
            "Computer science/Algorithms/Mathematical"
        ],
        "date": "2020-05-23",
        "abstract": "\nThis work presents a formal proof in Isabelle/HOL of an algorithm to\ntransform a matrix into its Smith normal form, a canonical matrix\nform, in a general setting: the algorithm is parameterized by\noperations to prove its existence over elementary divisor rings, while\nexecution is guaranteed over Euclidean domains. We also provide a\nformal proof on some results about the generality of this algorithm as\nwell as the uniqueness of the Smith normal form.  Since Isabelle/HOL\ndoes not feature dependent types, the development is carried out\nswitching conveniently between two different existing libraries: the\nHermite normal form (based on HOL Analysis) and the Jordan normal form\nAFP entries. This permits to reuse results from both developments and\nit is done by means of the lifting and transfer package together with\nthe use of local type definitions.",
        "licence": "BSD",
        "dependencies": [
            "Hermite",
            "Perron_Frobenius",
            "List-Index",
            "Berlekamp_Zassenhaus"
        ],
        "theories": [
            "Smith_Normal_Form",
            "Diagonal_To_Smith",
            "Mod_Type_Connect",
            "SNF_Missing_Lemmas",
            "Cauchy_Binet",
            "Smith_Normal_Form_JNF",
            "Rings2_Extended",
            "Finite_Field_Mod_Type_Connection",
            "Admits_SNF_From_Diagonal_Iff_Bezout_Ring",
            "SNF_Uniqueness",
            "Cauchy_Binet_HOL_Analysis",
            "Diagonalize",
            "SNF_Algorithm_Two_Steps",
            "Diagonal_To_Smith_JNF",
            "SNF_Algorithm_Two_Steps_JNF",
            "SNF_Algorithm",
            "SNF_Algorithm_HOL_Analysis",
            "Elementary_Divisor_Rings",
            "SNF_Algorithm_Euclidean_Domain",
            "Smith_Certified"
        ]
    },
    {
        "session": "AutoFocus-Stream",
        "title": "AutoFocus Stream Processing for Single-Clocking and Multi-Clocking Semantics",
        "authors": [
            "David Trachtenherz"
        ],
        "date": "2011-02-23",
        "topics": [
            "Computer science/Programming languages/Language definitions"
        ],
        "abstract": "We formalize the AutoFocus Semantics (a time-synchronous subset of the Focus formalism) as stream processing functions on finite and infinite message streams represented as finite/infinite lists. The formalization comprises both the conventional single-clocking semantics (uniform global clock for all components and communications channels) and its extension to multi-clocking semantics (internal execution clocking of a component may be a multiple of the external communication clocking). The semantics is defined by generic stream processing functions making it suitable for simulation/code generation in Isabelle/HOL. Furthermore, a number of AutoFocus semantics properties are formalized using definitions from the IntervalLogic theories.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-03-08"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-24"
            },
            {
                "2011-1": "2011-10-11"
            },
            {
                "2011": "2011-02-24"
            }
        ],
        "dependencies": [
            "Nat-Interval-Logic"
        ],
        "theories": [
            "ListSlice",
            "AF_Stream",
            "AF_Stream_Exec",
            "IL_AF_Stream",
            "IL_AF_Stream_Exec"
        ]
    },
    {
        "session": "MFMC_Countable",
        "title": "A Formal Proof of the Max-Flow Min-Cut Theorem for Countable Networks",
        "authors": [
            "Andreas Lochbihler"
        ],
        "date": "2016-05-09",
        "topics": [
            "Mathematics/Graph theory"
        ],
        "abstract": "\nThis article formalises a proof of the maximum-flow minimal-cut\ntheorem for networks with countably many edges.  A network is a\ndirected graph with non-negative real-valued edge labels and two\ndedicated vertices, the source and the sink.  A flow in a network\nassigns non-negative real numbers to the edges such that for all\nvertices except for the source and the sink, the sum of values on\nincoming edges equals the sum of values on outgoing edges.  A cut is a\nsubset of the vertices which contains the source, but not the sink.\nOur theorem states that in every network, there is a flow and a cut\nsuch that the flow saturates all the edges going out of the cut and is\nzero on all the incoming edges.  The proof is based on the paper\n<emph>The Max-Flow Min-Cut theorem for countable networks</emph> by\nAharoni et al.  Additionally, we prove a characterisation of the\nlifting operation for relations on discrete probability distributions,\nwhich leads to a concise proof of its distributivity over relation\ncomposition.",
        "extra": {
            "Change history": "[2017-09-06]\nderive characterisation for the lifting operations on discrete distributions from finite version of the max-flow min-cut theorem\n(revision a7a198f5bab0)<br>\n[2020-12-19]\nsimpler proof of linkability for bounded unhindered bipartite webs, leading to a simpler proof for networks with bounded out-capacities\n(revision 93ca33f4d915)<br>"
        },
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-05-09"
            }
        ],
        "dependencies": [
            "EdmondsKarp_Maxflow"
        ],
        "theories": [
            "MFMC_Misc",
            "MFMC_Finite",
            "Matrix_For_Marginals",
            "MFMC_Network",
            "MFMC_Web",
            "MFMC_Reduction",
            "MFMC_Bounded",
            "MFMC_Flow_Attainability",
            "MFMC_Unbounded",
            "Max_Flow_Min_Cut_Countable",
            "Rel_PMF_Characterisation"
        ]
    },
    {
        "session": "Verified-Prover",
        "title": "A Mechanically Verified, Efficient, Sound and Complete Theorem Prover For First Order Logic",
        "authors": [
            "Tom Ridge"
        ],
        "date": "2004-09-28",
        "topics": [
            "Logic/General logic/Mechanization of proofs"
        ],
        "abstract": "Soundness and completeness for a system of first order logic are formally proved, building on James Margetson's formalization of work by Wainer and Wallen. The completeness proofs naturally suggest an algorithm to derive proofs. This algorithm, which can be implemented tail recursively, is formalized in Isabelle/HOL. The algorithm can be executed via the rewriting tactics of Isabelle. Alternatively, the definitions can be exported to OCaml, yielding a directly executable program.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-03-02"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-24"
            },
            {
                "2011-1": "2011-10-11"
            },
            {
                "2011": "2011-02-11"
            },
            {
                "2009-2": "2010-07-01"
            },
            {
                "2009-1": "2009-12-12"
            },
            {
                "2009": "2009-04-29"
            },
            {
                "2008": "2008-06-10"
            },
            {
                "2007": "2007-11-27"
            },
            {
                "2005": "2005-10-14"
            },
            {
                "2004": "2004-09-28"
            }
        ],
        "theories": [
            "Prover"
        ]
    },
    {
        "session": "Bernoulli",
        "title": "Bernoulli Numbers",
        "authors": [
            "Lukas Bulwahn",
            "Manuel Eberl"
        ],
        "topics": [
            "Mathematics/Analysis",
            "Mathematics/Number theory"
        ],
        "date": "2017-01-24",
        "abstract": "\n<p>Bernoulli numbers were first discovered in the closed-form\nexpansion of the sum 1<sup>m</sup> +\n2<sup>m</sup> + &hellip; + n<sup>m</sup>\nfor a fixed m and appear in many other places. This entry provides\nthree different definitions for them: a recursive one, an explicit\none, and one through their exponential generating function.</p>\n<p>In addition, we prove some basic facts, e.g. their relation\nto sums of powers of integers and that all odd Bernoulli numbers\nexcept the first are zero, and some advanced facts like their\nrelationship to the Riemann zeta function on positive even\nintegers.</p>\n<p>We also prove the correctness of the\nAkiyama&ndash;Tanigawa algorithm for computing Bernoulli numbers\nwith reasonable efficiency, and we define the periodic Bernoulli\npolynomials (which appear e.g. in the Euler&ndash;MacLaurin\nsummation formula and the expansion of the log-Gamma function) and\nprove their basic properties.</p>",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2017-01-24"
            }
        ],
        "theories": [
            "Bernoulli",
            "Periodic_Bernpoly",
            "Bernoulli_FPS",
            "Bernoulli_Zeta"
        ]
    },
    {
        "session": "CISC-Kernel",
        "title": "Formal Specification of a Generic Separation Kernel",
        "authors": [
            "Freek Verbeek",
            "Sergey Tverdyshev",
            "Oto Havle",
            "Holger Blasum",
            "Bruno Langenstein",
            "Werner Stephan",
            "Yakoub Nemouchi",
            "Abderrahmane Feliachi",
            "Burkhart Wolff",
            "Julien Schmaltz"
        ],
        "date": "2014-07-18",
        "topics": [
            "Computer science/Security"
        ],
        "abstract": "\n<p>Intransitive noninterference has been a widely studied topic in the last\nfew decades. Several well-established methodologies apply interactive\ntheorem proving to formulate a noninterference theorem over abstract\nacademic models. In joint work with several industrial and academic partners\nthroughout Europe, we are helping in the certification process of PikeOS, an\nindustrial separation kernel developed at SYSGO. In this process,\nestablished theories could not be applied. We present a new generic model of\nseparation kernels and a new theory of intransitive noninterference. The\nmodel is rich in detail, making it suitable for formal verification of\nrealistic and industrial systems such as PikeOS. Using a refinement-based\ntheorem proving approach, we ensure that proofs remain manageable.</p>\n<p>\nThis document corresponds to the deliverable D31.1 of the EURO-MILS\nProject <a href=\"http://www.euromils.eu\">http://www.euromils.eu</a>.</p>",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2014-07-18"
            }
        ],
        "theories": [
            "Option_Binders",
            "List_Theorems",
            "K",
            "SK",
            "ISK",
            "CISK",
            "Step_configuration",
            "Step_policies",
            "Step",
            "Step_invariants",
            "Step_vpeq",
            "Step_vpeq_locally_respects",
            "Step_vpeq_weakly_step_consistent",
            "Separation_kernel_model",
            "Link_separation_kernel_model_to_CISK"
        ]
    },
    {
        "session": "Taylor_Models",
        "title": "Taylor Models",
        "authors": [
            "Christoph Traut",
            "Fabian Immler"
        ],
        "topics": [
            "Computer science/Algorithms/Mathematical",
            "Computer science/Data structures",
            "Mathematics/Analysis",
            "Mathematics/Algebra"
        ],
        "date": "2018-01-08",
        "abstract": "\nWe present a formally verified implementation of multivariate Taylor\nmodels. Taylor models are a form of rigorous polynomial approximation,\nconsisting of an approximation polynomial based on Taylor expansions,\ncombined with a rigorous bound on the approximation error. Taylor\nmodels were introduced as a tool to mitigate the dependency problem of\ninterval arithmetic. Our implementation automatically computes Taylor\nmodels for the class of elementary functions, expressed by composition\nof arithmetic operations and basic functions like exp, sin, or square\nroot.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2018-01-08"
            }
        ],
        "dependencies": [
            "Affine_Arithmetic"
        ],
        "theories": [
            "Polynomial_Expression",
            "Float_Topology",
            "Horner_Eval",
            "Polynomial_Expression_Additional",
            "Taylor_Models_Misc",
            "Taylor_Models",
            "Experiments"
        ]
    },
    {
        "session": "Lp",
        "title": "Lp spaces",
        "authors": [
            "Sebastien Gouezel"
        ],
        "date": "2016-10-05",
        "topics": [
            "Mathematics/Analysis"
        ],
        "abstract": "\nLp is the space of functions whose p-th power is integrable. It is one of the most fundamental Banach spaces that is used in analysis and probability. We develop a framework for function spaces, and then implement the Lp spaces in this framework using the existing integration theory in Isabelle/HOL. Our development contains most fundamental properties of Lp spaces, notably the Hölder and Minkowski inequalities, completeness of Lp, duality, stability under almost sure convergence, multiplication of functions in Lp and Lq, stability under conditional expectation.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            }
        ],
        "dependencies": [
            "Ergodic_Theory"
        ],
        "theories": [
            "Functional_Spaces",
            "Lp"
        ]
    },
    {
        "session": "Hidden_Markov_Models",
        "title": "Hidden Markov Models",
        "authors": [
            "Simon Wimmer"
        ],
        "topics": [
            "Mathematics/Probability theory",
            "Computer science/Algorithms"
        ],
        "date": "2018-05-25",
        "abstract": "\nThis entry contains a formalization of hidden Markov models [3] based\non Johannes Hölzl's formalization of discrete time Markov chains\n[1]. The basic definitions are provided and the correctness of two\nmain (dynamic programming) algorithms for hidden Markov models is\nproved: the forward algorithm for computing the likelihood of an\nobserved sequence, and the Viterbi algorithm for decoding the most\nprobable hidden state sequence. The Viterbi algorithm is made\nexecutable including memoization.  Hidden markov models have various\napplications in natural language processing. For an introduction see\nJurafsky and Martin [2].",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2018-05-25"
            }
        ],
        "dependencies": [
            "Markov_Models",
            "Monad_Memo_DP"
        ],
        "theories": [
            "Auxiliary",
            "Hidden_Markov_Model",
            "HMM_Implementation",
            "HMM_Example"
        ]
    },
    {
        "session": "Stream_Fusion_Code",
        "title": "Stream Fusion in HOL with Code Generation",
        "authors": [
            "Andreas Lochbihler",
            "Alexandra Maximova"
        ],
        "date": "2014-10-10",
        "topics": [
            "Computer science/Functional programming"
        ],
        "abstract": "Stream Fusion is a system for removing intermediate list data structures from functional programs, in particular Haskell. This entry adapts stream fusion to Isabelle/HOL and its code generator. We define stream types for finite and possibly infinite lists and stream versions for most of the fusible list functions in the theories List and Coinductive_List, and prove them correct with respect to the conversion functions between lists and streams. The Stream Fusion transformation itself is implemented as a simproc in the preprocessor of the code generator. [Brian Huffman's <a href=\"http://isa-afp.org/entries/Stream-Fusion.html\">AFP entry</a> formalises stream fusion in HOLCF for the domain of lazy lists to prove the GHC compiler rewrite rules correct. In contrast, this work enables Isabelle's code generator to perform stream fusion itself. To that end, it covers both finite and coinductive lists from the HOL library and the Coinductive entry. The fusible list functions require specification and proof principles different from Huffman's.]",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-10-13"
            },
            {
                "2014": "2014-10-10"
            }
        ],
        "dependencies": [
            "Coinductive"
        ],
        "theories": [
            "Stream_Fusion",
            "files/stream_fusion.ML",
            "Stream_Fusion_List",
            "Stream_Fusion_LList",
            "Stream_Fusion_Examples"
        ]
    },
    {
        "session": "SumSquares",
        "title": "Sums of Two and Four Squares",
        "authors": [
            "Roelof Oosterhuis"
        ],
        "date": "2007-08-12",
        "topics": [
            "Mathematics/Number theory"
        ],
        "abstract": "This document presents the mechanised proofs of the following results:<ul><li>any prime number of the form 4m+1 can be written as the sum of two squares;</li><li>any natural number can be written as the sum of four squares</li></ul>",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-24"
            },
            {
                "2011-1": "2011-10-11"
            },
            {
                "2011": "2011-02-11"
            },
            {
                "2009-2": "2010-07-01"
            },
            {
                "2009-1": "2009-12-12"
            },
            {
                "2009": "2009-04-29"
            },
            {
                "2008": "2008-06-10"
            },
            {
                "2007": "2007-11-27"
            }
        ],
        "theories": [
            "TwoSquares",
            "FourSquares"
        ]
    },
    {
        "session": "Relation_Algebra",
        "title": "Relation Algebra",
        "authors": [
            "Alasdair Armstrong",
            "Simon Foster",
            "Georg Struth",
            "Tjark Weber"
        ],
        "date": "2014-01-25",
        "topics": [
            "Mathematics/Algebra"
        ],
        "abstract": "Tarski's algebra of binary relations is formalised along the lines of\nthe standard textbooks of Maddux and Schmidt and Ströhlein. This\nincludes relation-algebraic concepts such as subidentities, vectors and\na domain operation as well as various notions associated to functions.\nRelation algebras are also expanded by a reflexive transitive closure\noperation, and they are linked with Kleene algebras and models of binary\nrelations and Boolean matrices.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2014-01-31"
            },
            {
                "2013-2": "2014-01-25"
            }
        ],
        "dependencies": [
            "Kleene_Algebra"
        ],
        "theories": [
            "More_Boolean_Algebra",
            "Relation_Algebra",
            "Relation_Algebra_Vectors",
            "Relation_Algebra_Tests",
            "Relation_Algebra_Functions",
            "Relation_Algebra_Direct_Products",
            "Relation_Algebra_RTC",
            "Relation_Algebra_Models"
        ]
    },
    {
        "session": "Combinatorics_Words",
        "title": "Combinatorics on Words Basics",
        "authors": [
            "Štěpán Holub",
            "Martin Raška",
            "Štěpán Starosta"
        ],
        "topics": [
            "Computer science/Automata and formal languages"
        ],
        "date": "2021-05-24",
        "abstract": "\nWe formalize basics of Combinatorics on Words. This is an extension of\nexisting theories on lists. We provide additional properties related\nto prefix, suffix, factor, length and rotation. The topics include\nprefix and suffix comparability, mismatch, word power, total and\nreversed morphisms, border, periods, primitivity and roots. We also\nformalize basic, mostly folklore results related to word equations:\nequidivisibility, commutation and conjugation. Slightly advanced\nproperties include the Periodicity lemma (often cited as the Fine and\nWilf theorem) and the variant of the Lyndon-Schützenberger theorem for\nwords. We support the algebraic point of view which sees words as\ngenerators of submonoids of a free monoid. This leads to the concepts\nof the (free) hull, the (free) basis (or code).",
        "licence": "BSD",
        "theories": [
            "Arithmetical_Hints",
            "Reverse_Symmetry",
            "CoWBasic",
            "Submonoids",
            "Periodicity_Lemma",
            "Lyndon_Schutzenberger",
            "CoWAll"
        ]
    },
    {
        "session": "Ramsey-Infinite",
        "title": "Ramsey's theorem, infinitary version",
        "authors": [
            "Tom Ridge"
        ],
        "date": "2004-09-20",
        "topics": [
            "Mathematics/Combinatorics"
        ],
        "abstract": "This formalization of Ramsey's theorem (infinitary version) is taken from Boolos and Jeffrey, <i>Computability and Logic</i>, 3rd edition, Chapter 26. It differs slightly from the text by assuming a slightly stronger hypothesis. In particular, the induction hypothesis is stronger, holding for any infinite subset of the naturals. This avoids the rather peculiar mapping argument between kj and aikj on p.263, which is unnecessary and slightly mars this really beautiful result.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-24"
            },
            {
                "2011-1": "2011-10-11"
            },
            {
                "2011": "2011-02-11"
            },
            {
                "2009-2": "2010-07-01"
            },
            {
                "2009-1": "2009-12-12"
            },
            {
                "2009": "2009-04-29"
            },
            {
                "2008": "2008-06-10"
            },
            {
                "2007": "2007-11-27"
            },
            {
                "2005": "2005-10-14"
            },
            {
                "2004": "2004-09-21"
            },
            {
                "2004": "2004-09-20"
            }
        ],
        "theories": [
            "Ramsey"
        ]
    },
    {
        "session": "FocusStreamsCaseStudies",
        "title": "Stream Processing Components: Isabelle/HOL Formalisation and Case Studies",
        "authors": [
            "Maria Spichkova"
        ],
        "date": "2013-11-14",
        "topics": [
            "Computer science/Programming languages/Language definitions"
        ],
        "abstract": "This set of theories presents an Isabelle/HOL formalisation of stream processing components introduced\nin Focus,\na framework for formal specification and development of interactive systems.\nThis is an extended and updated version of the formalisation, which was\nelaborated within the methodology \"Focus on Isabelle\".\nIn addition, we also applied the formalisation on three case studies\nthat cover different application areas: process control (Steam Boiler System),\ndata transmission (FlexRay communication protocol),\nmemory and processing components (Automotive-Gateway System).",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-18"
            }
        ],
        "theories": [
            "ArithExtras",
            "ListExtras",
            "arith_hints",
            "stream",
            "BitBoolTS",
            "JoinSplitTime",
            "SteamBoiler",
            "SteamBoiler_proof",
            "FR_types",
            "FR",
            "FR_proof",
            "Gateway_types",
            "Gateway",
            "Gateway_proof_aux",
            "Gateway_proof"
        ]
    },
    {
        "session": "Perron_Frobenius",
        "title": "Perron-Frobenius Theorem for Spectral Radius Analysis",
        "authors": [
            "Jose Divasón",
            "Ondřej Kunčar",
            "René Thiemann",
            "Akihisa Yamada"
        ],
        "date": "2016-05-20",
        "topics": [
            "Mathematics/Algebra"
        ],
        "abstract": "\n<p>The spectral radius of a matrix A is the maximum norm of all\neigenvalues of A. In previous work we already formalized that for a\ncomplex matrix A, the values in A<sup>n</sup> grow polynomially in n\nif and only if the spectral radius is at most one. One problem with\nthe above characterization is the determination of all\n<em>complex</em> eigenvalues. In case A contains only non-negative\nreal values, a simplification is possible with the help of the\nPerron&ndash;Frobenius theorem, which tells us that it suffices to consider only\nthe <em>real</em> eigenvalues of A, i.e., applying Sturm's method can\ndecide the polynomial growth of A<sup>n</sup>. </p><p> We formalize\nthe Perron&ndash;Frobenius theorem based on a proof via Brouwer's fixpoint\ntheorem, which is available in the HOL multivariate analysis (HMA)\nlibrary. Since the results on the spectral radius is based on matrices\nin the Jordan normal form (JNF) library, we further develop a\nconnection which allows us to easily transfer theorems between HMA and\nJNF. With this connection we derive the combined result: if A is a\nnon-negative real matrix, and no real eigenvalue of A is strictly\nlarger than one, then A<sup>n</sup> is polynomially bounded in n. </p>",
        "extra": {
            "Change history": "[2017-10-18]\nadded Perron-Frobenius theorem for irreducible matrices with generalization\n(revision bda1f1ce8a1c)<br/>\n[2018-05-17]\nprove conjecture of CPP'18 paper Jordan blocks of spectral radius have maximum size\n(revision ffdb3794e5d5)"
        },
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-18"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-05-20"
            }
        ],
        "dependencies": [
            "Jordan_Normal_Form",
            "Polynomial_Factorization",
            "Rank_Nullity_Theorem",
            "Sturm_Sequences"
        ],
        "theories": [
            "Cancel_Card_Constraint",
            "files/cancel_card_constraint.ML",
            "Bij_Nat",
            "HMA_Connect",
            "Perron_Frobenius_Aux",
            "Perron_Frobenius",
            "Roots_Unity",
            "Perron_Frobenius_Irreducible",
            "Perron_Frobenius_General",
            "Spectral_Radius_Theory",
            "Spectral_Radius_Largest_Jordan_Block",
            "Hom_Gauss_Jordan",
            "Spectral_Radius_Theory_2",
            "Check_Matrix_Growth"
        ]
    },
    {
        "session": "Gromov_Hyperbolicity",
        "title": "Gromov Hyperbolicity",
        "authors": [
            "Sebastien Gouezel"
        ],
        "topics": [
            "Mathematics/Geometry"
        ],
        "date": "2018-01-16",
        "abstract": "\nA geodesic metric space is Gromov hyperbolic if all its geodesic\ntriangles are thin, i.e., every side is contained in a fixed\nthickening of the two other sides. While this definition looks\ninnocuous, it has proved extremely important and versatile in modern\ngeometry since its introduction by Gromov.  We formalize the basic\nclassical properties of Gromov hyperbolic spaces, notably the Morse\nlemma asserting that quasigeodesics are close to geodesics, the\ninvariance of hyperbolicity under quasi-isometries, we define and\nstudy the Gromov boundary and its associated distance, and prove that\na quasi-isometry between Gromov hyperbolic spaces extends to a\nhomeomorphism of the boundaries. We also prove a less classical\ntheorem, by Bonk and Schramm, asserting that a Gromov hyperbolic space\nembeds isometrically in a geodesic Gromov-hyperbolic space. As the\noriginal proof uses a transfinite sequence of Cauchy completions, this\nis an interesting formalization exercise.  Along the way, we introduce\nbasic material on isometries, quasi-isometries, Lipschitz maps,\ngeodesic spaces, the Hausdorff distance, the Cauchy completion of a\nmetric space, and the exponential on extended real numbers.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            }
        ],
        "dependencies": [
            "Ergodic_Theory"
        ],
        "theories": [
            "Library_Complements",
            "Eexp_Eln",
            "Hausdorff_Distance",
            "Isometries",
            "Metric_Completion",
            "Gromov_Hyperbolicity",
            "Morse_Gromov_Theorem",
            "Bonk_Schramm_Extension",
            "Gromov_Boundary",
            "Boundary_Extension",
            "Busemann_Function",
            "Isometries_Classification"
        ]
    },
    {
        "session": "Projective_Measurements",
        "title": "Quantum projective measurements and the CHSH inequality",
        "authors": [
            "Mnacho Echenim"
        ],
        "topics": [
            "Computer science/Algorithms/Quantum computing",
            "Mathematics/Physics/Quantum information"
        ],
        "date": "2021-03-03",
        "abstract": "\nThis work contains a formalization of quantum projective measurements,\nalso known as von Neumann measurements, which are based on elements of\nspectral theory. We also formalized the CHSH inequality, an inequality\ninvolving expectations in a probability space that is violated by\nquantum measurements, thus proving that quantum mechanics cannot be modeled with an underlying local hidden-variable theory.",
        "licence": "BSD",
        "dependencies": [
            "Isabelle_Marries_Dirac",
            "QHLProver"
        ],
        "theories": [
            "Linear_Algebra_Complements",
            "Projective_Measurements",
            "CHSH_Inequality"
        ]
    },
    {
        "session": "Lazy-Lists-II",
        "title": "Lazy Lists II",
        "authors": [
            "Stefan Friedrich"
        ],
        "date": "2004-04-26",
        "topics": [
            "Computer science/Data structures"
        ],
        "abstract": "This theory contains some useful extensions to the LList (lazy list) theory by <a href=\"http://www.cl.cam.ac.uk/~lp15/\">Larry Paulson</a>, including finite, infinite, and positive llists over an alphabet, as well as the new constants take and drop and the prefix order of llists. Finally, the notions of safety and liveness in the sense of Alpern and Schneider (1985) are defined.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-24"
            },
            {
                "2011-1": "2011-10-11"
            },
            {
                "2011": "2011-02-11"
            },
            {
                "2009-2": "2010-07-01"
            },
            {
                "2009-1": "2009-12-12"
            },
            {
                "2009": "2009-04-29"
            },
            {
                "2008": "2008-06-10"
            },
            {
                "2007": "2007-11-27"
            },
            {
                "2005": "2005-10-14"
            },
            {
                "2004": "2004-05-21"
            },
            {
                "2004": "2004-04-27"
            }
        ],
        "dependencies": [
            "Coinductive"
        ],
        "theories": [
            "LList2"
        ]
    },
    {
        "session": "Core_SC_DOM",
        "title": "The Safely Composable DOM",
        "authors": [
            "Achim D. Brucker",
            "Michael Herzberg"
        ],
        "topics": [
            "Computer science/Data structures"
        ],
        "date": "2020-09-28",
        "abstract": "\nIn this AFP entry, we formalize the core of the Safely Composable\nDocument Object Model (SC DOM). The SC DOM improve the standard DOM\n(as formalized in the AFP entry \"Core DOM\") by strengthening\nthe tree boundaries set by shadow roots: in the SC DOM, the shadow\nroot is a sub-class of the document class (instead of a base class).\nThis modifications also results in changes to some API methods (e.g.,\ngetOwnerDocument) to return the nearest shadow root rather than the\ndocument root. As a result, many API methods that, when called on a\nnode inside a shadow tree, would previously ``break out''\nand return or modify nodes that are possibly outside the shadow tree,\nnow stay within its boundaries. This change in behavior makes programs\nthat operate on shadow trees more predictable for the developer and\nallows them to make more assumptions about other code accessing the\nDOM.",
        "licence": "BSD",
        "theories": [
            "Hiding_Type_Variables",
            "Ref",
            "Core_DOM_Basic_Datatypes",
            "BaseClass",
            "Heap_Error_Monad",
            "BaseMonad",
            "ObjectPointer",
            "ObjectClass",
            "ObjectMonad",
            "NodePointer",
            "NodeClass",
            "NodeMonad",
            "ElementPointer",
            "CharacterDataPointer",
            "DocumentPointer",
            "ShadowRootPointer",
            "ElementClass",
            "ElementMonad",
            "CharacterDataClass",
            "CharacterDataMonad",
            "DocumentClass",
            "DocumentMonad",
            "Core_DOM_Functions",
            "Core_DOM_Heap_WF",
            "Core_DOM",
            "Testing_Utils",
            "Core_DOM_BaseTest",
            "Document_adoptNode",
            "Document_getElementById",
            "Node_insertBefore",
            "Node_removeChild",
            "Core_DOM_Tests"
        ]
    },
    {
        "session": "Arith_Prog_Rel_Primes",
        "title": "Arithmetic progressions and relative primes",
        "authors": [
            "José Manuel Rodríguez Caballero"
        ],
        "topics": [
            "Mathematics/Number theory"
        ],
        "date": "2020-02-01",
        "abstract": "\nThis article provides a formalization of the solution obtained by the\nauthor of the Problem “ARITHMETIC PROGRESSIONS” from the\n<a href=\"https://www.ocf.berkeley.edu/~wwu/riddles/putnam.shtml\">\nPutnam exam problems of 2002</a>. The statement of the problem is\nas follows: For which integers <em>n</em> > 1 does the set of positive\nintegers less than and relatively prime to <em>n</em> constitute an\narithmetic progression?",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2020-02-10"
            }
        ],
        "theories": [
            "Arith_Prog_Rel_Primes"
        ]
    },
    {
        "session": "HotelKeyCards",
        "title": "Hotel Key Card System",
        "authors": [
            "Tobias Nipkow"
        ],
        "date": "2006-09-09",
        "topics": [
            "Computer science/Security"
        ],
        "abstract": "Two models of an electronic hotel key card system are contrasted: a state based and a trace based one. Both are defined, verified, and proved equivalent in the theorem prover Isabelle/HOL. It is shown that if a guest follows a certain safety policy regarding her key cards, she can be sure that nobody but her can enter her room.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-24"
            },
            {
                "2011-1": "2011-10-11"
            },
            {
                "2011": "2011-02-11"
            },
            {
                "2009-2": "2010-07-01"
            },
            {
                "2009-1": "2009-12-12"
            },
            {
                "2009": "2009-04-29"
            },
            {
                "2008": "2008-06-10"
            },
            {
                "2007": "2007-11-27"
            }
        ],
        "theories": [
            "Notation",
            "Basis",
            "State",
            "NewCard",
            "Trace",
            "Equivalence"
        ]
    },
    {
        "session": "LinearQuantifierElim",
        "title": "Quantifier Elimination for Linear Arithmetic",
        "authors": [
            "Tobias Nipkow"
        ],
        "date": "2008-01-11",
        "topics": [
            "Logic/General logic/Decidability of theories"
        ],
        "abstract": "This article formalizes quantifier elimination procedures for dense linear orders, linear real arithmetic and Presburger arithmetic. In each case both a DNF-based non-elementary algorithm and one or more (doubly) exponential NNF-based algorithms are formalized, including the well-known algorithms by Ferrante and Rackoff and by Cooper. The NNF-based algorithms for dense linear orders are new but based on Ferrante and Rackoff and on an algorithm by Loos and Weisspfenning which simulates infenitesimals. All algorithms are directly executable. In particular, they yield reflective quantifier elimination procedures for HOL itself. The formalization makes heavy use of locales and is therefore highly modular.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-03-02"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-24"
            },
            {
                "2011-1": "2011-10-11"
            },
            {
                "2011": "2011-02-11"
            },
            {
                "2009-2": "2010-07-01"
            },
            {
                "2009-1": "2009-12-12"
            },
            {
                "2009": "2009-04-29"
            },
            {
                "2008": "2008-06-10"
            },
            {
                "2007": "2008-02-12"
            },
            {
                "2007": "2008-01-24"
            },
            {
                "2007": "2008-01-11"
            }
        ],
        "theories": [
            "Logic",
            "QE",
            "DLO",
            "QEdlo",
            "QEdlo_ex",
            "QEdlo_fr",
            "QEdlo_inf",
            "LinArith",
            "QElin",
            "QElin_opt",
            "FRE",
            "QElin_inf",
            "PresArith",
            "QEpres",
            "Cooper",
            "CertDlo",
            "CertLin"
        ]
    },
    {
        "session": "Slicing",
        "title": "Towards Certified Slicing",
        "authors": [
            "Daniel Wasserrab"
        ],
        "date": "2008-09-16",
        "topics": [
            "Computer science/Programming languages/Static analysis"
        ],
        "abstract": "Slicing is a widely-used technique with applications in e.g. compiler technology and software security. Thus verification of algorithms in these areas is often based on the correctness of slicing, which should ideally be proven independent of concrete programming languages and with the help of well-known verifying techniques such as proof assistants. As a first step in this direction, this contribution presents a framework for dynamic and static intraprocedural slicing based on control flow and program dependence graphs. Abstracting from concrete syntax we base the framework on a graph representation of the program fulfilling certain structural and well-formedness properties.<br><br>The formalization consists of the basic framework (in subdirectory Basic/), the correctness proof for dynamic slicing (in subdirectory Dynamic/), the correctness proof for static intraprocedural slicing (in subdirectory StaticIntra/) and instantiations of the framework with a simple While language (in subdirectory While/) and the sophisticated object-oriented bytecode language of Jinja (in subdirectory JinjaVM/). For more information on the framework, see the TPHOLS 2008 paper by Wasserrab and Lochbihler and the PLAS 2009 paper by Wasserrab et al.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-24"
            },
            {
                "2011-1": "2011-10-11"
            },
            {
                "2011": "2011-02-11"
            },
            {
                "2009-2": "2010-07-01"
            },
            {
                "2009-1": "2009-12-12"
            },
            {
                "2009": "2009-04-30"
            },
            {
                "2009": "2009-04-29"
            },
            {
                "2008": "2008-09-22"
            }
        ],
        "dependencies": [
            "Jinja"
        ],
        "theories": [
            "AuxLemmas",
            "BasicDefs",
            "CFG",
            "CFGExit",
            "Postdomination",
            "CFG_wf",
            "CFGExit_wf",
            "SemanticsCFG",
            "DynDataDependence",
            "DynStandardControlDependence",
            "DynWeakControlDependence",
            "DynPDG",
            "DependentLiveVariables",
            "BitVector",
            "DynSlice",
            "Observable",
            "Distance",
            "DataDependence",
            "Slice",
            "StandardControlDependence",
            "WeakControlDependence",
            "PDG",
            "WeakOrderDependence",
            "CDepInstantiations",
            "ControlDependenceRelations",
            "Com",
            "WCFG",
            "Interpretation",
            "Labels",
            "WellFormed",
            "AdditionalLemmas",
            "DynamicControlDependences",
            "Semantics",
            "WEquivalence",
            "SemanticsWellFormed",
            "StaticControlDependences",
            "JVMCFG",
            "JVMInterpretation",
            "JVMPostdomination",
            "JVMCFG_wf",
            "JVMControlDependences",
            "SemanticsWF",
            "Slicing"
        ]
    },
    {
        "session": "Dependent_SIFUM_Refinement",
        "title": "Compositional Security-Preserving Refinement for Concurrent Imperative Programs",
        "authors": [
            "Toby Murray",
            "Robert Sison",
            "Edward Pierzchalski",
            "Christine Rizkallah"
        ],
        "date": "2016-06-28",
        "topics": [
            "Computer science/Security"
        ],
        "abstract": "\nThe paper \"Compositional Verification and Refinement of Concurrent\nValue-Dependent Noninterference\" by Murray et. al. (CSF 2016) presents\na compositional theory of refinement for a value-dependent\nnoninterference property, defined in (Murray, PLAS 2015), for\nconcurrent programs. This development formalises that refinement\ntheory, and demonstrates its application on some small examples.",
        "extra": {
            "Change history": "[2016-08-19]\nRemoved unused \"stop\" parameters from the sifum_refinement locale.\n(revision dbc482d36372)\n[2016-09-02]\nTobyM extended \"simple\" refinement theory to be usable for all bisimulations.\n(revision 547f31c25f60)"
        },
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-06-28"
            }
        ],
        "dependencies": [
            "Dependent_SIFUM_Type_Systems"
        ],
        "theories": [
            "CompositionalRefinement",
            "Eg1",
            "Eg1RefinementTrivial",
            "Eg2",
            "Eg1Eg2",
            "Eg1Eg2RefinementSimple",
            "EgHighBranchRevC"
        ]
    },
    {
        "session": "IMP2_Binary_Heap",
        "title": "Binary Heaps for IMP2",
        "authors": [
            "Simon Griebel"
        ],
        "topics": [
            "Computer science/Data structures",
            "Computer science/Algorithms"
        ],
        "date": "2019-06-13",
        "abstract": "\nIn this submission array-based binary minimum heaps are formalized.\nThe correctness of the following heap operations is proved: insert,\nget-min, delete-min and make-heap. These are then used to verify an\nin-place heapsort. The formalization is based on IMP2, an imperative\nprogram verification framework implemented in Isabelle/HOL. The\nverified heap functions are iterative versions of the partly recursive\nfunctions found in \"Algorithms and Data Structures – The Basic\nToolbox\" by K. Mehlhorn and P. Sanders and \"Introduction to\nAlgorithms\" by T. H. Cormen, C. E. Leiserson, R. L. Rivest and C.\nStein.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-13"
            }
        ],
        "dependencies": [
            "IMP2"
        ],
        "theories": [
            "IMP2_Binary_Heap"
        ]
    },
    {
        "session": "Auto2_HOL",
        "title": "Auto2 Prover",
        "authors": [
            "Bohua Zhan"
        ],
        "topics": [
            "Tools"
        ],
        "date": "2018-11-20",
        "abstract": "\nAuto2 is a saturation-based heuristic prover for higher-order logic,\nimplemented as a tactic in Isabelle.  This entry contains the\ninstantiation of auto2 for Isabelle/HOL, along with two basic\nexamples: solutions to some of the Pelletier’s problems, and\nelementary number theory of primes.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-11-29"
            }
        ],
        "theories": [
            "HOL_Base",
            "Auto2_HOL",
            "files/AFP/Auto2_HOL/util.ML",
            "files/AFP/Auto2_HOL/util_base.ML",
            "files/auto2_hol.ML",
            "files/AFP/Auto2_HOL/util_logic.ML",
            "files/AFP/Auto2_HOL/box_id.ML",
            "files/AFP/Auto2_HOL/consts.ML",
            "files/AFP/Auto2_HOL/property.ML",
            "files/AFP/Auto2_HOL/wellform.ML",
            "files/AFP/Auto2_HOL/wfterm.ML",
            "files/AFP/Auto2_HOL/rewrite.ML",
            "files/AFP/Auto2_HOL/propertydata.ML",
            "files/AFP/Auto2_HOL/matcher.ML",
            "files/AFP/Auto2_HOL/items.ML",
            "files/AFP/Auto2_HOL/wfdata.ML",
            "files/AFP/Auto2_HOL/auto2_data.ML",
            "files/AFP/Auto2_HOL/status.ML",
            "files/AFP/Auto2_HOL/normalize.ML",
            "files/AFP/Auto2_HOL/proofsteps.ML",
            "files/AFP/Auto2_HOL/auto2_state.ML",
            "files/AFP/Auto2_HOL/logic_steps.ML",
            "files/AFP/Auto2_HOL/auto2.ML",
            "files/AFP/Auto2_HOL/auto2_outer.ML",
            "files/acdata.ML",
            "files/ac_steps.ML",
            "files/unfolding.ML",
            "files/induct_outer.ML",
            "files/extra_hol.ML",
            "Logic_Thms",
            "Order_Thms",
            "files/util_arith.ML",
            "Arith_Thms",
            "files/arith.ML",
            "files/order.ML",
            "files/order_test.ML",
            "files/nat_sub.ML",
            "files/nat_sub_test.ML",
            "Set_Thms",
            "Lists_Thms",
            "files/list_ac.ML",
            "files/list_ac_test.ML",
            "Auto2_Main",
            "Auto2_Test",
            "files/util_test.ML",
            "files/rewrite_test.ML",
            "files/matcher_test.ML",
            "files/normalize_test.ML",
            "files/logic_steps_test.ML",
            "files/acdata_test.ML",
            "Pelletier",
            "Primes_Ex"
        ]
    },
    {
        "session": "Bounded_Deducibility_Security",
        "title": "Bounded-Deducibility Security",
        "authors": [
            "Andrei Popescu",
            "Peter Lammich"
        ],
        "date": "2014-04-22",
        "topics": [
            "Computer science/Security"
        ],
        "abstract": "This is a formalization of bounded-deducibility security (BD\nsecurity), a flexible notion of information-flow security applicable\nto arbitrary input-output automata. It generalizes Sutherland's\nclassic notion of nondeducibility by factoring in declassification\nbounds and trigger, whereas nondeducibility states that, in a\nsystem, information cannot flow between specified sources and sinks,\nBD security indicates upper bounds for the flow and triggers under\nwhich these upper bounds are no longer guaranteed.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2014-04-24"
            }
        ],
        "theories": [
            "Trivia",
            "IO_Automaton",
            "BD_Security",
            "Compositional_Reasoning",
            "Bounded_Deducibility_Security"
        ]
    },
    {
        "session": "Shadow_DOM",
        "title": "A Formal Model of the Document Object Model with Shadow Roots",
        "authors": [
            "Achim D. Brucker",
            "Michael Herzberg"
        ],
        "topics": [
            "Computer science/Data structures"
        ],
        "date": "2020-09-28",
        "abstract": "\nIn this AFP entry, we extend our formalization of the core DOM with\nShadow Roots. Shadow roots are a recent proposal of the web community\nto support a component-based development approach for client-side web\napplications.  Shadow roots are a significant extension to the DOM\nstandard and, as web standards are condemned to be backward\ncompatible, such extensions often result in complex specification that\nmay contain unwanted subtleties that can be detected by a\nformalization.  Our Isabelle/HOL formalization is, in the sense of\nobject-orientation, an extension of our formalization of the core DOM\nand enjoys the same basic properties, i.e., it is extensible, i.e.,\ncan be extended without the need of re-proving already proven\nproperties and executable, i.e., we can generate executable code from\nour specification. We exploit the executability to show that our\nformalization complies to the official standard of the W3C,\nrespectively, the WHATWG.",
        "licence": "BSD",
        "dependencies": [
            "Core_DOM"
        ],
        "theories": [
            "ShadowRootClass",
            "ShadowRootMonad",
            "Shadow_DOM",
            "Shadow_DOM_BaseTest",
            "slots",
            "slots_fallback",
            "Shadow_DOM_Document_adoptNode",
            "Shadow_DOM_Document_getElementById",
            "Shadow_DOM_Node_insertBefore",
            "Shadow_DOM_Node_removeChild",
            "Shadow_DOM_Tests"
        ]
    },
    {
        "session": "Allen_Calculus",
        "title": "Allen's Interval Calculus",
        "authors": [
            "Fadoua Ghourabi"
        ],
        "date": "2016-09-29",
        "topics": [
            "Logic/General logic/Temporal logic",
            "Mathematics/Order"
        ],
        "abstract": "\nAllen’s interval calculus is a qualitative temporal representation of\ntime events. Allen introduced 13 binary relations that describe all\nthe possible arrangements between two events, i.e. intervals with\nnon-zero finite length. The compositions are pertinent to\nreasoning about knowledge of time. In particular, a consistency\nproblem of relation constraints is commonly solved with a guideline\nfrom these compositions. We formalize the relations together with an\naxiomatic system. We proof the validity of the 169 compositions of\nthese relations. We also define nests as the sets of intervals that\nshare a meeting point. We prove that nests give the ordering\nproperties of points without introducing a new datatype for points.\n[1] J.F. Allen. Maintaining Knowledge about Temporal Intervals. In\nCommun. ACM, volume 26, pages 832–843, 1983. [2] J. F. Allen and P. J.\nHayes. A Common-sense Theory of Time. In Proceedings of the 9th\nInternational Joint Conference on Artificial Intelligence (IJCAI’85),\npages 528–531, 1985.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-28"
            },
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-10-05"
            },
            {
                "2016": "2016-09-29"
            }
        ],
        "theories": [
            "xor_cal",
            "axioms",
            "allen",
            "disjoint_relations",
            "jointly_exhaustive",
            "examples",
            "nest"
        ]
    },
    {
        "session": "Architectural_Design_Patterns",
        "title": "A Theory of Architectural Design Patterns",
        "authors": [
            "Diego Marmsoler"
        ],
        "topics": [
            "Computer science/System description languages"
        ],
        "date": "2018-03-01",
        "abstract": "\nThe following document formalizes and verifies several architectural\ndesign patterns. Each pattern specification is formalized in terms of\na locale where the locale assumptions correspond to the assumptions\nwhich a pattern poses on an architecture. Thus, pattern specifications\nmay build on top of each other by interpreting the corresponding\nlocale. A pattern is verified using the framework provided by the AFP\nentry Dynamic Architectures. Currently, the document consists of\nformalizations of 4 different patterns: the singleton, the publisher\nsubscriber, the blackboard pattern, and the blockchain pattern.\nThereby, the publisher component of the publisher subscriber pattern\nis modeled as an instance of the singleton pattern and the blackboard\npattern is modeled as an instance of the publisher subscriber pattern.\nIn general, this entry provides the first steps towards an overall\ntheory of architectural design patterns.",
        "extra": {
            "Change history": "[2018-05-25] changing the major assumption for blockchain architectures from alternative minings to relative mining frequencies (revision 5043c5c71685)<br>\n[2019-04-08] adapting the terminology honest instead of trusted, dishonest instead of untrusted (revision 7af3431a22ae)"
        },
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2018-03-01"
            }
        ],
        "dependencies": [
            "DynamicArchitectures"
        ],
        "theories": [
            "Singleton",
            "Publisher_Subscriber",
            "Blackboard",
            "Auxiliary",
            "RF_LTL",
            "Blockchain"
        ]
    },
    {
        "session": "BNF_Operations",
        "title": "Operations on Bounded Natural Functors",
        "authors": [
            "Jasmin Christian Blanchette",
            "Andrei Popescu",
            "Dmitriy Traytel"
        ],
        "topics": [
            "Tools"
        ],
        "date": "2017-12-19",
        "abstract": "\nThis entry formalizes the closure property of bounded natural functors\n(BNFs) under seven operations. These operations and the corresponding\nproofs constitute the core of Isabelle's (co)datatype package. To\nbe close to the implemented tactics, the proofs are deliberately\nformulated as detailed apply scripts. The (co)datatypes together with\n(co)induction principles and (co)recursors are byproducts of the\nfixpoint operations LFP and GFP. Composition of BNFs is subdivided\ninto four simpler operations: Compose, Kill, Lift, and Permute. The\nN2M operation provides mutual (co)induction principles and\n(co)recursors for nested (co)datatypes.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            }
        ],
        "theories": [
            "LFP",
            "GFP",
            "Compose",
            "Kill",
            "Lift",
            "Permute",
            "N2M"
        ]
    },
    {
        "session": "DynamicArchitectures",
        "title": "Dynamic Architectures",
        "authors": [
            "Diego Marmsoler"
        ],
        "topics": [
            "Computer science/System description languages"
        ],
        "date": "2017-07-28",
        "abstract": "\nThe architecture of a system describes the system's overall\norganization into components and connections between those components.\nWith the emergence of mobile computing, dynamic architectures have\nbecome increasingly important. In such architectures, components may\nappear or disappear, and connections may change over time. In the\nfollowing we mechanize a theory of dynamic architectures and verify\nthe soundness of a corresponding calculus. Therefore, we first\nformalize the notion of configuration traces as a model for dynamic\narchitectures. Then, the behavior of single components is formalized\nin terms of behavior traces and an operator is introduced and studied\nto extract the behavior of a single component out of a given\nconfiguration trace. Then, behavior trace assertions are introduced as\na temporal specification technique to specify behavior of components.\nReasoning about component behavior in a dynamic context is formalized\nin terms of a calculus for dynamic architectures. Finally, the\nsoundness of the calculus is verified by introducing an alternative\ninterpretation for behavior trace assertions over configuration traces\nand proving the rules of the calculus. Since projection may lead to\nfinite as well as infinite behavior traces, they are formalized in\nterms of coinductive lists. Thus, our theory is based on\nLochbihler's formalization of coinductive lists. The theory may\nbe applied to verify properties for dynamic architectures.",
        "extra": {
            "Change history": "[2018-06-07] adding logical operators to specify configuration traces (revision 09178f08f050)<br>"
        },
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2017-07-31"
            }
        ],
        "dependencies": [
            "Coinductive"
        ],
        "theories": [
            "Configuration_Traces",
            "Dynamic_Architecture_Calculus"
        ]
    },
    {
        "session": "Containers",
        "title": "Light-weight Containers",
        "authors": [
            "Andreas Lochbihler"
        ],
        "contributors": [
            "René Thiemann"
        ],
        "date": "2013-04-15",
        "topics": [
            "Computer science/Data structures"
        ],
        "abstract": "\nThis development provides a framework for container types like sets and maps such that generated code implements these containers with different (efficient) data structures.\nThanks to type classes and refinement during code generation, this light-weight approach can seamlessly replace Isabelle's default setup for code generation.\nHeuristics automatically pick one of the available data structures depending on the type of elements to be stored, but users can also choose on their own.\nThe extensible design permits to add more implementations at any time.\n<p>\nTo support arbitrary nesting of sets, we define a linear order on sets based on a linear order of the elements and provide efficient implementations.\nIt even allows to compare complements with non-complements.",
        "extra": {
            "Change history": "[2013-07-11] add pretty printing for sets (revision 7f3f52c5f5fa)<br>\n[2013-09-20]\nprovide generators for canonical type class instantiations\n(revision 159f4401f4a8 by René Thiemann)<br>\n[2014-07-08] add support for going from partial functions to mappings (revision 7a6fc957e8ed)<br>\n[2018-03-05] add two application examples depth-first search and 2SAT (revision e5e1a1da2411)"
        },
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-04-23"
            }
        ],
        "dependencies": [
            "Deriving",
            "Finger-Trees",
            "Regular-Sets",
            "Automatic_Refinement",
            "Collections"
        ],
        "theories": [
            "Containers_Auxiliary",
            "Card_Datatype",
            "List_Fusion",
            "Lexicographic_Order",
            "Extend_Partial_Order",
            "Set_Linorder",
            "Containers_Generator",
            "files/containers_generator.ML",
            "Collection_Order",
            "files/ccompare_generator.ML",
            "List_Proper_Interval",
            "Collection_Eq",
            "files/ceq_generator.ML",
            "Collection_Enum",
            "files/cenum_generator.ML",
            "Equal",
            "RBT_ext",
            "RBT_Mapping2",
            "AssocList",
            "DList_Set",
            "RBT_Set2",
            "Closure_Set",
            "Set_Impl",
            "files/set_impl_generator.ML",
            "Mapping_Impl",
            "files/mapping_impl_generator.ML",
            "Map_To_Mapping",
            "Containers",
            "Compatibility_Containers_Regular_Sets",
            "Containers_Userguide",
            "Card_Datatype_Ex",
            "Map_To_Mapping_Ex",
            "TwoSat_Ex",
            "Containers_DFS_Ex",
            "Containers_TwoSat_Ex"
        ]
    },
    {
        "session": "Descartes_Sign_Rule",
        "title": "Descartes' Rule of Signs",
        "authors": [
            "Manuel Eberl"
        ],
        "date": "2015-12-28",
        "topics": [
            "Mathematics/Analysis"
        ],
        "abstract": "\n<p>\nDescartes' Rule of Signs relates the number of positive real roots of a\npolynomial with the number of sign changes in its coefficient sequence.\n</p><p>\nOur proof follows the simple inductive proof given by Rob Arthan, which was also\nused by John Harrison in his HOL Light formalisation. We proved most of the\nlemmas for arbitrary linearly-ordered integrity domains (e.g. integers,\nrationals, reals); the main result, however, requires the intermediate value\ntheorem and was therefore only proven for real polynomials.\n</p>",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2016-01-05"
            }
        ],
        "theories": [
            "Descartes_Sign_Rule"
        ]
    },
    {
        "session": "Impossible_Geometry",
        "title": "Proving the Impossibility of Trisecting an Angle and Doubling the Cube",
        "authors": [
            "Ralph Romanos",
            "Lawrence C. Paulson"
        ],
        "topics": [
            "Mathematics/Algebra",
            "Mathematics/Geometry"
        ],
        "date": "2012-08-05",
        "abstract": "Squaring the circle, doubling the cube and trisecting an angle, using a compass and straightedge alone, are classic unsolved problems first posed by the ancient Greeks. All three problems were proved to be impossible in the 19th century. The following document presents the proof of the impossibility of solving the latter two problems using Isabelle/HOL, following a proof by Carrega. The proof uses elementary methods: no Galois theory or field extensions. The set of points constructible using a compass and straightedge is defined inductively. Radical expressions, which involve only square roots and arithmetic of rational numbers, are defined, and we find that all constructive points have radical coordinates. Finally, doubling the cube and trisecting certain angles requires solving certain cubic equations that can be proved to have no rational roots. The Isabelle proofs require a great many detailed calculations.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-08-07"
            },
            {
                "2012": "2012-08-06"
            }
        ],
        "theories": [
            "Impossible_Geometry"
        ]
    },
    {
        "session": "AxiomaticCategoryTheory",
        "title": "Axiom Systems for Category Theory in Free Logic",
        "authors": [
            "Christoph Benzmüller",
            "Dana Scott"
        ],
        "topics": [
            "Mathematics/Category theory"
        ],
        "date": "2018-05-23",
        "abstract": "\nThis document provides a concise overview on the core results of our\nprevious work on the exploration of axioms systems for category\ntheory. Extending the previous studies\n(http://arxiv.org/abs/1609.01493) we include one further axiomatic\ntheory in our experiments. This additional theory has been suggested\nby Mac Lane in 1948. We show that the axioms proposed by Mac Lane are\nequivalent to the ones we studied before, which includes an axioms set\nsuggested by Scott in the 1970s and another axioms set proposed by\nFreyd and Scedrov in 1990, which we slightly modified to remedy a\nminor technical issue.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2018-05-23"
            }
        ],
        "theories": [
            "AxiomaticCategoryTheory"
        ]
    },
    {
        "session": "Stream-Fusion",
        "title": "Stream Fusion",
        "authors": [
            "Brian Huffman"
        ],
        "topics": [
            "Computer science/Functional programming"
        ],
        "date": "2009-04-29",
        "abstract": "Stream Fusion is a system for removing intermediate list structures from Haskell programs; it consists of a Haskell library along with several compiler rewrite rules. (The library is available <a href=\"http://hackage.haskell.org/package/stream-fusion\">online</a>.)<br><br>These theories contain a formalization of much of the Stream Fusion library in HOLCF. Lazy list and stream types are defined, along with coercions between the two types, as well as an equivalence relation for streams that generate the same list. List and stream versions of map, filter, foldr, enumFromTo, append, zipWith, and concatMap are defined, and the stream versions are shown to respect stream equivalence.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-24"
            },
            {
                "2011-1": "2011-10-11"
            },
            {
                "2011": "2011-02-11"
            },
            {
                "2009-2": "2010-07-01"
            },
            {
                "2009-1": "2009-12-12"
            },
            {
                "2009": "2009-05-13"
            },
            {
                "2009": "2009-05-11"
            }
        ],
        "theories": [
            "LazyList",
            "Stream",
            "StreamFusion"
        ]
    },
    {
        "session": "CRDT",
        "title": "A framework for establishing Strong Eventual Consistency for Conflict-free Replicated Datatypes",
        "authors": [
            "Victor B. F. Gomes",
            "Martin Kleppmann",
            "Dominic P. Mulligan",
            "Alastair R. Beresford"
        ],
        "topics": [
            "Computer science/Algorithms/Distributed",
            "Computer science/Data structures"
        ],
        "date": "2017-07-07",
        "abstract": "\nIn this work, we focus on the correctness of Conflict-free Replicated\nData Types (CRDTs), a class of algorithm that provides strong eventual\nconsistency guarantees for replicated data. We develop a modular and\nreusable framework for verifying the correctness of CRDT algorithms.\nWe avoid correctness issues that have dogged previous mechanised\nproofs in this area by including a network model in our formalisation,\nand proving that our theorems hold in all possible network behaviours.\nOur axiomatic network model is a standard abstraction that accurately\nreflects the behaviour of real-world computer networks. Moreover, we\nidentify an abstract convergence theorem, a property of order\nrelations, which provides a formal definition of strong eventual\nconsistency. We then obtain the first machine-checked correctness\ntheorems for three concrete CRDTs: the Replicated Growable Array, the\nObserved-Remove Set, and an Increment-Decrement Counter.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2017-07-07"
            }
        ],
        "theories": [
            "Util",
            "Convergence",
            "Network",
            "Ordered_List",
            "RGA",
            "Counter",
            "ORSet"
        ]
    },
    {
        "session": "Stirling_Formula",
        "title": "Stirling's formula",
        "authors": [
            "Manuel Eberl"
        ],
        "date": "2016-09-01",
        "topics": [
            "Mathematics/Analysis"
        ],
        "abstract": "\n<p>This work contains a proof of Stirling's formula both for the factorial $n! \\sim \\sqrt{2\\pi n} (n/e)^n$ on natural numbers and the real\nGamma function $\\Gamma(x)\\sim \\sqrt{2\\pi/x} (x/e)^x$. The proof is based on work by <a\nhref=\"http://www.maths.lancs.ac.uk/~jameson/stirlgamma.pdf\">Graham Jameson</a>.</p>\n<p>This is then extended to the full asymptotic expansion\n$$\\log\\Gamma(z) = \\big(z - \\tfrac{1}{2}\\big)\\log z - z + \\tfrac{1}{2}\\log(2\\pi) + \\sum_{k=1}^{n-1} \\frac{B_{k+1}}{k(k+1)} z^{-k}\\\\\n{} - \\frac{1}{n} \\int_0^\\infty B_n([t])(t + z)^{-n}\\,\\text{d}t$$\nuniformly for all complex $z\\neq 0$ in the cone $\\text{arg}(z)\\leq \\alpha$ for any $\\alpha\\in(0,\\pi)$, with which the above asymptotic\nrelation for &Gamma; is also extended to complex arguments.</p>",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            }
        ],
        "dependencies": [
            "Bernoulli",
            "Landau_Symbols"
        ],
        "theories": [
            "Stirling_Formula",
            "Gamma_Asymptotics"
        ]
    },
    {
        "session": "Special_Function_Bounds",
        "title": "Real-Valued Special Functions: Upper and Lower Bounds",
        "authors": [
            "Lawrence C. Paulson"
        ],
        "date": "2014-08-29",
        "topics": [
            "Mathematics/Analysis"
        ],
        "abstract": "This development proves upper and lower bounds for several familiar real-valued functions. For sin, cos, exp and sqrt, it defines and verifies infinite families of upper and lower bounds, mostly based on Taylor series expansions. For arctan, ln and exp, it verifies a finite collection of upper and lower bounds, originally obtained from the functions' continued fraction expansions using the computer algebra system Maple. A common theme in these proofs is to take the difference between a function and its approximation, which should be zero at one point, and then consider the sign of the derivative. The immediate purpose of this development is to verify axioms used by MetiTarski, an automatic theorem prover for real-valued special functions. Crucial to MetiTarski's operation is the provision of upper and lower bounds for each function of interest.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-09-05"
            },
            {
                "2014": "2014-08-29"
            }
        ],
        "dependencies": [
            "Sturm_Sequences"
        ],
        "theories": [
            "Bounds_Lemmas",
            "Atan_CF_Bounds",
            "Exp_Bounds",
            "Log_CF_Bounds",
            "Sin_Cos_Bounds",
            "Sqrt_Bounds"
        ]
    },
    {
        "session": "SequentInvertibility",
        "title": "Invertibility in Sequent Calculi",
        "authors": [
            "Peter Chapman"
        ],
        "date": "2009-08-28",
        "topics": [
            "Logic/Proof theory"
        ],
        "license": "LGPL",
        "abstract": "The invertibility of the rules of a sequent calculus is important for guiding proof search and can be used in some formalised proofs of Cut admissibility. We present sufficient conditions for when a rule is invertible with respect to a calculus. We illustrate the conditions with examples. It must be noted we give purely syntactic criteria; no guarantees are given as to the suitability of the rules.",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-24"
            },
            {
                "2011-1": "2011-10-11"
            },
            {
                "2011": "2011-02-11"
            },
            {
                "2009-2": "2010-07-01"
            },
            {
                "2009-1": "2009-12-12"
            },
            {
                "2009": "2009-09-01"
            }
        ],
        "theories": [
            "MultiSequents",
            "SingleSuccedent",
            "NominalSequents",
            "ModalSequents",
            "SRCTransforms",
            "SequentInvertibility"
        ]
    },
    {
        "session": "Automated_Stateful_Protocol_Verification",
        "title": "Automated Stateful Protocol Verification",
        "authors": [
            "Andreas V. Hess",
            "Sebastian Mödersheim",
            "Achim D. Brucker",
            "Anders Schlichtkrull"
        ],
        "topics": [
            "Computer science/Security",
            "Tools"
        ],
        "date": "2020-04-08",
        "abstract": "\nIn protocol verification we observe a wide spectrum from fully\nautomated methods to interactive theorem proving with proof assistants\nlike Isabelle/HOL. In this AFP entry, we present a fully-automated\napproach for verifying stateful security protocols, i.e., protocols\nwith mutable state that may span several sessions. The approach\nsupports reachability goals like secrecy and authentication. We also\ninclude a simple user-friendly transaction-based protocol\nspecification language that is embedded into Isabelle.",
        "licence": "BSD",
        "dependencies": [
            "Stateful_Protocol_Composition_and_Typing"
        ],
        "theories": [
            "Transactions",
            "Term_Abstraction",
            "Stateful_Protocol_Model",
            "Term_Variants",
            "Term_Implication",
            "Stateful_Protocol_Verification",
            "Eisbach_Protocol_Verification",
            "ml_yacc_lib",
            "files/ml-yacc-lib/base.sig",
            "files/ml-yacc-lib/join.sml",
            "files/ml-yacc-lib/lrtable.sml",
            "files/ml-yacc-lib/stream.sml",
            "files/ml-yacc-lib/parser2.sml",
            "trac_term",
            "trac_fp_parser",
            "files/trac_parser/trac_fp.grm.sig",
            "files/trac_parser/trac_fp.lex.sml",
            "files/trac_parser/trac_fp.grm.sml",
            "trac_protocol_parser",
            "files/trac_parser/trac_protocol.grm.sig",
            "files/trac_parser/trac_protocol.lex.sml",
            "files/trac_parser/trac_protocol.grm.sml",
            "trac",
            "PSPSP",
            "Keyserver",
            "Keyserver2",
            "Keyserver_Composition",
            "PKCS_Model03",
            "PKCS_Model07",
            "PKCS_Model09",
            "Examples"
        ]
    },
    {
        "session": "Residuated_Lattices",
        "title": "Residuated Lattices",
        "authors": [
            "Victor B. F. Gomes",
            "Georg Struth"
        ],
        "date": "2015-04-15",
        "topics": [
            "Mathematics/Algebra"
        ],
        "abstract": "\nThe theory of residuated lattices, first proposed by Ward and Dilworth, is\nformalised in Isabelle/HOL. This includes concepts of residuated functions;\ntheir adjoints and conjugates. It also contains necessary and sufficient\nconditions for the existence of these operations in an arbitrary lattice.\nThe mathematical components for residuated lattices are linked to the AFP\nentry for relation algebra. In particular, we prove Jonsson and Tsinakis\nconditions for a residuated boolean algebra to form a relation algebra.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-28"
            },
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2015-04-16"
            }
        ],
        "dependencies": [
            "Relation_Algebra"
        ],
        "theories": [
            "Residuated_Lattices",
            "Residuated_Boolean_Algebras",
            "Involutive_Residuated",
            "Action_Algebra",
            "Action_Algebra_Models",
            "Residuated_Relation_Algebra"
        ]
    },
    {
        "session": "Abstract-Rewriting",
        "title": "Abstract Rewriting",
        "topics": [
            "Logic/Rewriting"
        ],
        "date": "2010-06-14",
        "authors": [
            "Christian Sternagel",
            "René Thiemann"
        ],
        "license": "LGPL",
        "abstract": "\nWe present an Isabelle formalization of abstract rewriting (see, e.g.,\nthe book by Baader and Nipkow). First, we define standard relations like\n<i>joinability</i>, <i>meetability</i>, <i>conversion</i>, etc. Then, we\nformalize important properties of abstract rewrite systems, e.g.,\nconfluence and strong normalization. Our main concern is on strong\nnormalization, since this formalization is the basis of <a\nhref=\"http://cl-informatik.uibk.ac.at/software/ceta\">CeTA</a> (which is\nmainly about strong normalization of term rewrite systems). Hence lemmas\ninvolving strong normalization constitute by far the biggest part of this\ntheory. One of those is Newman's lemma.",
        "extra": {
            "Change history": "[2010-09-17] Added theories defining several (ordered)\nsemirings related to strong normalization and giving some standard\ninstances. <br>\n[2013-10-16] Generalized delta-orders from rationals to Archimedean fields."
        },
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-24"
            },
            {
                "2011-1": "2011-10-11"
            },
            {
                "2011": "2011-02-11"
            },
            {
                "2009-2": "2010-06-30"
            },
            {
                "2009-1": "2010-06-17"
            }
        ],
        "dependencies": [
            "Regular-Sets"
        ],
        "theories": [
            "Seq",
            "Abstract_Rewriting",
            "Relative_Rewriting",
            "SN_Orders",
            "SN_Order_Carrier"
        ]
    },
    {
        "session": "Separata",
        "title": "Separata: Isabelle tactics for Separation Algebra",
        "authors": [
            "Zhe Hou",
            "David Sanan",
            "Alwen Tiu",
            "Rajeev Gore",
            "Ranald Clouston"
        ],
        "date": "2016-11-16",
        "topics": [
            "Computer science/Programming languages/Logics",
            "Tools"
        ],
        "abstract": "\nWe bring the labelled sequent calculus $LS_{PASL}$ for propositional\nabstract separation logic to Isabelle. The tactics given here are\ndirectly applied on an extension of the Separation Algebra in the AFP.\nIn addition to the cancellative separation algebra, we further\nconsider some useful properties in the heap model of separation logic,\nsuch as indivisible unit, disjointness, and cross-split. The tactics\nare essentially a proof search procedure for the calculus $LS_{PASL}$.\nWe wrap the tactics in an Isabelle method called separata, and give a\nfew examples of separation logic formulae which are provable by\nseparata.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-11-17"
            }
        ],
        "dependencies": [
            "Separation_Algebra"
        ],
        "theories": [
            "Separata"
        ]
    },
    {
        "session": "Polynomial_Factorization",
        "title": "Polynomial Factorization",
        "topics": [
            "Mathematics/Algebra"
        ],
        "authors": [
            "René Thiemann",
            "Akihisa Yamada"
        ],
        "date": "2016-01-29",
        "abstract": "\nBased on existing libraries for polynomial interpolation and matrices,\nwe formalized several factorization algorithms for polynomials, including\nKronecker's algorithm for integer polynomials,\nYun's square-free factorization algorithm for field polynomials, and\nBerlekamp's algorithm for polynomials over finite fields.\nBy combining the last one with Hensel's lifting,\nwe derive an efficient factorization algorithm for the integer polynomials,\nwhich is then lifted for rational polynomials by mechanizing Gauss' lemma.\nFinally, we assembled a combined factorization algorithm for rational polynomials,\nwhich combines all the mentioned algorithms and additionally uses the explicit formula for roots\nof quadratic polynomials and a rational root test.\n<p>\nAs side products, we developed division algorithms for polynomials over integral domains,\nas well as primality-testing and prime-factorization algorithms for integers.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            }
        ],
        "dependencies": [
            "JNF-AFP-Lib",
            "Partial_Function_MR",
            "Polynomial_Interpolation",
            "Show",
            "Sqrt_Babylonian"
        ],
        "theories": [
            "Missing_List",
            "Missing_Multiset",
            "Precomputation",
            "Order_Polynomial",
            "Explicit_Roots",
            "Dvd_Int_Poly",
            "Missing_Polynomial_Factorial",
            "Gauss_Lemma",
            "Prime_Factorization",
            "Rational_Root_Test",
            "Kronecker_Factorization",
            "Polynomial_Divisibility",
            "Fundamental_Theorem_Algebra_Factorized",
            "Square_Free_Factorization",
            "Gcd_Rat_Poly",
            "Rational_Factorization"
        ]
    },
    {
        "session": "Refine_Imperative_HOL",
        "title": "The Imperative Refinement Framework",
        "authors": [
            "Peter Lammich"
        ],
        "date": "2016-08-08",
        "topics": [
            "Computer science/Programming languages/Transformations",
            "Computer science/Data structures"
        ],
        "abstract": "\nWe present the Imperative Refinement Framework (IRF), a tool that\nsupports a stepwise refinement based approach to imperative programs.\nThis entry is based on the material we presented in [ITP-2015,\nCPP-2016].  It uses the Monadic Refinement Framework as a frontend for\nthe specification of the abstract programs, and Imperative/HOL as a\nbackend to generate executable imperative programs.  The IRF comes\nwith tool support to synthesize imperative programs from more\nabstract, functional ones, using efficient imperative implementations\nfor the abstract data structures.  This entry also includes the\nImperative Isabelle Collection Framework (IICF), which provides a\nlibrary of re-usable imperative collection data structures.  Moreover,\nthis entry contains a quickstart guide and a reference manual, which\nprovide an introduction to using the IRF for Isabelle/HOL experts. It\nalso provids a collection of (partly commented) practical examples,\nsome highlights being Dijkstra's Algorithm, Nested-DFS, and a generic\nworklist algorithm with subsumption.  Finally, this entry contains\nbenchmark scripts that compare the runtime of some examples against\nreference implementations of the algorithms in Java and C++.\n[ITP-2015] Peter Lammich: Refinement to Imperative/HOL. ITP 2015:\n253--269  [CPP-2016] Peter Lammich: Refinement based verification of\nimperative data structures. CPP 2016: 27--36",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-08-08"
            }
        ],
        "dependencies": [
            "Sepref_Prereq",
            "Isar_Ref",
            "List-Index",
            "Collections_Examples",
            "DFS_Framework",
            "Dijkstra_Shortest_Path"
        ],
        "theories": [
            "Concl_Pres_Clarification",
            "Named_Theorems_Rev",
            "Pf_Add",
            "Pf_Mono_Prover",
            "PO_Normalizer",
            "files/PO_Normalizer.ML",
            "Sepref_Misc",
            "Structured_Apply",
            "Term_Synth",
            "User_Smashing",
            "Sepref_Chapter_Tool",
            "Sepref_Id_Op",
            "Sepref_Basic",
            "Sepref_Monadify",
            "Sepref_Constraints",
            "Sepref_Frame",
            "Sepref_Rules",
            "Sepref_Combinator_Setup",
            "Sepref_Translate",
            "Sepref_Definition",
            "Sepref_Intf_Util",
            "Sepref_Tool",
            "Sepref_Chapter_Setup",
            "Sepref_HOL_Bindings",
            "Sepref_Foreach",
            "Sepref_Improper",
            "Sepref",
            "Sepref_Chapter_IICF",
            "IICF_Set",
            "IICF_List_SetO",
            "IICF_Multiset",
            "IICF_Prio_Bag",
            "IICF_List_Mset",
            "IICF_List_MsetO",
            "IICF_List",
            "IICF_Abs_Heap",
            "IICF_HOL_List",
            "IICF_Array_List",
            "IICF_Impl_Heap",
            "IICF_Map",
            "IICF_Prio_Map",
            "IICF_Abs_Heapmap",
            "IICF_Array",
            "IICF_MS_Array_List",
            "IICF_Indexed_Array_List",
            "IICF_Impl_Heapmap",
            "IICF_Matrix",
            "IICF_Array_Matrix",
            "IICF_Sepl_Binding",
            "IICF",
            "Sepref_Chapter_Userguides",
            "Sepref_Guide_Quickstart",
            "Sepref_Guide_Reference",
            "Sepref_Guide_General_Util",
            "Sepref_ICF_Bindings",
            "Sepref_WGraph",
            "Sepref_Chapter_Examples",
            "Sepref_Graph",
            "Sepref_DFS",
            "Sepref_Dijkstra",
            "Sepref_NDFS",
            "Sepref_Minitests",
            "Worklist_Subsumption",
            "Worklist_Subsumption_Impl",
            "Sepref_Snip_Datatype",
            "Sepref_Snip_Combinator",
            "Sepref_All_Examples",
            "Sepref_Chapter_Benchmarks",
            "Heapmap_Bench",
            "Dijkstra_Benchmark",
            "NDFS_Benchmark"
        ]
    },
    {
        "session": "Splay_Tree",
        "title": "Splay Tree",
        "authors": [
            "Tobias Nipkow"
        ],
        "date": "2014-08-12",
        "topics": [
            "Computer science/Data structures"
        ],
        "abstract": "\nSplay trees are self-adjusting binary search trees which were invented by Sleator and Tarjan [JACM 1985].\nThis entry provides executable and verified functional splay trees\nas well as the related splay heaps (due to Okasaki).\n<p>\nThe amortized complexity of splay trees and heaps is analyzed in the AFP entry\n<a href=\"http://isa-afp.org/entries/Amortized_Complexity.html\">Amortized Complexity</a>.",
        "extra": {
            "Change history": "[2016-07-12] Moved splay heaps here from Amortized_Complexity"
        },
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            }
        ],
        "theories": [
            "Splay_Tree",
            "Splay_Map",
            "Splay_Heap"
        ]
    },
    {
        "session": "Tycon",
        "title": "Type Constructor Classes and Monad Transformers",
        "authors": [
            "Brian Huffman"
        ],
        "date": "2012-06-26",
        "topics": [
            "Computer science/Functional programming"
        ],
        "abstract": "\nThese theories contain a formalization of first class type constructors\nand axiomatic constructor classes for HOLCF. This work is described\nin detail in the ICFP 2012 paper <i>Formal Verification of Monad\nTransformers</i> by the author. The formalization is a revised and\nupdated version of earlier joint work with Matthews and White.\n<P>\nBased on the hierarchy of type classes in Haskell, we define classes\nfor functors, monads, monad-plus, etc. Each one includes all the\nstandard laws as axioms. We also provide a new user command,\ntycondef, for defining new type constructors in HOLCF. Using tycondef,\nwe instantiate the type class hierarchy with various monads and monad\ntransformers.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-06-28"
            }
        ],
        "theories": [
            "TypeApp",
            "Coerce",
            "Functor",
            "files/tycondef.ML",
            "Monad",
            "Monad_Zero",
            "Monad_Plus",
            "Monad_Zero_Plus",
            "Lazy_List_Monad",
            "Maybe_Monad",
            "Error_Monad",
            "Writer_Monad",
            "Binary_Tree_Monad",
            "Lift_Monad",
            "Resumption_Transformer",
            "State_Transformer",
            "Error_Transformer",
            "Writer_Transformer",
            "Monad_Transformer"
        ]
    },
    {
        "session": "Modal_Logics_for_NTS",
        "title": "Modal Logics for Nominal Transition Systems",
        "authors": [
            "Tjark Weber",
            "Lars-Henrik Eriksson",
            "Joachim Parrow",
            "Johannes Borgström",
            "Ramunas Gutkovas"
        ],
        "date": "2016-10-25",
        "topics": [
            "Computer science/Concurrency/Process calculi",
            "Logic/General logic/Modal logic"
        ],
        "abstract": "\nWe formalize a uniform semantic substrate for a wide variety of\nprocess calculi where states and action labels can be from arbitrary\nnominal sets. A Hennessy-Milner logic for these systems is defined,\nand proved adequate for bisimulation equivalence. A main novelty is\nthe construction of an infinitary nominal data type to model formulas\nwith (finitely supported) infinite conjunctions and actions that may\ncontain binding names. The logic is generalized to treat different\nbisimulation variants such as early, late and open in a systematic\nway.",
        "extra": {
            "Change history": "[2017-01-29]\nFormalization of weak bisimilarity added\n(revision c87cc2057d9c)"
        },
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-10-27"
            },
            {
                "2016": "2016-10-25"
            }
        ],
        "dependencies": [
            "Nominal2"
        ],
        "theories": [
            "Nominal_Bounded_Set",
            "Nominal_Wellfounded",
            "Residual",
            "Transition_System",
            "Formula",
            "Validity",
            "Logical_Equivalence",
            "Bisimilarity_Implies_Equivalence",
            "Equivalence_Implies_Bisimilarity",
            "Disjunction",
            "Expressive_Completeness",
            "FS_Set",
            "FL_Transition_System",
            "FL_Formula",
            "FL_Validity",
            "FL_Logical_Equivalence",
            "FL_Bisimilarity_Implies_Equivalence",
            "FL_Equivalence_Implies_Bisimilarity",
            "L_Transform",
            "Weak_Transition_System",
            "Weak_Formula",
            "Weak_Validity",
            "Weak_Logical_Equivalence",
            "Weak_Bisimilarity_Implies_Equivalence",
            "Weak_Equivalence_Implies_Bisimilarity",
            "Weak_Expressive_Completeness",
            "S_Transform"
        ]
    },
    {
        "session": "Pratt_Certificate",
        "title": "Pratt's Primality Certificates",
        "authors": [
            "Simon Wimmer",
            "Lars Noschinski"
        ],
        "date": "2013-07-22",
        "topics": [
            "Mathematics/Number theory"
        ],
        "abstract": "In 1975, Pratt introduced a proof system for certifying primes. He showed that a number <i>p</i> is prime iff a primality certificate for <i>p</i> exists. By showing a logarithmic upper bound on the length of the certificates in size of the prime number, he concluded that the decision problem for prime numbers is in NP. This work formalizes soundness and completeness of Pratt's proof system as well as an upper bound for the size of the certificate.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-07-29"
            }
        ],
        "dependencies": [
            "Lehmer"
        ],
        "theories": [
            "Pratt_Certificate",
            "files/pratt.ML",
            "Pratt_Certificate_Code"
        ]
    },
    {
        "session": "Presburger-Automata",
        "title": "Formalizing the Logic-Automaton Connection",
        "authors": [
            "Stefan Berghofer",
            "Markus Reiter"
        ],
        "date": "2009-12-03",
        "topics": [
            "Computer science/Automata and formal languages",
            "Logic/General logic/Decidability of theories"
        ],
        "abstract": "This work presents a formalization of a library for automata on bit strings. It forms the basis of a reflection-based decision procedure for Presburger arithmetic, which is efficiently executable thanks to Isabelle's code generator. With this work, we therefore provide a mechanized proof of a well-known connection between logic and automata theory. The formalization is also described in a publication [TPHOLs 2009].",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-03-02"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-24"
            },
            {
                "2011-1": "2011-10-11"
            },
            {
                "2011": "2011-02-11"
            },
            {
                "2009-2": "2010-07-01"
            },
            {
                "2009-1": "2009-12-12"
            }
        ],
        "theories": [
            "DFS",
            "Presburger_Automata",
            "Exec"
        ]
    },
    {
        "session": "Transitive-Closure",
        "title": "Executable Transitive Closures of Finite Relations",
        "topics": [
            "Computer science/Algorithms/Graph"
        ],
        "date": "2011-03-14",
        "authors": [
            "Christian Sternagel",
            "René Thiemann"
        ],
        "license": "LGPL",
        "abstract": "We provide a generic work-list algorithm to compute the transitive closure of finite relations where only successors of newly detected states are generated. This algorithm is then instantiated for lists over arbitrary carriers and red black trees (which are faster but require a linear order on the carrier), respectively.  Our formalization was performed as part of the IsaFoR/CeTA project where reflexive transitive closures of large tree automata have to be computed.",
        "extra": {
            "Change history": "[2014-09-04] added example simprocs in Finite_Transitive_Closure_Simprocs"
        },
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-24"
            },
            {
                "2011-1": "2011-10-12"
            },
            {
                "2011-1": "2011-10-11"
            },
            {
                "2011": "2011-03-14"
            }
        ],
        "dependencies": [
            "Collections",
            "Matrix"
        ],
        "theories": [
            "Transitive_Closure_Impl",
            "Transitive_Closure_List_Impl",
            "RBT_Map_Set_Extension",
            "Transitive_Closure_RBT_Impl",
            "Finite_Transitive_Closure_Simprocs"
        ]
    },
    {
        "session": "Timed_Automata",
        "title": "Timed Automata",
        "authors": [
            "Simon Wimmer"
        ],
        "date": "2016-03-08",
        "topics": [
            "Computer science/Automata and formal languages"
        ],
        "abstract": "\nTimed automata are a widely used formalism for modeling real-time\nsystems, which is employed  in a class of successful model checkers\nsuch as UPPAAL [LPY97], HyTech [HHWt97] or Kronos [Yov97].  This work\nformalizes the theory for the subclass of diagonal-free timed\nautomata,  which is sufficient to model many interesting problems.  We\nfirst define the basic concepts and semantics of diagonal-free timed\nautomata.  Based on this, we prove two types of decidability results\nfor the language emptiness problem.    The first is the classic result\nof Alur and Dill [AD90, AD94],  which uses a finite partitioning of\nthe state space into so-called `regions`.    Our second result focuses\non an approach based on `Difference Bound Matrices (DBMs)`,  which is\npractically used by model checkers.  We prove the correctness of the\nbasic forward analysis operations on DBMs.  One of these operations is\nthe Floyd-Warshall algorithm for the all-pairs shortest paths problem.\nTo obtain a finite search space, a widening operation has to be used\nfor this kind of analysis.  We use Patricia Bouyer's [Bou04] approach\nto prove that this widening operation  is correct in the sense that\nDBM-based forward analysis in combination with the widening operation\nalso decides language emptiness. The interesting property of this\nproof is that the first  decidability result is reused to obtain the\nsecond one.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-03-11"
            }
        ],
        "theories": [
            "Floyd_Warshall",
            "Timed_Automata",
            "DBM",
            "Paths_Cycles",
            "DBM_Basics",
            "DBM_Operations",
            "DBM_Zone_Semantics",
            "Misc",
            "DBM_Normalization",
            "Regions_Beta",
            "Regions",
            "Closure",
            "Approx_Beta",
            "Normalized_Zone_Semantics"
        ]
    },
    {
        "session": "Fishburn_Impossibility",
        "title": "The Incompatibility of Fishburn-Strategyproofness and Pareto-Efficiency",
        "authors": [
            "Felix Brandt",
            "Manuel Eberl",
            "Christian Saile",
            "Christian Stricker"
        ],
        "topics": [
            "Mathematics/Games and economics"
        ],
        "date": "2018-03-22",
        "abstract": "\n<p>This formalisation contains the proof that there is no\nanonymous Social Choice Function for at least three agents and\nalternatives that fulfils both Pareto-Efficiency and\nFishburn-Strategyproofness. It was derived from a proof of <a\nhref=\"http://dss.in.tum.de/files/brandt-research/stratset.pdf\">Brandt\n<em>et al.</em></a>, which relies on an unverified\ntranslation of a fixed finite instance of the original problem to SAT.\nThis Isabelle proof contains a machine-checked version of both the\nstatement for exactly three agents and alternatives and the lifting to\nthe general case.</p>",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2018-06-10"
            },
            {
                "2017": "2018-03-23"
            }
        ],
        "dependencies": [
            "Randomised_Social_Choice"
        ],
        "theories": [
            "Social_Choice_Functions",
            "Fishburn_Impossibility"
        ]
    },
    {
        "session": "Decreasing-Diagrams",
        "title": "Decreasing Diagrams",
        "authors": [
            "Harald Zankl"
        ],
        "license": "LGPL",
        "date": "2013-11-01",
        "topics": [
            "Logic/Rewriting"
        ],
        "abstract": "This theory contains a formalization of decreasing diagrams showing that any locally decreasing abstract rewrite system is confluent. We consider the valley (van Oostrom, TCS 1994) and the conversion version (van Oostrom, RTA 2008) and closely follow the original proofs. As an application we prove Newman's lemma.",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-12-02"
            },
            {
                "2013-1": "2013-11-18"
            }
        ],
        "dependencies": [
            "Abstract-Rewriting"
        ],
        "theories": [
            "Decreasing_Diagrams"
        ]
    },
    {
        "session": "Applicative_Lifting",
        "title": "Applicative Lifting",
        "authors": [
            "Andreas Lochbihler",
            "Joshua Schneider"
        ],
        "date": "2015-12-22",
        "topics": [
            "Computer science/Functional programming"
        ],
        "abstract": "Applicative functors augment computations with effects by lifting function application to types which model the effects.  As the structure of the computation cannot depend on the effects, applicative expressions can be analysed statically.  This allows us to lift universally quantified equations to the effectful types, as observed by Hinze. Thus, equational reasoning over effectful computations can be reduced to pure types.\n</p><p>\nThis entry provides a package for registering applicative functors and two proof methods for lifting of equations over applicative functors. The first method normalises applicative expressions according to the laws of applicative functors. This way, equations whose two sides contain the same list of variables can be lifted to every applicative functor.\n</p><p>\nTo lift larger classes of equations, the second method exploits a number of additional properties (e.g., commutativity of effects) provided the properties have been declared for the concrete applicative functor at hand upon registration.\n</p><p>\nWe declare several types from the Isabelle library as applicative functors and illustrate the use of the methods with two examples: the lifting of the arithmetic type class hierarchy to streams and the verification of a relabelling function on binary trees. We also formalise and verify the normalisation algorithm used by the first proof method.\n</p>",
        "extra": {
            "Change history": "[2016-03-03] added formalisation of lifting with combinators<br>\n[2016-06-10]\nimplemented automatic derivation of lifted combinator reductions;\nsupport arbitrary lifted relations using relators;\nimproved compatibility with locale interpretation\n(revision ec336f354f37)<br>"
        },
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-12-22"
            }
        ],
        "theories": [
            "Applicative",
            "files/applicative.ML",
            "Applicative_Environment",
            "Applicative_Option",
            "Applicative_Sum",
            "Applicative_Set",
            "Applicative_List",
            "Applicative_DNEList",
            "Applicative_Monoid",
            "Applicative_Filter",
            "Applicative_State",
            "Applicative_Stream",
            "Applicative_Open_State",
            "Applicative_PMF",
            "Applicative_Probability_List",
            "Applicative_Star",
            "Applicative_Vector",
            "Applicative_Functor",
            "Applicative_Environment_Algebra",
            "Stream_Algebra",
            "Tree_Relabelling",
            "Applicative_Examples",
            "Joinable",
            "Beta_Eta",
            "Combinators",
            "Idiomatic_Terms",
            "Abstract_AF",
            "Applicative_Test"
        ]
    },
    {
        "session": "Efficient-Mergesort",
        "title": "Efficient Mergesort",
        "topics": [
            "Computer science/Algorithms"
        ],
        "date": "2011-11-09",
        "authors": [
            "Christian Sternagel"
        ],
        "abstract": "We provide a formalization of the mergesort algorithm as used in GHC's Data.List module, proving correctness and stability. Furthermore, experimental data suggests that generated (Haskell-)code for this algorithm is much faster than for previous algorithms available in the Isabelle distribution.",
        "extra": {
            "Change history": "[2012-10-24]\nAdded reference to journal article.<br>\n[2018-09-17]\nAdded theory Efficient_Mergesort that works exclusively with the mutual\ninduction schemas generated by the function package.<br>\n[2018-09-19]\nAdded theory Mergesort_Complexity that proves an upper bound on the number of\ncomparisons that are required by mergesort.<br>\n[2018-09-19]\nTheory Efficient_Mergesort replaces theory Efficient_Sort but keeping the old\nname Efficient_Sort.\n[2020-11-20]\nAdditional theory Natural_Mergesort that developes an efficient mergesort\nalgorithm without key-functions for educational purposes."
        },
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-03-02"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-24"
            },
            {
                "2011-1": "2011-11-10"
            }
        ],
        "theories": [
            "Efficient_Sort",
            "Mergesort_Complexity",
            "Natural_Mergesort"
        ]
    },
    {
        "session": "Markov_Models",
        "title": "Markov Models",
        "authors": [
            "Johannes Hölzl",
            "Tobias Nipkow"
        ],
        "date": "2012-01-03",
        "topics": [
            "Mathematics/Probability theory",
            "Computer science/Automata and formal languages"
        ],
        "abstract": "This is a formalization of Markov models in Isabelle/HOL. It\nbuilds on Isabelle's probability theory. The available models are\ncurrently Discrete-Time Markov Chains and a extensions of them with\nrewards.\n<p>\nAs application of these models we formalize probabilistic model\nchecking of pCTL formulas, analysis of IPv4 address allocation in\nZeroConf and an analysis of the anonymity of the Crowds protocol.\n<a href=\"http://arxiv.org/abs/1212.3870\">See here for the corresponding paper.</a>",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-24"
            },
            {
                "2011-1": "2012-01-08"
            },
            {
                "2011-1": "2012-01-05"
            }
        ],
        "dependencies": [
            "Coinductive",
            "Gauss-Jordan-Elim-Fun"
        ],
        "theories": [
            "Markov_Models_Auxiliary",
            "Discrete_Time_Markov_Chain",
            "Trace_Space_Equals_Markov_Processes",
            "Classifying_Markov_Chain_States",
            "Markov_Decision_Process",
            "MDP_Reachability_Problem",
            "Discrete_Time_Markov_Process",
            "Continuous_Time_Markov_Chain",
            "Markov_Models",
            "Example_A",
            "Example_B",
            "PCTL",
            "PGCL",
            "Crowds_Protocol",
            "Zeroconf_Analysis",
            "Gossip_Broadcast",
            "MDP_RP_Certification",
            "MDP_RP"
        ]
    },
    {
        "session": "Projective_Geometry",
        "title": "Projective Geometry",
        "authors": [
            "Anthony Bordg"
        ],
        "topics": [
            "Mathematics/Geometry"
        ],
        "date": "2018-06-14",
        "abstract": "\nWe formalize the basics of projective geometry. In particular, we give\na proof of the so-called Hessenberg's theorem in projective plane\ngeometry. We also provide a proof of the so-called Desargues's\ntheorem based on an axiomatization of (higher) projective space\ngeometry using the notion of rank of a matroid. This last approach\nallows to handle incidence relations in an homogeneous way dealing\nonly with points and without the need of talking explicitly about\nlines, planes or any higher entity.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2018-06-15"
            }
        ],
        "theories": [
            "Projective_Plane_Axioms",
            "Pappus_Property",
            "Pascal_Property",
            "Desargues_Property",
            "Pappus_Desargues",
            "Higher_Projective_Space_Rank_Axioms",
            "Matroid_Rank_Properties",
            "Desargues_2D",
            "Desargues_3D",
            "Projective_Space_Axioms",
            "Higher_Projective_Space_Axioms"
        ]
    },
    {
        "session": "Universal_Turing_Machine",
        "title": "Universal Turing Machine",
        "authors": [
            "Jian Xu",
            "Xingyuan Zhang",
            "Christian Urban",
            "Sebastiaan J. C. Joosten"
        ],
        "topics": [
            "Logic/Computability",
            "Computer science/Automata and formal languages"
        ],
        "date": "2019-02-08",
        "abstract": "\nWe formalise results from computability theory: recursive functions,\nundecidability of the halting problem, and the existence of a\nuniversal Turing machine. This formalisation is the AFP entry\ncorresponding to the paper Mechanising Turing Machines and Computability Theory\nin Isabelle/HOL, ITP 2013.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2019-02-12"
            }
        ],
        "theories": [
            "Turing",
            "Turing_Hoare",
            "Uncomputable",
            "Abacus_Mopup",
            "Abacus",
            "Abacus_Defs",
            "Rec_Def",
            "Abacus_Hoare",
            "Recursive",
            "Recs",
            "UF",
            "UTM"
        ]
    },
    {
        "session": "Well_Quasi_Orders",
        "title": "Well-Quasi-Orders",
        "authors": [
            "Christian Sternagel"
        ],
        "date": "2012-04-13",
        "topics": [
            "Mathematics/Combinatorics"
        ],
        "abstract": "Based on Isabelle/HOL's type class for preorders,\nwe introduce a type class for well-quasi-orders (wqo)\nwhich is characterized by the absence of \"bad\" sequences\n(our proofs are along the lines of the proof of Nash-Williams,\nfrom which we also borrow terminology). Our main results are\ninstantiations for the product type, the list type, and a type of finite trees,\nwhich (almost) directly follow from our proofs of (1) Dickson's Lemma, (2)\nHigman's Lemma, and (3) Kruskal's Tree Theorem. More concretely:\n<ul>\n<li>If the sets A and B are wqo then their Cartesian product is wqo.</li>\n<li>If the set A is wqo then the set of finite lists over A is wqo.</li>\n<li>If the set A is wqo then the set of finite trees over A is wqo.</li>\n</ul>\nThe research was funded by the Austrian Science Fund (FWF): J3202.",
        "extra": {
            "Change history": "[2012-06-11] Added Kruskal's Tree Theorem.<br>\n[2012-12-19] New variant of Kruskal's tree theorem for terms (as opposed to\nvariadic terms, i.e., trees), plus finite version of the tree theorem as\ncorollary.<br>\n[2013-05-16] Simplified construction of minimal bad sequences.<br>\n[2014-07-09] Simplified proofs of Higman's lemma and Kruskal's tree theorem,\nbased on homogeneous sequences.<br>\n[2016-01-03] An alternative proof of Higman's lemma by open induction.<br>\n[2017-06-08] Proved (classical) equivalence to inductive definition of\nalmost-full relations according to the ITP 2012 paper \"Stop When You Are\nAlmost-Full\" by Vytiniotis, Coquand, and Wahlstedt."
        },
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-24"
            }
        ],
        "dependencies": [
            "Abstract-Rewriting",
            "Open_Induction"
        ],
        "theories": [
            "Infinite_Sequences",
            "Minimal_Elements",
            "Least_Enum",
            "Almost_Full",
            "Minimal_Bad_Sequences",
            "Higman_OI",
            "Almost_Full_Relations",
            "Well_Quasi_Orders",
            "Kruskal",
            "Kruskal_Examples",
            "Wqo_Instances",
            "Multiset_Extension",
            "Wqo_Multiset"
        ]
    },
    {
        "session": "Imperative_Insertion_Sort",
        "title": "Imperative Insertion Sort",
        "authors": [
            "Christian Sternagel"
        ],
        "date": "2014-09-25",
        "topics": [
            "Computer science/Algorithms"
        ],
        "abstract": "The insertion sort algorithm of Cormen et al. (Introduction to Algorithms) is expressed in Imperative HOL and proved to be correct and terminating. For this purpose we also provide a theory about imperative loop constructs with accompanying induction/invariant rules for proving partial and total correctness. Furthermore, the formalized algorithm is fit for code generation.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-09-25"
            }
        ],
        "theories": [
            "Imperative_Loops",
            "Imperative_Insertion_Sort"
        ]
    },
    {
        "session": "Real_Impl",
        "title": "Implementing field extensions of the form Q[sqrt(b)]",
        "authors": [
            "René Thiemann"
        ],
        "date": "2014-02-06",
        "license": "LGPL",
        "topics": [
            "Mathematics/Analysis"
        ],
        "abstract": "\nWe apply data refinement to implement the real numbers, where we support all\nnumbers in the field extension Q[sqrt(b)], i.e., all numbers of the form p +\nq * sqrt(b) for rational numbers p and q and some fixed natural number b. To\nthis end, we also developed algorithms to precisely compute roots of a\nrational number, and to perform a factorization of natural numbers which\neliminates duplicate prime factors.\n<p>\nOur results have been used to certify termination proofs which involve\npolynomial interpretations over the reals.",
        "extra": {
            "Change history": "[2014-07-11] Moved NthRoot_Impl to Sqrt-Babylonian."
        },
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2014-02-11"
            }
        ],
        "dependencies": [
            "Sqrt_Babylonian",
            "Show",
            "Deriving"
        ],
        "theories": [
            "Real_Impl_Auxiliary",
            "Prime_Product",
            "Real_Impl",
            "Real_Unique_Impl"
        ]
    },
    {
        "session": "QHLProver",
        "title": "Quantum Hoare Logic",
        "authors": [
            "Junyi Liu",
            "Bohua Zhan",
            "Shuling Wang",
            "Shenggang Ying",
            "Tao Liu",
            "Yangjia Li",
            "Mingsheng Ying",
            "Naijun Zhan"
        ],
        "topics": [
            "Computer science/Programming languages/Logics",
            "Computer science/Semantics"
        ],
        "date": "2019-03-24",
        "abstract": "\nWe formalize quantum Hoare logic as given in [1]. In particular, we\nspecify the syntax and denotational semantics of a simple model of\nquantum programs. Then, we write down the rules of quantum Hoare logic\nfor partial correctness, and show the soundness and completeness of\nthe resulting proof system. As an application, we verify the\ncorrectness of Grover’s algorithm.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2019-03-25"
            }
        ],
        "dependencies": [
            "Jordan_Normal_Form",
            "Deep_Learning"
        ],
        "theories": [
            "Complex_Matrix",
            "files/mat_alg.ML",
            "Matrix_Limit",
            "Quantum_Program",
            "Partial_State",
            "Gates",
            "Quantum_Hoare",
            "Grover"
        ]
    },
    {
        "session": "Jordan_Normal_Form",
        "title": "Matrices, Jordan Normal Forms, and Spectral Radius Theory",
        "topics": [
            "Mathematics/Algebra"
        ],
        "authors": [
            "René Thiemann",
            "Akihisa Yamada"
        ],
        "contributors": [
            "Alexander Bentkamp"
        ],
        "date": "2015-08-21",
        "abstract": "\n<p>\nMatrix interpretations are useful as measure functions in termination proving. In order to use these interpretations also for complexity analysis, the growth rate of matrix powers has to examined. Here, we formalized a central result of spectral radius theory, namely that the growth rate is polynomially bounded if and only if the spectral radius of a matrix is at most one.\n</p><p>\nTo formally prove this result we first studied the growth rates of matrices in Jordan normal form, and prove the result that every complex matrix has a Jordan normal form using a constructive prove via Schur decomposition.\n</p><p>\nThe whole development is based on a new abstract type for matrices, which is also executable by a suitable setup of the code generator. It completely subsumes our former AFP-entry on executable matrices, and its main advantage is its close connection to the HMA-representation which allowed us to easily adapt existing proofs on determinants.\n</p><p>\nAll the results have been applied to improve CeTA, our certifier to validate termination and complexity proof certificates.\n</p>",
        "extra": {
            "Change history": "[2016-01-07] Added Schur-decomposition, Gram-Schmidt orthogonalization, uniqueness of Jordan normal forms<br/>\n[2018-04-17] Integrated lemmas from deep-learning AFP-entry of Alexander Bentkamp"
        },
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-08-23"
            }
        ],
        "dependencies": [
            "JNF-AFP-Lib",
            "Polynomial_Factorization"
        ],
        "theories": [
            "Missing_Ring",
            "Missing_Permutations",
            "Conjugate",
            "Matrix",
            "Matrix_IArray_Impl",
            "Gauss_Jordan_Elimination",
            "Gauss_Jordan_IArray_Impl",
            "Column_Operations",
            "Determinant",
            "Determinant_Impl",
            "Show_Matrix",
            "Char_Poly",
            "Jordan_Normal_Form",
            "Missing_VectorSpace",
            "VS_Connect",
            "Gram_Schmidt",
            "Schur_Decomposition",
            "Jordan_Normal_Form_Existence",
            "Matrix_Impl",
            "Strassen_Algorithm",
            "Strassen_Algorithm_Code",
            "Matrix_Comparison",
            "Ring_Hom_Matrix",
            "Derivation_Bound",
            "Complexity_Carrier",
            "Show_Arctic",
            "Matrix_Complexity",
            "Matrix_Kernel",
            "Jordan_Normal_Form_Uniqueness",
            "Spectral_Radius",
            "DL_Missing_List",
            "DL_Rank",
            "DL_Missing_Sublist",
            "DL_Submatrix",
            "DL_Rank_Submatrix"
        ]
    },
    {
        "session": "Pairing_Heap",
        "title": "Pairing Heap",
        "authors": [
            "Hauke Brinkop",
            "Tobias Nipkow"
        ],
        "date": "2016-07-14",
        "topics": [
            "Computer science/Data structures"
        ],
        "abstract": "\nThis library defines three different versions of pairing heaps: a\nfunctional version of the original design based on binary\ntrees [Fredman et al. 1986], the version by Okasaki [1998] and\na modified version of the latter that is free of structural invariants.\n<p>\nThe amortized complexity of pairing heaps is analyzed in the AFP article\n<a href=\"http://isa-afp.org/entries/Amortized_Complexity.html\">Amortized Complexity</a>.",
        "extra": {
            "Origin": "This library was extracted from Amortized Complexity and extended."
        },
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            }
        ],
        "theories": [
            "Pairing_Heap_Tree",
            "Pairing_Heap_List1",
            "Pairing_Heap_List2"
        ]
    },
    {
        "session": "UPF",
        "title": "The Unified Policy Framework (UPF)",
        "authors": [
            "Achim D. Brucker",
            "Lukas Brügger",
            "Burkhart Wolff"
        ],
        "date": "2014-11-28",
        "topics": [
            "Computer science/Security"
        ],
        "abstract": "\nWe present the Unified Policy Framework (UPF), a generic framework\nfor modelling security (access-control) policies. UPF emphasizes\nthe view that a policy is a policy decision function that grants or\ndenies access to resources, permissions, etc. In other words,\ninstead of modelling the relations of permitted or prohibited\nrequests directly, we model the concrete function that implements\nthe policy decision point in a system.  In more detail, UPF is\nbased on the following four principles: 1) Functional representation\nof policies, 2) No conflicts are possible, 3) Three-valued decision\ntype (allow, deny, undefined), 4) Output type not containing the\ndecision only.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2015-01-28"
            },
            {
                "2014": "2014-11-30"
            }
        ],
        "theories": [
            "Monads",
            "UPFCore",
            "ElementaryPolicies",
            "SeqComposition",
            "ParallelComposition",
            "Normalisation",
            "NormalisationTestSpecification",
            "Analysis",
            "UPF",
            "Service",
            "ServiceExample"
        ]
    },
    {
        "session": "SATSolverVerification",
        "title": "Formal Verification of Modern SAT Solvers",
        "authors": [
            "Filip Marić"
        ],
        "date": "2008-07-23",
        "topics": [
            "Computer science/Algorithms"
        ],
        "abstract": "This document contains formal correctness proofs of modern SAT solvers. Following (Krstic et al, 2007) and (Nieuwenhuis et al., 2006), solvers are described using state-transition systems. Several different SAT solver descriptions are given and their partial correctness and termination is proved. These include: <ul> <li> a solver based on classical DPLL procedure (using only a backtrack-search with unit propagation),</li> <li> a very general solver with backjumping and learning (similar to the description given in (Nieuwenhuis et al., 2006)), and</li> <li> a solver with a specific conflict analysis algorithm (similar to the description given in (Krstic et al., 2007)).</li> </ul> Within the SAT solver correctness proofs, a large number of lemmas about propositional logic and CNF formulae are proved. This theory is self-contained and could be used for further exploring of properties of CNF based SAT algorithms.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-03-02"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-24"
            },
            {
                "2011-1": "2011-10-11"
            },
            {
                "2011": "2011-02-11"
            },
            {
                "2009-2": "2010-07-01"
            },
            {
                "2009-1": "2009-12-12"
            },
            {
                "2009": "2009-04-29"
            },
            {
                "2008": "2008-07-27"
            }
        ],
        "theories": [
            "MoreList",
            "CNF",
            "Trail",
            "SatSolverVerification",
            "BasicDPLL",
            "NieuwenhuisOliverasTinelli",
            "KrsticGoel",
            "SatSolverCode",
            "AssertLiteral",
            "UnitPropagate",
            "Initialization",
            "ConflictAnalysis",
            "Decide",
            "SolveLoop",
            "FunctionalImplementation"
        ]
    },
    {
        "session": "Dirichlet_L",
        "title": "Dirichlet L-Functions and Dirichlet's Theorem",
        "authors": [
            "Manuel Eberl"
        ],
        "topics": [
            "Mathematics/Number theory",
            "Mathematics/Algebra"
        ],
        "date": "2017-12-21",
        "abstract": "\n<p>This article provides a formalisation of Dirichlet characters\nand Dirichlet <em>L</em>-functions including proofs of\ntheir basic properties &ndash; most notably their analyticity,\ntheir areas of convergence, and their non-vanishing for &Re;(s)\n&ge; 1. All of this is built in a very high-level style using\nDirichlet series. The proof of the non-vanishing follows a very short\nand elegant proof by Newman, which we attempt to reproduce faithfully\nin a similar level of abstraction in Isabelle.</p> <p>This\nalso leads to a relatively short proof of Dirichlet’s Theorem, which\nstates that, if <em>h</em> and <em>n</em> are\ncoprime, there are infinitely many primes <em>p</em> with\n<em>p</em> &equiv; <em>h</em> (mod\n<em>n</em>).</p>",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            }
        ],
        "dependencies": [
            "Zeta_Function",
            "Landau_Symbols",
            "Dirichlet_Series",
            "Bertrands_Postulate"
        ],
        "theories": [
            "Group_Adjoin",
            "Multiplicative_Characters",
            "Dirichlet_Characters",
            "Dirichlet_L_Functions",
            "Dirichlet_Theorem"
        ]
    },
    {
        "session": "Kleene_Algebra",
        "title": "Kleene Algebra",
        "authors": [
            "Alasdair Armstrong",
            "Georg Struth",
            "Tjark Weber"
        ],
        "date": "2013-01-15",
        "topics": [
            "Computer science/Programming languages/Logics",
            "Computer science/Automata and formal languages",
            "Mathematics/Algebra"
        ],
        "abstract": "\nThese files contain a formalisation of variants of Kleene algebras and\ntheir most important models as axiomatic type classes in Isabelle/HOL.\nKleene algebras are foundational structures in computing with\napplications ranging from automata and language theory to computational\nmodeling, program construction and verification.\n<p>\nWe start with formalising dioids, which are additively idempotent\nsemirings, and expand them by axiomatisations of the Kleene star for\nfinite iteration and an omega operation for infinite iteration. We\nshow that powersets over a given monoid, (regular) languages, sets of\npaths in a graph, sets of computation traces, binary relations and\nformal power series form Kleene algebras, and consider further models\nbased on lattices, max-plus semirings and min-plus semirings. We also\ndemonstrate that dioids are closed under the formation of matrices\n(proofs for Kleene algebras remain to be completed).\n<p>\nOn the one hand we have aimed at a reference formalisation of variants\nof Kleene algebras that covers a wide range of variants and the core\ntheorems in a structured and modular way and provides readable proofs\nat text book level. On the other hand, we intend to use this algebraic\nhierarchy and its models as a generic algebraic middle-layer from which\nprogramming applications can quickly be explored, implemented and verified.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-03-02"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2013-01-16"
            }
        ],
        "theories": [
            "Signatures",
            "Dioid",
            "Dioid_Models",
            "Matrix",
            "Conway",
            "Kleene_Algebra",
            "Kleene_Algebra_Models",
            "Omega_Algebra",
            "Omega_Algebra_Models",
            "DRA",
            "PHL_KA",
            "PHL_DRA",
            "Finite_Suprema",
            "Formal_Power_Series",
            "Inf_Matrix"
        ]
    },
    {
        "session": "Bertrands_Postulate",
        "title": "Bertrand's postulate",
        "authors": [
            "Julian Biendarra",
            "Manuel Eberl"
        ],
        "contributors": [
            "Lawrence C. Paulson"
        ],
        "topics": [
            "Mathematics/Number theory"
        ],
        "date": "2017-01-17",
        "abstract": "\n<p>Bertrand's postulate is an early result on the\ndistribution of prime numbers: For every positive integer n, there\nexists a prime number that lies strictly between n and 2n.\nThe proof is ported from John Harrison's formalisation\nin HOL Light. It proceeds by first showing that the property is true\nfor all n greater than or equal to 600 and then showing that it also\nholds for all n below 600 by case distinction. </p>",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2017-01-18"
            }
        ],
        "dependencies": [
            "Pratt_Certificate"
        ],
        "theories": [
            "Bertrand",
            "files/bertrand.ML"
        ]
    },
    {
        "session": "BenOr_Kozen_Reif",
        "title": "The BKR Decision Procedure for Univariate Real Arithmetic",
        "authors": [
            "Katherine Cordwell",
            "Yong Kiam Tan",
            "André Platzer"
        ],
        "topics": [
            "Computer science/Algorithms/Mathematical"
        ],
        "date": "2021-04-24",
        "abstract": "\nWe formalize the univariate case of Ben-Or, Kozen, and Reif's\ndecision procedure for first-order real arithmetic (the BKR\nalgorithm). We also formalize the univariate case of Renegar's\nvariation of the BKR algorithm. The two formalizations differ\nmathematically in minor ways (that have significant impact on the\nmultivariate case), but are quite similar in proof structure.  Both\nrely on sign-determination (finding the set of consistent sign\nassignments for a set of polynomials).  The method used for\nsign-determination is similar to Tarski's original quantifier\nelimination algorithm (it stores key information in a matrix\nequation), but with a reduction step to keep complexity low.",
        "licence": "BSD",
        "dependencies": [
            "Algebraic_Numbers",
            "Sturm_Tarski"
        ],
        "theories": [
            "More_Matrix",
            "BKR_Algorithm",
            "Matrix_Equation_Construction",
            "BKR_Proofs",
            "BKR_Decision",
            "Renegar_Algorithm",
            "Renegar_Proofs",
            "Renegar_Decision"
        ]
    },
    {
        "session": "TESL_Language",
        "title": "A Formal Development of a Polychronous Polytimed Coordination Language",
        "authors": [
            "Hai Nguyen Van",
            "Frédéric Boulanger",
            "Burkhart Wolff"
        ],
        "topics": [
            "Computer science/System description languages",
            "Computer science/Semantics",
            "Computer science/Concurrency"
        ],
        "date": "2019-07-30",
        "abstract": "\nThe design of complex systems involves different formalisms for\nmodeling their different parts or aspects. The global model of a\nsystem may therefore consist of a coordination of concurrent\nsub-models that use different paradigms.  We develop here a theory for\na language used to specify the timed coordination of such\nheterogeneous subsystems by addressing the following issues:\n<ul><li>the\nbehavior of the sub-systems is observed only at a series of discrete\ninstants,</li><li>events may occur in different sub-systems at unrelated\ntimes, leading to polychronous systems, which do not necessarily have\na common base clock,</li><li>coordination between subsystems involves\ncausality, so the occurrence of an event may enforce the occurrence of\nother events, possibly after a certain duration has elapsed or an\nevent has occurred a given number of times,</li><li>the domain of time\n(discrete, rational, continuous...) may be different in the\nsubsystems, leading to polytimed systems,</li><li>the time frames of\ndifferent sub-systems may be related (for instance, time in a GPS\nsatellite and in a GPS receiver on Earth are related although they are\nnot the same).</li></ul>\nFirstly, a denotational semantics of the language is\ndefined. Then, in order to be able to incrementally check the behavior\nof systems, an operational semantics is given, with proofs of\nprogress, soundness and completeness with regard to the denotational\nsemantics. These proofs are made according to a setup that can scale\nup when new operators are added to the language. In order for\nspecifications to be composed in a clean way, the language should be\ninvariant by stuttering (i.e., adding observation instants at which\nnothing happens). The proof of this invariance is also given.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-07-31"
            }
        ],
        "theories": [
            "Introduction",
            "TESL",
            "Run",
            "Denotational",
            "SymbolicPrimitive",
            "Operational",
            "Corecursive_Prop",
            "Hygge_Theory",
            "StutteringDefs",
            "StutteringLemmas",
            "Stuttering",
            "Config_Morphisms"
        ]
    },
    {
        "session": "LLL_Basis_Reduction",
        "title": "A verified LLL algorithm",
        "authors": [
            "Ralph Bottesch",
            "Jose Divasón",
            "Maximilian P. L. Haslbeck",
            "Sebastiaan J. C. Joosten",
            "René Thiemann",
            "Akihisa Yamada"
        ],
        "topics": [
            "Computer science/Algorithms/Mathematical",
            "Mathematics/Algebra"
        ],
        "date": "2018-02-02",
        "abstract": "\nThe Lenstra-Lenstra-Lovász basis reduction algorithm, also known as\nLLL algorithm, is an algorithm to find a basis with short, nearly\northogonal vectors of an integer lattice. Thereby, it can also be seen\nas an approximation to solve the shortest vector problem (SVP), which\nis an NP-hard problem, where the approximation quality solely depends\non the dimension of the lattice, but not the lattice itself. The\nalgorithm also possesses many applications in diverse fields of\ncomputer science, from cryptanalysis to number theory, but it is\nspecially well-known since it was used to implement the first\npolynomial-time algorithm to factor polynomials. In this work we\npresent the first mechanized soundness proof of the LLL algorithm to\ncompute short vectors in lattices. The formalization follows a\ntextbook by von zur Gathen and Gerhard.",
        "extra": {
            "Change history": "[2018-04-16] Integrated formal complexity bounds (Haslbeck, Thiemann)\n[2018-05-25] Integrated much faster LLL implementation based on integer arithmetic (Bottesch, Haslbeck, Thiemann)"
        },
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-09-07"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2018-02-03"
            }
        ],
        "dependencies": [
            "Berlekamp_Zassenhaus",
            "Algebraic_Numbers"
        ],
        "theories": [
            "Missing_Lemmas",
            "More_IArray",
            "Norms",
            "Int_Rat_Operations",
            "Cost",
            "List_Representation",
            "Gram_Schmidt_2",
            "Gram_Schmidt_Int",
            "LLL",
            "LLL_Impl",
            "LLL_Complexity",
            "LLL_Number_Bounds",
            "LLL_Certification",
            "FPLLL_Solver"
        ]
    },
    {
        "session": "Decl_Sem_Fun_PL",
        "title": "Declarative Semantics for Functional Languages",
        "authors": [
            "Jeremy Siek"
        ],
        "topics": [
            "Computer science/Programming languages"
        ],
        "date": "2017-07-21",
        "abstract": "\nWe present a semantics for an applied call-by-value lambda-calculus\nthat is compositional, extensional, and elementary. We present four\ndifferent views of the semantics: 1) as a relational (big-step)\nsemantics that is not operational but instead declarative, 2) as a\ndenotational semantics that does not use domain theory, 3) as a\nnon-deterministic interpreter, and 4) as a variant of the intersection\ntype systems of the Torino group.  We prove that the semantics is\ncorrect by showing that it is sound and complete with respect to\noperational semantics on programs and that is sound with respect to\ncontextual equivalence. We have not yet investigated whether it is\nfully abstract. We demonstrate that this approach to semantics is\nuseful with three case studies. First, we use the semantics to prove\ncorrectness of a compiler optimization that inlines function\napplication. Second, we adapt the semantics to the polymorphic\nlambda-calculus extended with general recursion and prove semantic\ntype soundness.  Third, we adapt the semantics to the call-by-value\nlambda-calculus with mutable references.\n<br>\nThe paper that accompanies these Isabelle theories is <a href=\"https://arxiv.org/abs/1707.03762\">available on arXiv</a>.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2017-07-24"
            }
        ],
        "theories": [
            "Lambda",
            "SmallStepLam",
            "BigStepLam",
            "ValuesFSet",
            "ValuesFSetProps",
            "RelationalSemFSet",
            "DeclSemAsDenotFSet",
            "EquivRelationalDenotFSet",
            "ChangeEnv",
            "DeclSemAsNDInterpFSet",
            "InterTypeSystem",
            "Values",
            "ValueProps",
            "DeclSemAsDenot",
            "DenotLam5",
            "EquivDenotInterTypes",
            "DenotSoundFSet",
            "DenotCompleteFSet",
            "DenotCongruenceFSet",
            "DenotEqualitiesFSet",
            "Optimizer",
            "SystemF",
            "MutableRef",
            "MutableRefProps"
        ]
    },
    {
        "session": "IsaGeoCoq",
        "title": "Tarski's Parallel Postulate implies the 5th Postulate of Euclid, the Postulate of Playfair and the original Parallel Postulate of Euclid",
        "authors": [
            "Roland Coghetto"
        ],
        "topics": [
            "Mathematics/Geometry"
        ],
        "license": "LGPL",
        "date": "2021-01-31",
        "abstract": "\n<p>The <a href=\"https://geocoq.github.io/GeoCoq/\">GeoCoq library</a>  contains a formalization\nof geometry using the Coq proof assistant. It contains both proofs\nabout the foundations of geometry and high-level proofs in the same\nstyle as in high school.  We port a part of the GeoCoq\n2.4.0 library to Isabelle/HOL: more precisely,\nthe files Chap02.v to Chap13_3.v, suma.v as well as the associated\ndefinitions and some useful files for the demonstration of certain\nparallel postulates. The synthetic approach of the demonstrations is directly\ninspired by those contained in GeoCoq. The names of the lemmas and\ntheorems used are kept as far as possible as well as the definitions.\n</p>\n<p>It should be noted that T.J.M. Makarios has done\n<a href=\"https://www.isa-afp.org/entries/Tarskis_Geometry.html\">some proofs in Tarski's Geometry</a>. It uses a definition that does not quite\ncoincide with the definition used in Geocoq and here.\nFurthermore, corresponding definitions in the <a href=\"https://www.isa-afp.org/entries/Poincare_Disc.html\">Poincaré Disc Model\ndevelopment</a> are not identical to those defined in GeoCoq.\n</p>\n<p>In the last part, it is\nformalized that, in the neutral/absolute space, the axiom of the\nparallels of Tarski's system implies the Playfair axiom, the 5th\npostulate of Euclid and Euclid's original parallel postulate. These\nproofs, which are not constructive, are directly inspired by Pierre\nBoutry, Charly Gries, Julien Narboux and Pascal Schreck.\n</p>",
        "theories": [
            "Tarski_Neutral"
        ]
    },
    {
        "session": "Probabilistic_Prime_Tests",
        "title": "Probabilistic Primality Testing",
        "authors": [
            "Daniel Stüwe",
            "Manuel Eberl"
        ],
        "topics": [
            "Mathematics/Number theory"
        ],
        "date": "2019-02-11",
        "abstract": "\n<p>The most efficient known primality tests are\n<em>probabilistic</em> in the sense that they use\nrandomness and may, with some probability, mistakenly classify a\ncomposite number as prime &ndash; but never a prime number as\ncomposite. Examples of this are the Miller&ndash;Rabin test, the\nSolovay&ndash;Strassen test, and (in most cases) Fermat's\ntest.</p> <p>This entry defines these three tests and\nproves their correctness. It also develops some of the\nnumber-theoretic foundations, such as Carmichael numbers and the\nJacobi symbol with an efficient executable algorithm to compute\nit.</p>",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2019-02-15"
            }
        ],
        "theories": [
            "Legendre_Symbol",
            "Algebraic_Auxiliaries",
            "Jacobi_Symbol",
            "Residues_Nat",
            "QuadRes",
            "Euler_Witness",
            "Carmichael_Numbers",
            "Fermat_Witness",
            "Generalized_Primality_Test",
            "Fermat_Test",
            "Miller_Rabin_Test",
            "Solovay_Strassen_Test"
        ]
    },
    {
        "session": "Constructive_Cryptography",
        "title": "Constructive Cryptography in HOL",
        "authors": [
            "Andreas Lochbihler",
            "S. Reza Sefidgar"
        ],
        "topics": [
            "Computer science/Security/Cryptography",
            "Mathematics/Probability theory"
        ],
        "date": "2018-12-17",
        "abstract": "\nInspired by Abstract Cryptography, we extend CryptHOL, a framework for\nformalizing game-based proofs, with an abstract model of Random\nSystems and provide proof rules about their composition and equality.\nThis foundation facilitates the formalization of Constructive\nCryptography proofs, where the security of a cryptographic scheme is\nrealized as a special form of construction in which a complex random\nsystem is built from simpler ones. This is a first step towards a\nfully-featured compositional framework, similar to Universal\nComposability framework, that supports formalization of\nsimulation-based proofs.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-12-20"
            },
            {
                "2018": "2018-12-19"
            }
        ],
        "dependencies": [
            "CryptHOL"
        ],
        "theories": [
            "Resource",
            "Converter",
            "Converter_Rewrite",
            "Random_System",
            "Distinguisher",
            "Wiring",
            "Constructive_Cryptography",
            "System_Construction",
            "One_Time_Pad",
            "Message_Authentication_Code",
            "Secure_Channel",
            "Examples"
        ]
    },
    {
        "session": "Bicategory",
        "title": "Bicategories",
        "authors": [
            "Eugene W. Stark"
        ],
        "topics": [
            "Mathematics/Category theory"
        ],
        "date": "2020-01-06",
        "abstract": "\n<p>\nTaking as a starting point the author's previous work on\ndeveloping aspects of category theory in Isabelle/HOL, this article\ngives a compatible formalization of the notion of\n\"bicategory\" and develops a framework within which formal\nproofs of facts about bicategories can be given.  The framework\nincludes a number of basic results, including the Coherence Theorem,\nthe Strictness Theorem, pseudofunctors and biequivalence, and facts\nabout internal equivalences and adjunctions in a bicategory.  As a\ndriving application and demonstration of the utility of the framework,\nit is used to give a formal proof of a theorem, due to Carboni,\nKasangian, and Street, that characterizes up to biequivalence the\nbicategories of spans in a category with pullbacks.  The formalization\neffort necessitated the filling-in of many details that were not\nevident from the brief presentation in the original paper, as well as\nidentifying a few minor corrections along the way.\n</p><p>\nRevisions made subsequent to the first version of this article added\nadditional material on pseudofunctors, pseudonatural transformations,\nmodifications, and equivalence of bicategories; the main thrust being\nto give a proof that a pseudofunctor is a biequivalence if and only\nif it can be extended to an equivalence of bicategories.\n</p>",
        "extra": {
            "Change history": "[2020-02-15]\nMove ConcreteCategory.thy from Bicategory to Category3 and use it systematically.\nMake other minor improvements throughout.\n(revision a51840d36867)<br>\n[2020-11-04]\nAdded new material on equivalence of bicategories, with associated changes.\n(revision 472cb2268826)<br>"
        },
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2020-01-09"
            }
        ],
        "dependencies": [
            "MonoidalCategory"
        ],
        "theories": [
            "Strictness",
            "BicategoryOfSpans",
            "CanonicalIsos",
            "Prebicategory",
            "Bicategory",
            "EquivalenceOfBicategories",
            "Modification",
            "Pseudofunctor",
            "Tabulation",
            "IsomorphismClass",
            "InternalEquivalence",
            "PseudonaturalTransformation",
            "SpanBicategory",
            "Coherence",
            "InternalAdjunction",
            "Subbicategory"
        ]
    },
    {
        "session": "EdmondsKarp_Maxflow",
        "title": "Formalizing the Edmonds-Karp Algorithm",
        "authors": [
            "Peter Lammich",
            "S. Reza Sefidgar"
        ],
        "date": "2016-08-12",
        "topics": [
            "Computer science/Algorithms/Graph"
        ],
        "abstract": "\nWe present a formalization of the Ford-Fulkerson method for computing\nthe maximum flow in a network. Our formal proof closely follows a\nstandard textbook proof, and is accessible even without being an\nexpert in Isabelle/HOL--- the interactive theorem prover used for the\nformalization. We then use stepwise refinement to obtain the\nEdmonds-Karp algorithm, and formally prove a bound on its complexity.\nFurther refinement yields a verified implementation, whose execution\ntime compares well to an unverified reference implementation in Java.\nThis entry is based on our ITP-2016 paper with the same title.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-08-12"
            }
        ],
        "dependencies": [
            "Flow_Networks"
        ],
        "theories": [
            "FordFulkerson_Algo",
            "EdmondsKarp_Termination_Abstract",
            "EdmondsKarp_Algo",
            "Augmenting_Path_BFS",
            "EdmondsKarp_Impl",
            "Edka_Checked_Impl",
            "Edka_Benchmark_Export"
        ]
    },
    {
        "session": "HOLCF-Prelude",
        "title": "HOLCF-Prelude",
        "authors": [
            "Joachim Breitner",
            "Brian Huffman",
            "Neil Mitchell",
            "Christian Sternagel"
        ],
        "topics": [
            "Computer science/Functional programming"
        ],
        "date": "2017-07-15",
        "abstract": "\nThe Isabelle/HOLCF-Prelude is a formalization of a large part of\nHaskell's standard prelude in Isabelle/HOLCF. We use it to prove\nthe correctness of the Eratosthenes' Sieve, in its\nself-referential implementation commonly used to showcase\nHaskell's laziness; prove correctness of GHC's\n\"fold/build\" rule and related rewrite rules; and certify a\nnumber of hints suggested by HLint.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2017-07-15"
            }
        ],
        "theories": [
            "HOLCF_Main",
            "Type_Classes",
            "Numeral_Cpo",
            "Data_Function",
            "Data_Bool",
            "Data_Tuple",
            "Data_Integer",
            "Data_List",
            "Data_Maybe",
            "Definedness",
            "List_Comprehension",
            "Num_Class",
            "HOLCF_Prelude",
            "Fibs",
            "Sieve_Primes",
            "GHC_Rewrite_Rules",
            "HLint"
        ]
    },
    {
        "session": "Extended_Finite_State_Machines",
        "title": "A Formal Model of Extended Finite State Machines",
        "authors": [
            "Michael Foster",
            "Achim D. Brucker",
            "Ramsay G. Taylor",
            "John Derrick"
        ],
        "topics": [
            "Computer science/Automata and formal languages"
        ],
        "date": "2020-09-07",
        "abstract": "\nIn this AFP entry, we provide a formalisation of extended finite state\nmachines (EFSMs) where models are represented as finite sets of\ntransitions between states. EFSMs execute traces to produce observable\noutputs. We also define various simulation and equality metrics for\nEFSMs in terms of traces and prove their strengths in relation to each\nother. Another key contribution is a framework of function definitions\nsuch that LTL properties can be phrased over EFSMs. Finally, we\nprovide a simple example case study in the form of a drinks machine.",
        "licence": "BSD",
        "theories": [
            "Trilean",
            "Value",
            "VName",
            "Value_Lexorder",
            "AExp",
            "AExp_Lexorder",
            "GExp",
            "GExp_Lexorder",
            "FSet_Utils",
            "Transition",
            "Transition_Lexorder",
            "EFSM",
            "EFSM_LTL",
            "Drinks_Machine",
            "Drinks_Machine_2",
            "Drinks_Machine_LTL"
        ]
    },
    {
        "session": "Name_Carrying_Type_Inference",
        "title": "Verified Metatheory and Type Inference for a Name-Carrying Simply-Typed Lambda Calculus",
        "authors": [
            "Michael Rawson"
        ],
        "topics": [
            "Computer science/Programming languages/Type systems"
        ],
        "date": "2017-07-09",
        "abstract": "\nI formalise a Church-style simply-typed\n\\(\\lambda\\)-calculus, extended with pairs, a unit value, and\nprojection functions, and show some metatheory of the calculus, such\nas the subject reduction property. Particular attention is paid to the\ntreatment of names in the calculus. A nominal style of binding is\nused, but I use a manual approach over Nominal Isabelle in order to\nextract an executable type inference algorithm. More information can\nbe found in my <a\nhref=\"http://www.openthesis.org/documents/Verified-Metatheory-Type-Inference-Simply-603182.html\">undergraduate\ndissertation</a>.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2017-07-15"
            }
        ],
        "theories": [
            "Fresh",
            "Permutation",
            "PreSimplyTyped",
            "SimplyTyped"
        ]
    },
    {
        "session": "Tree-Automata",
        "title": "Tree Automata",
        "authors": [
            "Peter Lammich"
        ],
        "date": "2009-11-25",
        "topics": [
            "Computer science/Automata and formal languages"
        ],
        "abstract": "This work presents a machine-checked tree automata library for Standard-ML, OCaml and Haskell. The algorithms are efficient by using appropriate data structures like RB-trees. The available algorithms for non-deterministic automata include membership query, reduction, intersection, union, and emptiness check with computation of a witness for non-emptiness. The executable algorithms are derived from less-concrete, non-executable algorithms using data-refinement techniques. The concrete data structures are from the Isabelle Collections Framework. Moreover, this work contains a formalization of the class of tree-regular languages and its closure properties under set operations.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-03-02"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-24"
            },
            {
                "2011-1": "2012-03-15"
            },
            {
                "2011-1": "2011-10-12"
            },
            {
                "2011-1": "2011-10-11"
            },
            {
                "2011": "2011-02-11"
            },
            {
                "2009-2": "2010-07-01"
            },
            {
                "2009-1": "2009-12-13"
            },
            {
                "2009-1": "2009-12-12"
            },
            {
                "2009": "2009-11-29"
            }
        ],
        "dependencies": [
            "Collections",
            "Collections_Examples"
        ],
        "theories": [
            "Tree",
            "Ta",
            "AbsAlgo",
            "Ta_impl",
            "Ta_impl_codegen"
        ]
    },
    {
        "session": "LOFT",
        "title": "LOFT — Verified Migration of Linux Firewalls to SDN",
        "authors": [
            "Julius Michaelis",
            "Cornelius Diekmann"
        ],
        "date": "2016-10-21",
        "topics": [
            "Computer science/Networks"
        ],
        "abstract": "\nWe present LOFT — Linux firewall OpenFlow Translator, a system that\ntransforms the main routing table and FORWARD chain of iptables of a\nLinux-based firewall into a set of static OpenFlow rules. Our\nimplementation is verified against a model of a simplified Linux-based\nrouter and we can directly show how much of the original functionality\nis preserved.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-10-21"
            }
        ],
        "dependencies": [
            "Iptables_Semantics"
        ],
        "theories": [
            "OpenFlow_Helpers",
            "Sort_Descending",
            "List_Group",
            "OpenFlow_Matches",
            "OpenFlow_Action",
            "Semantics_OpenFlow",
            "OpenFlow_Serialize",
            "Featherweight_OpenFlow_Comparison",
            "LinuxRouter_OpenFlow_Translation",
            "OF_conv_test",
            "RFC2544",
            "OpenFlow_Documentation"
        ]
    },
    {
        "session": "GaleStewart_Games",
        "title": "Gale-Stewart Games",
        "authors": [
            "Sebastiaan J. C. Joosten"
        ],
        "topics": [
            "Mathematics/Games and economics"
        ],
        "date": "2021-04-23",
        "abstract": "\nThis is a formalisation of the main result of Gale and Stewart from\n1953, showing that closed finite games are determined. This property\nis now known as the Gale Stewart Theorem. While the original paper\nshows some additional theorems as well, we only formalize this main\nresult, but do so in a somewhat general way. We formalize games of a\nfixed arbitrary length, including infinite length, using co-inductive\nlists, and show that defensive strategies exist unless the other\nplayer is winning. For closed games, defensive strategies are winning\nfor the closed player, proving that such games are determined. For\nfinite games, which are a special case in our formalisation, all games\nare closed.",
        "licence": "BSD",
        "dependencies": [
            "Parity_Game"
        ],
        "theories": [
            "MoreCoinductiveList2",
            "MoreENat",
            "MorePrefix",
            "AlternatingLists",
            "GaleStewartGames",
            "GaleStewartDefensiveStrategies",
            "GaleStewartDeterminedGames"
        ]
    },
    {
        "session": "LatticeProperties",
        "title": "Lattice Properties",
        "authors": [
            "Viorel Preoteasa"
        ],
        "topics": [
            "Mathematics/Order"
        ],
        "date": "2011-09-22",
        "abstract": "This formalization introduces and collects some algebraic structures based on lattices and complete lattices for use in other developments. The structures introduced are modular, and lattice ordered groups. In addition to the results proved for the new lattices, this formalization also introduces theorems about latices and complete lattices in general.",
        "extra": {
            "Change history": "[2012-01-05] Removed the theory about distributive complete lattices which is in the standard library now.\nAdded a theory about well founded and transitive relations and a result about fixpoints in complete lattices and well founded relations.\nMoved the results about conjunctive and disjunctive functions to a new theory.\nRemoved the syntactic classes for inf and sup which are in the standard library now."
        },
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-28"
            },
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-24"
            },
            {
                "2011-1": "2011-10-11"
            },
            {
                "2011": "2011-09-27"
            }
        ],
        "theories": [
            "WellFoundedTransitive",
            "Complete_Lattice_Prop",
            "Conj_Disj",
            "Lattice_Prop",
            "Modular_Distrib_Lattice",
            "Lattice_Ordered_Group"
        ]
    },
    {
        "session": "Gauss_Sums",
        "title": "Gauss Sums and the Pólya–Vinogradov Inequality",
        "authors": [
            "Rodrigo Raya",
            "Manuel Eberl"
        ],
        "topics": [
            "Mathematics/Number theory"
        ],
        "date": "2019-12-10",
        "abstract": "\n<p>This article provides a full formalisation of Chapter 8 of\nApostol's <em><a\nhref=\"https://www.springer.com/de/book/9780387901633\">Introduction\nto Analytic Number Theory</a></em>. Subjects that are\ncovered are:</p> <ul> <li>periodic arithmetic\nfunctions and their finite Fourier series</li>\n<li>(generalised) Ramanujan sums</li> <li>Gauss sums\nand separable characters</li> <li>induced moduli and\nprimitive characters</li> <li>the\nPólya&mdash;Vinogradov inequality</li> </ul>",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2020-01-10"
            }
        ],
        "dependencies": [
            "Dirichlet_Series",
            "Polynomial_Interpolation",
            "Dirichlet_L"
        ],
        "theories": [
            "Gauss_Sums_Auxiliary",
            "Periodic_Arithmetic",
            "Complex_Roots_Of_Unity",
            "Finite_Fourier_Series",
            "Ramanujan_Sums",
            "Gauss_Sums",
            "Polya_Vinogradov"
        ]
    },
    {
        "session": "Girth_Chromatic",
        "title": "A Probabilistic Proof of the Girth-Chromatic Number Theorem",
        "authors": [
            "Lars Noschinski"
        ],
        "date": "2012-02-06",
        "topics": [
            "Mathematics/Graph theory"
        ],
        "abstract": "This works presents a formalization of the Girth-Chromatic number theorem in graph theory, stating that graphs with arbitrarily large girth and chromatic number exist. The proof uses the theory of Random Graphs to prove the existence with probabilistic arguments.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-24"
            },
            {
                "2011-1": "2012-02-06"
            }
        ],
        "theories": [
            "Girth_Chromatic_Misc",
            "Ugraphs",
            "Girth_Chromatic"
        ]
    },
    {
        "session": "VerifyThis2018",
        "title": "VerifyThis 2018 - Polished Isabelle Solutions",
        "authors": [
            "Peter Lammich",
            "Simon Wimmer"
        ],
        "topics": [
            "Computer science/Algorithms"
        ],
        "date": "2018-04-27",
        "abstract": "\n<a\nhref=\"http://www.pm.inf.ethz.ch/research/verifythis.html\">VerifyThis\n2018</a> was a program verification competition associated with\nETAPS 2018. It was the 7th event in the VerifyThis competition series.\nIn this entry, we present polished and completed versions of our\nsolutions that we created during the competition.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2018-04-30"
            }
        ],
        "dependencies": [
            "Sepref_IICF"
        ],
        "theories": [
            "Dynamic_Array",
            "DRAT_Misc",
            "Array_Map_Default",
            "Synth_Definition",
            "Exc_Nres_Monad",
            "VTcomp",
            "DF_System",
            "Snippets",
            "Challenge1",
            "Challenge1_short",
            "Challenge2",
            "Challenge3"
        ]
    },
    {
        "session": "SenSocialChoice",
        "title": "Some classical results in Social Choice Theory",
        "authors": [
            "Peter Gammie"
        ],
        "date": "2008-11-09",
        "topics": [
            "Mathematics/Games and economics"
        ],
        "abstract": "Drawing on Sen's landmark work \"Collective Choice and Social Welfare\" (1970), this development proves Arrow's General Possibility Theorem, Sen's Liberal Paradox and May's Theorem in a general setting. The goal was to make precise the classical statements and proofs of these results, and to provide a foundation for more recent results such as the Gibbard-Satterthwaite and Duggan-Schwartz theorems.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-24"
            },
            {
                "2011-1": "2012-03-15"
            },
            {
                "2011-1": "2011-10-11"
            },
            {
                "2011": "2011-02-11"
            },
            {
                "2009-2": "2010-07-01"
            },
            {
                "2009-1": "2009-12-12"
            },
            {
                "2009": "2009-04-29"
            },
            {
                "2008": "2008-11-17"
            }
        ],
        "theories": [
            "FSext",
            "RPRs",
            "SCFs",
            "Arrow",
            "Sen",
            "May"
        ]
    },
    {
        "session": "Tree_Decomposition",
        "title": "Tree Decomposition",
        "authors": [
            "Christoph Dittmann"
        ],
        "date": "2016-05-31",
        "topics": [
            "Mathematics/Graph theory"
        ],
        "abstract": "\nWe formalize tree decompositions and tree width in Isabelle/HOL,\nproving that trees have treewidth 1.  We also show that every edge of\na tree decomposition is a separation of the underlying graph. As an\napplication of this theorem we prove that complete graphs of size n\nhave treewidth n-1.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-06-01"
            },
            {
                "2016": "2016-05-31"
            }
        ],
        "theories": [
            "Graph",
            "Tree",
            "TreeDecomposition",
            "TreewidthTree",
            "TreewidthCompleteGraph",
            "ExampleInstantiations"
        ]
    },
    {
        "session": "BirdKMP",
        "title": "Putting the `K' into Bird's derivation of Knuth-Morris-Pratt string matching",
        "authors": [
            "Peter Gammie"
        ],
        "topics": [
            "Computer science/Functional programming"
        ],
        "date": "2020-08-25",
        "abstract": "\nRichard Bird and collaborators have proposed a derivation of an\nintricate cyclic program that implements the Morris-Pratt string\nmatching algorithm. Here we provide a proof of total correctness for\nBird's derivation and complete it by adding Knuth's\noptimisation.",
        "licence": "BSD",
        "theories": [
            "HOLCF_ROOT",
            "Theory_Of_Lists",
            "KMP"
        ]
    },
    {
        "session": "Minimal_SSA",
        "title": "Minimal Static Single Assignment Form",
        "authors": [
            "Max Wagner",
            "Denis Lohner"
        ],
        "topics": [
            "Computer science/Programming languages/Transformations"
        ],
        "date": "2017-01-17",
        "abstract": "\n<p>This formalization is an extension to <a\nhref=\"https://www.isa-afp.org/entries/Formal_SSA.html\">\"Verified\nConstruction of Static Single Assignment Form\"</a>. In\ntheir work, the authors have shown that <a\nhref=\"https://doi.org/10.1007/978-3-642-37051-9_6\">Braun\net al.'s static single assignment (SSA) construction\nalgorithm</a> produces minimal SSA form for input programs with\na reducible control flow graph (CFG). However Braun et al. also\nproposed an extension to their algorithm that they claim produces\nminimal SSA form even for irreducible CFGs.<br> In this\nformalization we support that claim by giving a mechanized proof.\n</p>\n<p>As the extension of Braun et al.'s algorithm\naims for removing so-called redundant strongly connected components of\nphi functions, we show that this suffices to guarantee minimality\naccording to <a href=\"https://doi.org/10.1145/115372.115320\">Cytron et\nal.</a>.</p>",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2017-01-19"
            }
        ],
        "dependencies": [
            "Formal_SSA"
        ],
        "theories": [
            "Irreducible"
        ]
    },
    {
        "session": "Posix-Lexing",
        "title": "POSIX Lexing with Derivatives of Regular Expressions",
        "authors": [
            "Fahad Ausaf",
            "Roy Dyckhoff",
            "Christian Urban"
        ],
        "date": "2016-05-24",
        "topics": [
            "Computer science/Automata and formal languages"
        ],
        "abstract": "\nBrzozowski introduced the notion of derivatives for regular\nexpressions. They can be used for a very simple regular expression\nmatching algorithm. Sulzmann and Lu cleverly extended this algorithm\nin order to deal with POSIX matching, which is the underlying\ndisambiguation strategy for regular expressions needed in lexers. In\nthis entry we give our inductive definition of what a POSIX value is\nand show (i) that such a value is unique (for given regular expression\nand string being matched) and (ii) that Sulzmann and Lu's algorithm\nalways generates such a value (provided that the regular expression\nmatches the string). We also prove the correctness of an optimised\nversion of the POSIX matching algorithm.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-05-24"
            }
        ],
        "dependencies": [
            "Regular-Sets"
        ],
        "theories": [
            "Lexer",
            "Simplifying"
        ]
    },
    {
        "session": "Hybrid_Systems_VCs",
        "title": "Verification Components for Hybrid Systems",
        "authors": [
            "Jonathan Julian Huerta y Munive"
        ],
        "topics": [
            "Mathematics/Algebra",
            "Mathematics/Analysis"
        ],
        "date": "2019-09-10",
        "abstract": "\nThese components formalise a semantic framework for the deductive\nverification of hybrid systems. They support reasoning about\ncontinuous evolutions of hybrid programs in the style of differential\ndynamics logic. Vector fields or flows model these evolutions, and\ntheir verification is done with invariants for the former or orbits\nfor the latter. Laws of modal Kleene algebra or categorical predicate\ntransformers implement the verification condition generation. Examples\nshow the approach at work.",
        "extra": {
            "Change history": "[2020-12-13] added components based on Kleene algebras with tests. These implement differential Hoare logic (dH) and a Morgan-style differential refinement calculus (dR) for verification of hybrid programs."
        },
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-09-10"
            }
        ],
        "dependencies": [
            "Ordinary_Differential_Equations",
            "Transformer_Semantics",
            "KAD",
            "KAT_and_DRA"
        ],
        "theories": [
            "HS_Preliminaries",
            "HS_ODEs",
            "HS_VC_Spartan",
            "HS_VC_Examples",
            "HS_VC_PT",
            "HS_VC_PT_Examples",
            "HS_VC_MKA",
            "HS_VC_KA_rel",
            "HS_VC_MKA_rel",
            "HS_VC_MKA_Examples_rel",
            "HS_VC_KA_ndfun",
            "HS_VC_MKA_ndfun",
            "HS_VC_MKA_Examples_ndfun",
            "HS_VC_KAT",
            "HS_VC_KAT_rel",
            "HS_VC_KAT_Examples_rel",
            "HS_VC_KAT_ndfun",
            "HS_VC_KAT_Examples_ndfun"
        ]
    },
    {
        "session": "Abs_Int_ITP2012",
        "title": "Abstract Interpretation of Annotated Commands",
        "authors": [
            "Tobias Nipkow"
        ],
        "date": "2016-11-23",
        "topics": [
            "Computer science/Programming languages/Static analysis"
        ],
        "abstract": "\nThis is the Isabelle formalization of the material decribed in the\neponymous <a href=\"https://doi.org/10.1007/978-3-642-32347-8_9\">ITP 2012 paper</a>.\nIt develops a generic abstract interpreter for a\nwhile-language, including widening and narrowing. The collecting\nsemantics and the abstract interpreter operate on annotated commands:\nthe program is represented as a syntax tree with the semantic\ninformation directly embedded, without auxiliary labels. The aim of\nthe formalization is simplicity, not efficiency or\nprecision. This is motivated by the inclusion of the material in a\ntheorem prover based course on semantics. A similar (but more\npolished) development is covered in the book\n<a href=\"https://doi.org/10.1007/978-3-319-10542-0\">Concrete Semantics</a>.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            }
        ],
        "theories": [
            "Complete_Lattice_ix",
            "ACom",
            "Collecting",
            "Abs_Int0",
            "Abs_State",
            "Abs_Int1",
            "Abs_Int2",
            "Abs_Int2_ivl",
            "Abs_Int3",
            "Abs_Int1_const",
            "Abs_Int1_parity"
        ]
    },
    {
        "session": "Surprise_Paradox",
        "title": "Surprise Paradox",
        "authors": [
            "Joachim Breitner"
        ],
        "date": "2016-07-17",
        "topics": [
            "Logic/Proof theory"
        ],
        "abstract": "\nIn 1964, Fitch showed that the paradox of the surprise hanging can be\nresolved by showing that the judge’s verdict is inconsistent. His\nformalization builds on Gödel’s coding of provability.  In this\ntheory, we reproduce his proof in Isabelle, building on Paulson’s\nformalisation of Gödel’s incompleteness theorems.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-07-17"
            }
        ],
        "dependencies": [
            "Incompleteness"
        ],
        "theories": [
            "Surprise_Paradox"
        ]
    },
    {
        "session": "Stuttering_Equivalence",
        "title": "Stuttering Equivalence",
        "authors": [
            "Stephan Merz"
        ],
        "topics": [
            "Computer science/Automata and formal languages"
        ],
        "date": "2012-05-07",
        "abstract": "<p>Two omega-sequences are stuttering equivalent if they differ only by finite repetitions of elements. Stuttering equivalence is a fundamental concept in the theory of concurrent and distributed systems. Notably, Lamport argues that refinement notions for such systems should be insensitive to finite stuttering. Peled and Wilke showed that all PLTL (propositional linear-time temporal logic) properties that are insensitive to stuttering equivalence can be expressed without the next-time operator. Stuttering equivalence is also important for certain verification techniques such as partial-order reduction for model checking.</p> <p>We formalize stuttering equivalence in Isabelle/HOL. Our development relies on the notion of stuttering sampling functions that may skip blocks of identical sequence elements. We also encode PLTL and prove the theorem due to Peled and Wilke.</p>",
        "extra": {
            "Change history": "[2013-01-31] Added encoding of PLTL and proved Peled and Wilke's theorem. Adjusted abstract accordingly."
        },
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2013-02-02"
            },
            {
                "2012": "2012-05-24"
            },
            {
                "2011-1": "2012-05-08"
            }
        ],
        "dependencies": [
            "LTL"
        ],
        "theories": [
            "Samplers",
            "StutterEquivalence",
            "PLTL"
        ]
    },
    {
        "session": "Finite_Automata_HF",
        "title": "Finite Automata in Hereditarily Finite Set Theory",
        "authors": [
            "Lawrence C. Paulson"
        ],
        "date": "2015-02-05",
        "topics": [
            "Computer science/Automata and formal languages"
        ],
        "abstract": "Finite Automata, both deterministic and non-deterministic, for regular languages.\nThe Myhill-Nerode Theorem. Closure under intersection, concatenation, etc.\nRegular expressions define regular languages. Closure under reversal;\nthe powerset construction mapping NFAs to DFAs. Left and right languages; minimal DFAs.\nBrzozowski's minimization algorithm. Uniqueness up to isomorphism of minimal DFAs.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2015-02-05"
            }
        ],
        "dependencies": [
            "Regular-Sets",
            "HereditarilyFinite"
        ],
        "theories": [
            "Finite_Automata_HF"
        ]
    },
    {
        "session": "Syntax_Independent_Logic",
        "title": "Syntax-Independent Logic Infrastructure",
        "authors": [
            "Andrei Popescu",
            "Dmitriy Traytel"
        ],
        "topics": [
            "Logic/Proof theory"
        ],
        "date": "2020-09-16",
        "abstract": "\nWe formalize a notion of logic whose terms and formulas are kept\nabstract. In particular, logical connectives, substitution, free\nvariables, and provability are not defined, but characterized by their\ngeneral properties as locale assumptions. Based on this abstract\ncharacterization, we develop further reusable reasoning\ninfrastructure. For example, we define parallel substitution (along\nwith proving its characterizing theorems) from single-point\nsubstitution. Similarly, we develop a natural deduction style proof\nsystem starting from the abstract Hilbert-style one. These one-time\nefforts benefit different concrete logics satisfying our locales'\nassumptions.  We instantiate the syntax-independent logic\ninfrastructure to Robinson arithmetic (also known as Q) in the AFP\nentry <a\nhref=\"https://www.isa-afp.org/entries/Robinson_Arithmetic.html\">Robinson_Arithmetic</a>\nand to hereditarily finite set theory in the AFP entries <a\nhref=\"https://www.isa-afp.org/entries/Goedel_HFSet_Semantic.html\">Goedel_HFSet_Semantic</a>\nand <a\nhref=\"https://www.isa-afp.org/entries/Goedel_HFSet_Semanticless.html\">Goedel_HFSet_Semanticless</a>,\nwhich are part of our formalization of G&ouml;del's\nIncompleteness Theorems described in our CADE-27 paper <a\nhref=\"https://dx.doi.org/10.1007/978-3-030-29436-6_26\">A\nFormally Verified Abstract Account of Gödel's Incompleteness\nTheorems</a>.",
        "licence": "BSD",
        "theories": [
            "Prelim",
            "Syntax",
            "Deduction",
            "Natural_Deduction",
            "Pseudo_Term",
            "Standard_Model",
            "Syntax_Arith",
            "Deduction_Q"
        ]
    },
    {
        "session": "Sunflowers",
        "title": "The Sunflower Lemma of Erdős and Rado",
        "authors": [
            "René Thiemann"
        ],
        "topics": [
            "Mathematics/Combinatorics"
        ],
        "date": "2021-02-25",
        "abstract": "\nWe formally define sunflowers and provide a formalization of the\nsunflower lemma of Erd&odblac;s and Rado: whenever a set of\nsize-<i>k</i>-sets has a larger cardinality than\n<i>(r - 1)<sup>k</sup> &middot; k!</i>,\nthen it contains a sunflower of cardinality <i>r</i>.",
        "licence": "BSD",
        "theories": [
            "Sunflower",
            "Erdos_Rado_Sunflower"
        ]
    },
    {
        "session": "Gabow_SCC",
        "title": "Verified Efficient Implementation of Gabow's Strongly Connected Components Algorithm",
        "authors": [
            "Peter Lammich"
        ],
        "date": "2014-05-28",
        "topics": [
            "Computer science/Algorithms/Graph",
            "Mathematics/Graph theory"
        ],
        "abstract": "\nWe present an Isabelle/HOL formalization of Gabow's algorithm for\nfinding the strongly connected components of a directed graph.\nUsing data refinement techniques, we extract efficient code that\nperforms comparable to a reference implementation in Java.\nOur style of formalization allows for re-using large parts of the proofs\nwhen defining variants of the algorithm. We demonstrate this by\nverifying an algorithm for the emptiness check of generalized Büchi\nautomata, re-using most of the existing proofs.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2014-05-29"
            }
        ],
        "dependencies": [
            "CAVA_Automata"
        ],
        "theories": [
            "Gabow_Skeleton",
            "Gabow_SCC",
            "Find_Path",
            "Gabow_GBG",
            "Gabow_Skeleton_Code",
            "Gabow_SCC_Code",
            "Find_Path_Impl",
            "Gabow_GBG_Code",
            "All_Of_Gabow_SCC"
        ]
    },
    {
        "session": "Closest_Pair_Points",
        "title": "Closest Pair of Points Algorithms",
        "authors": [
            "Martin Rau",
            "Tobias Nipkow"
        ],
        "topics": [
            "Computer science/Algorithms/Geometry"
        ],
        "date": "2020-01-13",
        "abstract": "\nThis entry provides two related verified divide-and-conquer algorithms\nsolving the fundamental <em>Closest Pair of Points</em>\nproblem in Computational Geometry. Functional correctness and the\noptimal running time of <em>O</em>(<em>n</em> log <em>n</em>) are\nproved. Executable code is generated which is empirically competitive\nwith handwritten reference implementations.",
        "extra": {
            "Change history": "[2020-14-04] Incorporate Time_Monad of the AFP entry Root_Balanced_Tree."
        },
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2020-01-14"
            }
        ],
        "dependencies": [
            "Akra_Bazzi",
            "Root_Balanced_Tree"
        ],
        "theories": [
            "Common",
            "Closest_Pair",
            "Closest_Pair_Alternative"
        ]
    },
    {
        "session": "Euler_MacLaurin",
        "title": "The Euler–MacLaurin Formula",
        "authors": [
            "Manuel Eberl"
        ],
        "topics": [
            "Mathematics/Analysis"
        ],
        "date": "2017-03-10",
        "abstract": "\n<p>The Euler-MacLaurin formula relates the value of a\ndiscrete sum to that of the corresponding integral in terms of the\nderivatives at the borders of the summation and a remainder term.\nSince the remainder term is often very small as the summation bounds\ngrow, this can be used to compute asymptotic expansions for\nsums.</p>  <p>This entry contains a proof of this formula\nfor functions from the reals to an arbitrary Banach space. Two\nvariants of the formula are given: the standard textbook version and a\nvariant outlined in <em>Concrete Mathematics</em> that is\nmore useful for deriving asymptotic estimates.</p>  <p>As\nexample applications, we use that formula to derive the full\nasymptotic expansion of the harmonic numbers and the sum of inverse\nsquares.</p>",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2017-03-14"
            }
        ],
        "dependencies": [
            "Bernoulli",
            "Landau_Symbols"
        ],
        "theories": [
            "Euler_MacLaurin",
            "Euler_MacLaurin_Landau"
        ]
    },
    {
        "session": "Buchi_Complementation",
        "title": "Büchi Complementation",
        "authors": [
            "Julian Brunner"
        ],
        "topics": [
            "Computer science/Automata and formal languages"
        ],
        "date": "2017-10-19",
        "abstract": "\nThis entry provides a verified implementation of rank-based Büchi\nComplementation. The verification is done in three steps: <ol>\n<li>Definition of odd rankings and proof that an automaton\nrejects a word iff there exists an odd ranking for it.</li>\n<li>Definition of the complement automaton and proof that it\naccepts exactly those words for which there is an odd\nranking.</li> <li>Verified implementation of the\ncomplement automaton using the Isabelle Collections\nFramework.</li> </ol>",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-27"
            }
        ],
        "dependencies": [
            "Transition_Systems_and_Automata"
        ],
        "theories": [
            "Alternate",
            "Graph",
            "Ranking",
            "Complementation",
            "Complementation_Implement",
            "Formula",
            "Complementation_Final",
            "Complementation_Build",
            "files/code/Autool.mlb",
            "files/code/Prelude.sml",
            "files/code/Autool.sml"
        ]
    },
    {
        "session": "Goedel_HFSet_Semantic",
        "title": "From Abstract to Concrete G&ouml;del's Incompleteness Theorems&mdash;Part I",
        "authors": [
            "Andrei Popescu",
            "Dmitriy Traytel"
        ],
        "topics": [
            "Logic/Proof theory"
        ],
        "date": "2020-09-16",
        "abstract": "\nWe validate an abstract formulation of G&ouml;del's First and\nSecond Incompleteness Theorems from a <a\nhref=\"https://www.isa-afp.org/entries/Goedel_Incompleteness.html\">separate\nAFP entry</a> by instantiating them to the case of\n<i>finite sound extensions of the Hereditarily Finite (HF) Set\ntheory</i>, i.e., FOL theories extending the HF Set theory with\na finite set of axioms that are sound in the standard model. The\nconcrete results had been previously formalised in an <a\nhref=\"https://www.isa-afp.org/entries/Incompleteness.html\">AFP\nentry by Larry Paulson</a>; our instantiation reuses the\ninfrastructure developed in that entry.",
        "licence": "BSD",
        "dependencies": [
            "Goedel_Incompleteness",
            "Incompleteness"
        ],
        "theories": [
            "Instance"
        ]
    },
    {
        "session": "PropResPI",
        "title": "Propositional Resolution and Prime Implicates Generation",
        "authors": [
            "Nicolas Peltier"
        ],
        "date": "2016-03-11",
        "topics": [
            "Logic/General logic/Mechanization of proofs"
        ],
        "abstract": "\nWe provide formal proofs in Isabelle-HOL (using mostly structured Isar\nproofs) of the soundness and completeness of the Resolution rule in\npropositional logic.  The completeness proofs take into account the\nusual redundancy elimination rules (tautology elimination and\nsubsumption), and several refinements of the Resolution rule are\nconsidered: ordered resolution (with selection functions), positive\nand negative resolution, semantic resolution and unit resolution (the\nlatter refinement is complete only for clause sets that are Horn-\nrenamable). We also define a concrete procedure for computing\nsaturated sets and establish its soundness and completeness. The\nclause sets are not assumed to be finite, so that the results can be\napplied to formulas obtained by grounding sets of first-order clauses\n(however, a total ordering among atoms is assumed to be given).\nNext, we show that the unrestricted Resolution rule is deductive-\ncomplete, in the sense that it is able to generate all  (prime)\nimplicates of any set of propositional clauses (i.e., all entailment-\nminimal, non-valid, clausal consequences of the considered set). The\ngeneration of prime implicates is an important problem, with many\napplications in artificial intelligence and verification (for\nabductive reasoning, knowledge compilation, diagnosis, debugging\netc.). We also show that implicates can be computed in an incremental\nway, by fixing an ordering among all the atoms in the considered sets\nand resolving upon these atoms one by one in the considered order\n(with no backtracking). This feature is critical for the efficient\ncomputation of prime implicates. Building on these results, we provide\na procedure for computing such implicates and establish its soundness\nand completeness.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-03-11"
            }
        ],
        "theories": [
            "Propositional_Resolution",
            "Prime_Implicates"
        ]
    },
    {
        "session": "RIPEMD-160-SPARK",
        "title": "RIPEMD-160",
        "authors": [
            "Fabian Immler"
        ],
        "topics": [
            "Computer science/Programming languages/Static analysis"
        ],
        "date": "2011-01-10",
        "abstract": "This work presents a verification of an implementation in SPARK/ADA of the cryptographic hash-function RIPEMD-160. A functional specification of RIPEMD-160 is given in Isabelle/HOL. Proofs for the verification conditions generated by the static-analysis toolset of SPARK certify the functional correctness of the implementation.",
        "extra": {
            "Change history": "[2015-11-09] Entry is now obsolete, moved to Isabelle distribution."
        },
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-24"
            },
            {
                "2011-1": "2011-10-11"
            },
            {
                "2011": "2011-02-11"
            },
            {
                "2009-2": "2011-01-19"
            }
        ],
        "theories": [
            "RIPEMD_160_SPARK"
        ]
    },
    {
        "session": "Buffons_Needle",
        "title": "Buffon's Needle Problem",
        "authors": [
            "Manuel Eberl"
        ],
        "topics": [
            "Mathematics/Probability theory",
            "Mathematics/Geometry"
        ],
        "date": "2017-06-06",
        "abstract": "\nIn the 18th century, Georges-Louis Leclerc, Comte de Buffon posed and\nlater solved the following problem, which is often called the first\nproblem ever solved in geometric probability: Given a floor divided\ninto vertical strips of the same width, what is the probability that a\nneedle thrown onto the floor randomly will cross two strips?  This\nentry formally defines the problem in the case where the needle's\nposition is chosen uniformly at random in a single strip around the\norigin (which is equivalent to larger arrangements due to symmetry).\nIt then provides proofs of the simple solution in the case where the\nneedle's length is no greater than the width of the strips and\nthe more complicated solution in the opposite case.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2017-06-06"
            }
        ],
        "theories": [
            "Buffons_Needle"
        ]
    },
    {
        "session": "ShortestPath",
        "title": "An Axiomatic Characterization of the Single-Source Shortest Path Problem",
        "authors": [
            "Christine Rizkallah"
        ],
        "date": "2013-05-22",
        "topics": [
            "Mathematics/Graph theory"
        ],
        "abstract": "This theory is split into two sections. In the first section, we give a formal proof that a well-known axiomatic characterization of the single-source shortest path problem is correct. Namely, we prove that in a directed graph with a non-negative cost function on the edges the single-source shortest path function is the only function that satisfies a set of four axioms. In the second section, we give a formal proof of the correctness of an axiomatic characterization of the single-source shortest path problem for directed graphs with general cost functions. The axioms here are more involved because we have to account for potential negative cycles in the graph. The axioms are summarized in three Isabelle locales.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-05-30"
            }
        ],
        "dependencies": [
            "Graph_Theory"
        ],
        "theories": [
            "ShortestPath",
            "ShortestPathNeg"
        ]
    },
    {
        "session": "Generic_Deriving",
        "title": "Deriving generic class instances for datatypes",
        "authors": [
            "Jonas Rädle",
            "Lars Hupel"
        ],
        "topics": [
            "Computer science/Data structures"
        ],
        "date": "2018-11-06",
        "abstract": "\n<p>We provide a framework for automatically deriving instances for\ngeneric type classes. Our approach is inspired by Haskell's\n<i>generic-deriving</i> package and Scala's\n<i>shapeless</i> library.  In addition to generating the\ncode for type class functions, we also attempt to automatically prove\ntype class laws for these instances. As of now, however, some manual\nproofs are still required for recursive datatypes.</p>\n<p>Note: There are already articles in the AFP that provide\nautomatic instantiation for a number of classes. Concretely, <a href=\"https://www.isa-afp.org/entries/Deriving.html\">Deriving</a> allows the automatic instantiation of comparators, linear orders, equality, and hashing. <a href=\"https://www.isa-afp.org/entries/Show.html\">Show</a> instantiates a Haskell-style <i>show</i> class.</p><p>Our approach works for arbitrary classes (with some Isabelle/HOL overhead for each class), but a smaller set of datatypes.</p>",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-11-21"
            }
        ],
        "theories": [
            "Tagged_Prod_Sum",
            "Derive",
            "files/derive_util.ML",
            "files/derive_laws.ML",
            "files/derive_setup.ML",
            "files/derive.ML",
            "Derive_Datatypes",
            "Derive_Eq",
            "Derive_Encode",
            "Derive_Algebra",
            "Derive_Show",
            "Derive_Eq_Laws",
            "Derive_Algebra_Laws"
        ]
    },
    {
        "session": "Noninterference_Sequential_Composition",
        "title": "Conservation of CSP Noninterference Security under Sequential Composition",
        "authors": [
            "Pasquale Noce"
        ],
        "date": "2016-04-26",
        "topics": [
            "Computer science/Security",
            "Computer science/Concurrency/Process calculi"
        ],
        "abstract": "\n<p>In his outstanding work on Communicating Sequential Processes, Hoare\nhas defined two fundamental binary operations allowing to compose the\ninput processes into another, typically more complex, process:\nsequential composition and concurrent composition. Particularly, the\noutput of the former operation is a process that initially behaves\nlike the first operand, and then like the second operand once the\nexecution of the first one has terminated successfully, as long as it\ndoes.</p>\n<p>This paper formalizes Hoare's definition of sequential\ncomposition and proves, in the general case of a possibly intransitive\npolicy, that CSP noninterference security is conserved under this\noperation, provided that successful termination cannot be affected by\nconfidential events and cannot occur as an alternative to other events\nin the traces of the first operand. Both of these assumptions are\nshown, by means of counterexamples, to be necessary for the theorem to\nhold.</p>",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-04-26"
            }
        ],
        "dependencies": [
            "Noninterference_Ipurge_Unwinding"
        ],
        "theories": [
            "Propaedeutics",
            "SequentialComposition",
            "Counterexamples"
        ]
    },
    {
        "session": "Noninterference_Inductive_Unwinding",
        "title": "The Inductive Unwinding Theorem for CSP Noninterference Security",
        "topics": [
            "Computer science/Security"
        ],
        "authors": [
            "Pasquale Noce"
        ],
        "date": "2015-08-18",
        "abstract": "\n<p>\nThe necessary and sufficient condition for CSP noninterference security stated by the Ipurge Unwinding Theorem is expressed in terms of a pair of event lists varying over the set of process traces. This does not render it suitable for the subsequent application of rule induction in the case of a process defined inductively, since rule induction may rather be applied to a single variable ranging over an inductively defined set.\n</p><p>\nStarting from the Ipurge Unwinding Theorem, this paper derives a necessary and sufficient condition for CSP noninterference security that involves a single event list varying over the set of process traces, and is thus suitable for rule induction; hence its name, Inductive Unwinding Theorem. Similarly to the Ipurge Unwinding Theorem, the new theorem only requires to consider individual accepted and refused events for each process trace, and applies to the general case of a possibly intransitive noninterference policy. Specific variants of this theorem are additionally proven for deterministic processes and trace set processes.\n</p>",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-08-19"
            }
        ],
        "dependencies": [
            "Noninterference_Ipurge_Unwinding"
        ],
        "theories": [
            "InductiveUnwinding"
        ]
    },
    {
        "session": "Category3",
        "title": "Category Theory with Adjunctions and Limits",
        "authors": [
            "Eugene W. Stark"
        ],
        "date": "2016-06-26",
        "topics": [
            "Mathematics/Category theory"
        ],
        "abstract": "\n<p>\nThis article attempts to develop a usable framework for doing category\ntheory in Isabelle/HOL.  Our point of view, which to some extent\ndiffers from that of the previous AFP articles on the subject, is to\ntry to explore how category theory can be done efficaciously within\nHOL, rather than trying to match exactly the way things are done using\na traditional approach.  To this end, we define the notion of category\nin an \"object-free\" style, in which a category is represented by a\nsingle partial composition operation on arrows.  This way of defining\ncategories provides some advantages in the context of HOL, including\nthe ability to avoid the use of records and the possibility of\ndefining functors and natural transformations simply as certain\nfunctions on arrows, rather than as composite objects.  We define\nvarious constructions associated with the basic notions, including:\ndual category, product category, functor category, discrete category,\nfree category, functor composition, and horizontal and vertical\ncomposite of natural transformations.  A \"set category\" locale is\ndefined that axiomatizes the notion \"category of all sets at a type\nand all functions between them,\" and a fairly extensive set of\nproperties of set categories is derived from the locale assumptions.\nThe notion of a set category is used to prove the Yoneda Lemma in a\ngeneral setting of a category equipped with a \"hom embedding,\" which\nmaps arrows of the category to the \"universe\" of the set category.  We\nalso give a treatment of adjunctions, defining adjunctions via left\nand right adjoint functors, natural bijections between hom-sets, and\nunit and counit natural transformations, and showing the equivalence\nof these definitions.  We also develop the theory of limits, including\nrepresentations of functors, diagrams and cones, and diagonal\nfunctors.  We show that right adjoint functors preserve limits, and\nthat limits can be constructed via products and equalizers.  We\ncharacterize the conditions under which limits exist in a set\ncategory. We also examine the case of limits in a functor category,\nultimately culminating in a proof that the Yoneda embedding preserves\nlimits.\n</p><p>\nRevisions made subsequent to the first version of this article added\nmaterial on equivalence of categories, cartesian categories,\ncategories with pullbacks, categories with finite limits, and\ncartesian closed categories.  A construction was given of the category\nof hereditarily finite sets and functions between them, and it was\nshown that this category is cartesian closed.\n</p>",
        "extra": {
            "Change history": "[2018-05-29]\nRevised axioms for the category locale.  Introduced notation for composition and \"in hom\".\n(revision 8318366d4575)<br>\n[2020-02-15]\nMove ConcreteCategory.thy from Bicategory to Category3 and use it systematically.\nMake other minor improvements throughout.\n(revision a51840d36867)<br>\n[2020-07-10]\nAdded new material, mostly centered around cartesian categories.\n(revision 06640f317a79)<br>\n[2020-11-04]\nMinor modifications and extensions made in conjunction with the addition\nof new material to Bicategory.\n(revision 472cb2268826)<br>"
        },
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-06-26"
            }
        ],
        "dependencies": [
            "HereditarilyFinite"
        ],
        "theories": [
            "Category",
            "ConcreteCategory",
            "FreeCategory",
            "DiscreteCategory",
            "DualCategory",
            "EpiMonoIso",
            "InitialTerminal",
            "Functor",
            "SetCategory",
            "SetCat",
            "ProductCategory",
            "NaturalTransformation",
            "BinaryFunctor",
            "FunctorCategory",
            "Yoneda",
            "Adjunction",
            "Limit",
            "Subcategory",
            "EquivalenceOfCategories",
            "CartesianCategory",
            "CategoryWithPullbacks",
            "CategoryWithFiniteLimits",
            "CartesianClosedCategory",
            "HFSetCat"
        ]
    },
    {
        "session": "FinFun",
        "title": "Code Generation for Functions as Data",
        "authors": [
            "Andreas Lochbihler"
        ],
        "date": "2009-05-06",
        "topics": [
            "Computer science/Data structures"
        ],
        "abstract": "FinFuns are total functions that are constant except for a finite set of points, i.e. a generalisation of finite maps. They are formalised as a new type in Isabelle/HOL such that the code generator can handle equality tests and quantification on FinFuns. On the code output level, FinFuns are explicitly represented by constant functions and pointwise updates, similarly to associative lists. Inside the logic, they behave like ordinary functions with extensionality. Via the update/constant pattern, a recursion combinator and an induction rule for FinFuns allow for defining and reasoning about operators on FinFun that are also executable.",
        "extra": {
            "Change history": "[2010-08-13]\nnew concept domain of a FinFun as a FinFun\n(revision 34b3517cbc09)<br>\n[2010-11-04]\nnew conversion function from FinFun to list of elements in the domain\n(revision 0c167102e6ed)<br>\n[2012-03-07]\nreplace sets as FinFuns by predicates as FinFuns because the set type constructor has been reintroduced\n(revision b7aa87989f3a)"
        },
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-03-02"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-24"
            },
            {
                "2011-1": "2011-10-11"
            },
            {
                "2011": "2011-02-11"
            },
            {
                "2009-2": "2010-06-30"
            },
            {
                "2009-1": "2009-12-12"
            },
            {
                "2009": "2009-05-25"
            }
        ],
        "theories": [
            "FinFun",
            "FinFunPred"
        ]
    },
    {
        "session": "MFODL_Monitor_Optimized",
        "title": "Formalization of an Optimized Monitoring Algorithm for Metric First-Order Dynamic Logic with Aggregations",
        "authors": [
            "Thibault Dardinier",
            "Lukas Heimes",
            "Martin Raszyk",
            "Joshua Schneider",
            "Dmitriy Traytel"
        ],
        "topics": [
            "Computer science/Algorithms",
            "Logic/General logic/Modal logic",
            "Computer science/Automata and formal languages"
        ],
        "date": "2020-04-09",
        "abstract": "\nA monitor is a runtime verification tool that solves the following\nproblem: Given a stream of time-stamped events and a policy formulated\nin a specification language, decide whether the policy is satisfied at\nevery point in the stream. We verify the correctness of an executable\nmonitor for specifications given as formulas in metric first-order\ndynamic logic (MFODL), which combines the features of metric\nfirst-order temporal logic (MFOTL) and metric dynamic logic. Thus,\nMFODL supports real-time constraints, first-order parameters, and\nregular expressions. Additionally, the monitor supports aggregation\noperations such as count and sum. This formalization, which is\ndescribed in a <a\nhref=\"http://people.inf.ethz.ch/trayteld/papers/ijcar20-verimonplus/verimonplus.pdf\">\nforthcoming paper at IJCAR 2020</a>, significantly extends <a\nhref=\"https://www.isa-afp.org/entries/MFOTL_Monitor.html\">previous\nwork on a verified monitor</a> for MFOTL. Apart from the\naddition of regular expressions and aggregations, we implemented <a\nhref=\"https://www.isa-afp.org/entries/Generic_Join.html\">multi-way\njoins</a> and a specialized sliding window algorithm to further\noptimize the monitor.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2020-04-12"
            },
            {
                "2019": "2020-04-11"
            }
        ],
        "dependencies": [
            "MFOTL_Monitor",
            "IEEE_Floating_Point",
            "Generic_Join"
        ],
        "theories": [
            "Code_Double",
            "Event_Data",
            "Regex",
            "Formula",
            "Optimized_Join",
            "Monitor",
            "Optimized_MTL",
            "Monitor_Impl",
            "Monitor_Code"
        ]
    },
    {
        "session": "Abstract-Hoare-Logics",
        "title": "Abstract Hoare Logics",
        "authors": [
            "Tobias Nipkow"
        ],
        "date": "2006-08-08",
        "topics": [
            "Computer science/Programming languages/Logics"
        ],
        "abstract": "These therories describe Hoare logics for a number of imperative language constructs, from while-loops to mutually recursive procedures. Both partial and total correctness are treated. In particular a proof system for total correctness of recursive procedures in the presence of unbounded nondeterminism is presented.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-24"
            },
            {
                "2011-1": "2011-10-11"
            },
            {
                "2011": "2011-02-11"
            },
            {
                "2009-2": "2010-06-30"
            },
            {
                "2009-1": "2009-12-12"
            },
            {
                "2009": "2009-04-29"
            },
            {
                "2008": "2008-06-10"
            },
            {
                "2007": "2007-11-27"
            }
        ],
        "theories": [
            "Lang",
            "Hoare",
            "Termi",
            "HoareTotal",
            "PLang",
            "PHoare",
            "PTermi",
            "PHoareTotal",
            "PsLang",
            "PsHoare",
            "PsTermi",
            "PsHoareTotal"
        ]
    },
    {
        "session": "Quantales",
        "title": "Quantales",
        "authors": [
            "Georg Struth"
        ],
        "topics": [
            "Mathematics/Algebra"
        ],
        "date": "2018-12-11",
        "abstract": "\nThese mathematical components formalise basic properties of quantales,\ntogether with some important models, constructions, and concepts,\nincluding quantic nuclei and conuclei.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-12-19"
            }
        ],
        "dependencies": [
            "Order_Lattice_Props",
            "Kleene_Algebra"
        ],
        "theories": [
            "Dioid_Models_New",
            "Quantales",
            "Quantale_Star",
            "Quantale_Modules",
            "Quantale_Models",
            "Quantic_Nuclei_Conuclei",
            "Quantale_Left_Sided"
        ]
    },
    {
        "session": "Groebner_Macaulay",
        "title": "Gröbner Bases, Macaulay Matrices and Dubé's Degree Bounds",
        "authors": [
            "Alexander Maletzky"
        ],
        "topics": [
            "Mathematics/Algebra"
        ],
        "date": "2019-06-15",
        "abstract": "\nThis entry formalizes the connection between Gröbner bases and\nMacaulay matrices (sometimes also referred to as `generalized\nSylvester matrices'). In particular, it contains a method for\ncomputing Gröbner bases, which proceeds by first constructing some\nMacaulay matrix of the initial set of polynomials, then row-reducing\nthis matrix, and finally converting the result back into a set of\npolynomials. The output is shown to be a Gröbner basis if the Macaulay\nmatrix constructed in the first step is sufficiently large. In order\nto obtain concrete upper bounds on the size of the matrix (and hence\nturn the method into an effectively executable algorithm), Dubé's\ndegree bounds on Gröbner bases are utilized; consequently, they are\nalso part of the formalization.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-17"
            }
        ],
        "dependencies": [
            "Groebner_Bases"
        ],
        "theories": [
            "Degree_Section",
            "Degree_Bound_Utils",
            "Groebner_Macaulay",
            "Binomial_Int",
            "Poly_Fun",
            "Monomial_Module",
            "Dube_Prelims",
            "Hilbert_Function",
            "Cone_Decomposition",
            "Dube_Bound",
            "Groebner_Macaulay_Examples"
        ]
    },
    {
        "session": "Lowe_Ontological_Argument",
        "title": "Computer-assisted Reconstruction and Assessment of E. J. Lowe's Modal Ontological Argument",
        "authors": [
            "David Fuenmayor",
            "Christoph Benzmüller"
        ],
        "topics": [
            "Logic/Philosophical aspects"
        ],
        "date": "2017-09-21",
        "abstract": "\nComputers may help us to understand --not just verify-- philosophical\narguments. By utilizing modern proof assistants in an iterative\ninterpretive process, we can reconstruct and assess an argument by\nfully formal means. Through the mechanization of a variant of St.\nAnselm's ontological argument by E. J. Lowe, which is a\nparadigmatic example of a natural-language argument with strong ties\nto metaphysics and religion, we offer an ideal showcase for our\ncomputer-assisted interpretive method.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-16"
            }
        ],
        "theories": [
            "Relations",
            "QML",
            "LoweOntologicalArgument_1",
            "LoweOntologicalArgument_2",
            "LoweOntologicalArgument_3",
            "LoweOntologicalArgument_4",
            "LoweOntologicalArgument_5",
            "LoweOntologicalArgument_6",
            "LoweOntologicalArgument_7"
        ]
    },
    {
        "session": "FeatherweightJava",
        "title": "A Theory of Featherweight Java in Isabelle/HOL",
        "authors": [
            "J. Nathan Foster",
            "Dimitrios Vytiniotis"
        ],
        "date": "2006-03-31",
        "topics": [
            "Computer science/Programming languages/Language definitions"
        ],
        "abstract": "We formalize the type system, small-step operational semantics, and type soundness proof for Featherweight Java, a simple object calculus, in Isabelle/HOL.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-24"
            },
            {
                "2011-1": "2011-10-11"
            },
            {
                "2011": "2011-02-11"
            },
            {
                "2009-2": "2010-06-30"
            },
            {
                "2009-1": "2009-12-12"
            },
            {
                "2009": "2009-04-29"
            },
            {
                "2008": "2008-06-10"
            },
            {
                "2007": "2007-11-27"
            },
            {
                "2005": "2006-04-06"
            },
            {
                "2005": "2006-04-05"
            }
        ],
        "theories": [
            "FJDefs",
            "FJAux",
            "FJSound",
            "Execute",
            "Featherweight_Java"
        ]
    },
    {
        "session": "Strong_Security",
        "title": "A Formalization of Strong Security",
        "authors": [
            "Sylvia Grewe",
            "Alexander Lux",
            "Heiko Mantel",
            "Jens Sauer"
        ],
        "date": "2014-04-23",
        "topics": [
            "Computer science/Security",
            "Computer science/Programming languages/Type systems"
        ],
        "abstract": "Research in information-flow security aims at developing methods to\nidentify undesired information leaks within programs from private\nsources to public sinks. Noninterference captures this\nintuition. Strong security from Sabelfeld and Sands\nformalizes noninterference for concurrent systems.\n<p>\nWe present an Isabelle/HOL formalization of strong security for\narbitrary security lattices (Sabelfeld and Sands use\na two-element security lattice in the original publication).\nThe formalization includes\ncompositionality proofs for strong security and a soundness proof\nfor a security type system that checks strong security for programs\nin a simple while language with dynamic thread creation.\n<p>\nOur formalization of the security type system is abstract in the\nlanguage for expressions and in the semantic side conditions for\nexpressions. It can easily be instantiated with different syntactic\napproximations for these side conditions. The soundness proof of\nsuch an instantiation boils down to showing that these syntactic\napproximations imply the semantic side conditions.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2014-04-24"
            }
        ],
        "theories": [
            "Types",
            "Expr",
            "Domain_example",
            "MWLf",
            "Strong_Security",
            "Up_To_Technique",
            "Parallel_Composition",
            "Strongly_Secure_Skip_Assign",
            "Language_Composition",
            "Type_System",
            "Type_System_example"
        ]
    },
    {
        "session": "LLL_Factorization",
        "title": "A verified factorization algorithm for integer polynomials with polynomial complexity",
        "authors": [
            "Jose Divasón",
            "Sebastiaan J. C. Joosten",
            "René Thiemann",
            "Akihisa Yamada"
        ],
        "topics": [
            "Mathematics/Algebra"
        ],
        "date": "2018-02-06",
        "abstract": "\nShort vectors in lattices and factors of integer polynomials are\nrelated. Each factor of an integer polynomial belongs to a certain\nlattice. When factoring polynomials, the condition that we are looking\nfor an irreducible polynomial means that we must look for a small\nelement in a lattice, which can be done by a basis reduction\nalgorithm. In this development we formalize this connection and\nthereby one main application of the LLL basis reduction algorithm: an\nalgorithm to factor square-free integer polynomials which runs in\npolynomial time. The work is based on our previous\nBerlekamp–Zassenhaus development, where the exponential reconstruction\nphase has been replaced by the polynomial-time basis reduction\nalgorithm. Thanks to this formalization we found a serious flaw in a\ntextbook.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2018-02-07"
            }
        ],
        "dependencies": [
            "LLL_Basis_Reduction",
            "Perron_Frobenius"
        ],
        "theories": [
            "Factor_Bound_2",
            "Missing_Dvd_Int_Poly",
            "LLL_Factorization_Impl",
            "LLL_Factorization",
            "Sub_Sums",
            "Factorization_Algorithm_16_22",
            "Modern_Computer_Algebra_Problem"
        ]
    },
    {
        "session": "Roy_Floyd_Warshall",
        "title": "Transitive closure according to Roy-Floyd-Warshall",
        "authors": [
            "Makarius Wenzel"
        ],
        "date": "2014-05-23",
        "topics": [
            "Computer science/Algorithms/Graph"
        ],
        "abstract": "This formulation of the Roy-Floyd-Warshall algorithm for the\ntransitive closure bypasses matrices and arrays, but uses a more direct\nmathematical model with adjacency functions for immediate predecessors and\nsuccessors. This can be implemented efficiently in functional programming\nlanguages and is particularly adequate for sparse relations.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2020-01-14"
            },
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2014-05-24"
            }
        ],
        "theories": [
            "Roy_Floyd_Warshall"
        ]
    },
    {
        "session": "Random_BSTs",
        "title": "Expected Shape of Random Binary Search Trees",
        "authors": [
            "Manuel Eberl"
        ],
        "topics": [
            "Computer science/Data structures"
        ],
        "date": "2017-04-04",
        "abstract": "\n<p>This entry contains proofs for the textbook results about the\ndistributions of the height and internal path length of random binary\nsearch trees (BSTs), i.&thinsp;e. BSTs that are formed by taking\nan empty BST and inserting elements from a fixed set in random\norder.</p>  <p>In particular, we prove a logarithmic upper\nbound on the expected height and the <em>Θ(n log n)</em>\nclosed-form solution for the expected internal path length in terms of\nthe harmonic numbers. We also show how the internal path length\nrelates to the average-case cost of a lookup in a BST.</p>",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2017-04-04"
            }
        ],
        "dependencies": [
            "Quick_Sort_Cost",
            "Landau_Symbols"
        ],
        "theories": [
            "Random_BSTs"
        ]
    },
    {
        "session": "Card_Partitions",
        "title": "Cardinality of Set Partitions",
        "authors": [
            "Lukas Bulwahn"
        ],
        "date": "2015-12-12",
        "topics": [
            "Mathematics/Combinatorics"
        ],
        "abstract": "\nThe theory's main theorem states that the cardinality of set partitions of\nsize k on a carrier set of size n is expressed by Stirling numbers of the\nsecond kind. In Isabelle, Stirling numbers of the second kind are defined\nin the AFP entry `Discrete Summation` through their well-known recurrence\nrelation. The main theorem relates them to the alternative definition as\ncardinality of set partitions. The proof follows the simple and short\nexplanation in Richard P. Stanley's `Enumerative Combinatorics: Volume 1`\nand Wikipedia, and unravels the full details and implicit reasoning steps\nof these explanations.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-12-13"
            }
        ],
        "dependencies": [
            "Discrete_Summation"
        ],
        "theories": [
            "Set_Partition",
            "Injectivity_Solver",
            "Card_Partitions"
        ]
    },
    {
        "session": "Simple_Firewall",
        "title": "Simple Firewall",
        "authors": [
            "Cornelius Diekmann",
            "Julius Michaelis",
            "Maximilian P. L. Haslbeck"
        ],
        "date": "2016-08-24",
        "topics": [
            "Computer science/Networks"
        ],
        "abstract": "\nWe present a simple model of a firewall. The firewall can accept or\ndrop a packet and can match on interfaces, IP addresses, protocol, and\nports. It was designed to feature nice mathematical properties: The\ntype of match expressions was carefully crafted such that the\nconjunction of two match expressions is only one match expression.\nThis model is too simplistic to mirror all aspects of the real world.\nIn the upcoming entry \"Iptables Semantics\", we will translate the\nLinux firewall iptables to this model.  For a fixed service (e.g. ssh,\nhttp), we provide an algorithm to compute an overview of the\nfirewall's filtering behavior. The algorithm computes minimal service\nmatrices, i.e. graphs which partition the complete IPv4 and IPv6\naddress space and visualize the allowed accesses between partitions.\nFor a detailed description, see\n<a href=\"http://dl.ifip.org/db/conf/networking/networking2016/1570232858.pdf\">Verified iptables Firewall\nAnalysis</a>, IFIP Networking 2016.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-08-24"
            }
        ],
        "dependencies": [
            "IP_Addresses"
        ],
        "theories": [
            "Lib_Enum_toString",
            "L4_Protocol",
            "Simple_Packet",
            "Firewall_Common_Decision_State",
            "Iface",
            "SimpleFw_Syntax",
            "SimpleFw_Semantics",
            "List_Product_More",
            "Option_Helpers",
            "Generic_SimpleFw",
            "Shadowed",
            "IP_Partition_Preliminaries",
            "GroupF",
            "IP_Addr_WordInterval_toString",
            "Primitives_toString",
            "Service_Matrix",
            "SimpleFw_toString"
        ]
    },
    {
        "session": "KAT_and_DRA",
        "title": "Kleene Algebra with Tests and Demonic Refinement Algebras",
        "authors": [
            "Alasdair Armstrong",
            "Victor B. F. Gomes",
            "Georg Struth"
        ],
        "date": "2014-01-23",
        "topics": [
            "Computer science/Programming languages/Logics",
            "Computer science/Automata and formal languages",
            "Mathematics/Algebra"
        ],
        "abstract": "\nWe formalise Kleene algebra with tests (KAT) and demonic refinement\nalgebra (DRA) in Isabelle/HOL. KAT is relevant for program verification\nand correctness proofs in the partial correctness setting. While DRA\ntargets similar applications in the context of total correctness. Our\nformalisation contains the two most important models of these algebras:\nbinary relations in the case of KAT and predicate transformers in the\ncase of DRA. In addition, we derive the inference rules for Hoare logic\nin KAT and its relational model and present a simple formally verified\nprogram verification tool prototype based on the algebraic approach.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2014-01-29"
            }
        ],
        "dependencies": [
            "Kleene_Algebra"
        ],
        "theories": [
            "Test_Dioid",
            "Conway_Tests",
            "KAT",
            "DRAT",
            "DRA_Models",
            "KAT_Models",
            "FolkTheorem",
            "PHL_KAT",
            "PHL_DRAT",
            "KAT2",
            "DRAT2"
        ]
    },
    {
        "session": "Transition_Systems_and_Automata",
        "title": "Transition Systems and Automata",
        "authors": [
            "Julian Brunner"
        ],
        "topics": [
            "Computer science/Automata and formal languages"
        ],
        "date": "2017-10-19",
        "abstract": "\nThis entry provides a very abstract theory of transition systems that\ncan be instantiated to express various types of automata. A transition\nsystem is typically instantiated by providing a set of initial states,\na predicate for enabled transitions, and a transition execution\nfunction. From this, it defines the concepts of finite and infinite\npaths as well as the set of reachable states, among other things. Many\nuseful theorems, from basic path manipulation rules to coinduction and\nrun construction rules, are proven in this abstract transition system\ncontext. The library comes with instantiations for DFAs, NFAs, and\nBüchi automata.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-27"
            }
        ],
        "dependencies": [
            "Collections",
            "DFS_Framework",
            "Gabow_SCC"
        ],
        "theories": [
            "Basic",
            "Sequence",
            "Sequence_LTL",
            "Sequence_Zip",
            "Maps",
            "Acceptance",
            "Degeneralization",
            "Transition_System",
            "Transition_System_Extra",
            "Transition_System_Construction",
            "Deterministic",
            "DFA",
            "Nondeterministic",
            "NFA",
            "DBA",
            "DGBA",
            "DBA_Combine",
            "DBTA",
            "DGBTA",
            "DBTA_Combine",
            "DCA",
            "DGCA",
            "DCA_Combine",
            "DRA",
            "DRA_Combine",
            "Refine",
            "Acceptance_Refine",
            "Transition_System_Refine",
            "DRA_Refine",
            "Implement",
            "DRA_Implement",
            "DRA_Nodes",
            "DRA_Explicit",
            "DRA_Translate",
            "NBA",
            "NGBA",
            "NBA_Combine",
            "NBA_Graphs",
            "NBA_Refine",
            "NBA_Implement",
            "NBA_Algorithms",
            "NBA_Explicit",
            "NBA_Translate",
            "NGBA_Graphs",
            "NGBA_Refine",
            "NGBA_Implement",
            "Degeneralization_Refine",
            "NGBA_Algorithms",
            "NBTA",
            "NGBTA",
            "NBTA_Combine"
        ]
    },
    {
        "session": "Noninterference_Ipurge_Unwinding",
        "title": "The Ipurge Unwinding Theorem for CSP Noninterference Security",
        "authors": [
            "Pasquale Noce"
        ],
        "date": "2015-06-11",
        "topics": [
            "Computer science/Security"
        ],
        "abstract": "\n<p>\nThe definition of noninterference security for Communicating Sequential\nProcesses requires to consider any possible future, i.e. any indefinitely long\nsequence of subsequent events and any indefinitely large set of refused events\nassociated to that sequence, for each process trace. In order to render the\nverification of the security of a process more straightforward, there is a need\nof some sufficient condition for security such that just individual accepted and\nrefused events, rather than unbounded sequences and sets of events, have to be\nconsidered.\n</p><p>\nOf course, if such a sufficient condition were necessary as well, it would be\neven more valuable, since it would permit to prove not only that a process is\nsecure by verifying that the condition holds, but also that a process is not\nsecure by verifying that the condition fails to hold.\n</p><p>\nThis paper provides a necessary and sufficient condition for CSP noninterference\nsecurity, which indeed requires to just consider individual accepted and refused\nevents and applies to the general case of a possibly intransitive policy. This\ncondition follows Rushby's output consistency for deterministic state machines\nwith outputs, and has to be satisfied by a specific function mapping security\ndomains into equivalence relations over process traces. The definition of this\nfunction makes use of an intransitive purge function following Rushby's one;\nhence the name given to the condition, Ipurge Unwinding Theorem.\n</p><p>\nFurthermore, in accordance with Hoare's formal definition of deterministic\nprocesses, it is shown that a process is deterministic just in case it is a\ntrace set process, i.e. it may be identified by means of a trace set alone,\nmatching the set of its traces, in place of a failures-divergences pair. Then,\nvariants of the Ipurge Unwinding Theorem are proven for deterministic processes\nand trace set processes.\n</p>",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-06-13"
            }
        ],
        "dependencies": [
            "Noninterference_CSP",
            "List_Interleaving"
        ],
        "theories": [
            "IpurgeUnwinding",
            "DeterministicProcesses"
        ]
    },
    {
        "session": "Transcendence_Series_Hancl_Rucki",
        "title": "The Transcendence of Certain Infinite Series",
        "authors": [
            "Angeliki Koutsoukou-Argyraki",
            "Wenda Li"
        ],
        "topics": [
            "Mathematics/Analysis",
            "Mathematics/Number theory"
        ],
        "date": "2019-03-27",
        "abstract": "\nWe formalize the proofs of two transcendence criteria by J. Hančl\nand P. Rucki that assert the transcendence of the sums of certain\ninfinite series built up by sequences that fulfil certain properties.\nBoth proofs make use of Roth's celebrated theorem on diophantine\napproximations to algebraic numbers from 1955  which we implement as\nan assumption without having formalised its proof.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2019-03-28"
            }
        ],
        "dependencies": [
            "Prime_Number_Theorem"
        ],
        "theories": [
            "Transcendence_Series"
        ]
    },
    {
        "session": "Binomial-Heaps",
        "title": "Binomial Heaps and Skew Binomial Heaps",
        "authors": [
            "Rene Meis",
            "Finn Nielsen",
            "Peter Lammich"
        ],
        "date": "2010-10-28",
        "topics": [
            "Computer science/Data structures"
        ],
        "abstract": "\nWe implement and prove correct binomial heaps and skew binomial heaps.\nBoth are data-structures for priority queues.\nWhile binomial heaps have logarithmic <em>findMin</em>, <em>deleteMin</em>,\n<em>insert</em>, and <em>meld</em> operations,\nskew binomial heaps have constant time <em>findMin</em>, <em>insert</em>,\nand <em>meld</em> operations, and only the <em>deleteMin</em>-operation is\nlogarithmic. This is achieved by using <em>skew links</em> to avoid\ncascading linking on <em>insert</em>-operations, and <em>data-structural\nbootstrapping</em> to get constant-time <em>findMin</em> and <em>meld</em>\noperations.  Our implementation follows the paper by Brodal and Okasaki.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-03-02"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-24"
            },
            {
                "2011-1": "2011-10-11"
            },
            {
                "2011": "2011-02-11"
            },
            {
                "2009-2": "2010-10-28"
            }
        ],
        "theories": [
            "BinomialHeap",
            "SkewBinomialHeap",
            "Test"
        ]
    },
    {
        "session": "Goodstein_Lambda",
        "title": "Implementing the Goodstein Function in &lambda;-Calculus",
        "authors": [
            "Bertram Felgenhauer"
        ],
        "topics": [
            "Logic/Rewriting"
        ],
        "date": "2020-02-21",
        "abstract": "\nIn this formalization, we develop an implementation of the Goodstein\nfunction G in plain &lambda;-calculus, linked to a concise, self-contained\nspecification. The implementation works on a Church-encoded\nrepresentation of countable ordinals. The initial conversion to\nhereditary base 2 is not covered, but the material is sufficient to\ncompute the particular value G(16), and easily extends to other fixed\narguments.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2020-02-24"
            }
        ],
        "theories": [
            "Goodstein_Lambda"
        ]
    },
    {
        "session": "Rank_Nullity_Theorem",
        "title": "Rank-Nullity Theorem in Linear Algebra",
        "authors": [
            "Jose Divasón",
            "Jesús Aransay"
        ],
        "topics": [
            "Mathematics/Algebra"
        ],
        "date": "2013-01-16",
        "abstract": "In this contribution, we present some formalizations based on the HOL-Multivariate-Analysis session of Isabelle. Firstly, a generalization of several theorems of such library are presented. Secondly, some definitions and proofs involving Linear Algebra and the four fundamental subspaces of a matrix are shown. Finally, we present a proof of the result known in Linear Algebra as the ``Rank-Nullity Theorem'', which states that, given any linear map f from a finite dimensional vector space V to a vector space W, then the dimension of V is equal to the dimension of the kernel of f (which is a subspace of V) and the dimension of the range of f (which is a subspace of W). The proof presented here is based on the one given by Sheldon Axler in his book <i>Linear Algebra Done Right</i>. As a corollary of the previous theorem, and taking advantage of the relationship between linear maps and matrices, we prove that, for every matrix A (which has associated a linear map between finite dimensional vector spaces), the sum of its null space and its column space (which is equal to the range of the linear map) is equal to the number of columns of A.",
        "extra": {
            "Change history": "[2014-07-14] Added some generalizations that allow us to formalize the Rank-Nullity Theorem over finite dimensional vector spaces, instead of over the more particular euclidean spaces. Updated abstract."
        },
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2013-01-16"
            }
        ],
        "theories": [
            "Dual_Order",
            "Mod_Type",
            "Miscellaneous",
            "Fundamental_Subspaces",
            "Dim_Formula"
        ]
    },
    {
        "session": "Affine_Arithmetic",
        "title": "Affine Arithmetic",
        "authors": [
            "Fabian Immler"
        ],
        "date": "2014-02-07",
        "topics": [
            "Mathematics/Analysis"
        ],
        "abstract": "\nWe give a formalization of affine forms as abstract representations of zonotopes.\nWe provide affine operations as well as overapproximations of some non-affine operations like multiplication and division.\nExpressions involving those operations can automatically be turned into (executable) functions approximating the original\nexpression in affine arithmetic.",
        "extra": {
            "Change history": "[2015-01-31] added algorithm for zonotope/hyperplane intersection<br>\n[2017-09-20] linear approximations for all symbols from the floatarith data\ntype"
        },
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            }
        ],
        "dependencies": [
            "List-Index",
            "Deriving",
            "Show"
        ],
        "theories": [
            "Affine_Arithmetic_Auxiliarities",
            "Executable_Euclidean_Space",
            "Affine_Form",
            "Floatarith_Expression",
            "Straight_Line_Program",
            "Affine_Approximation",
            "Counterclockwise",
            "Counterclockwise_Vector",
            "Counterclockwise_2D_Strict",
            "Polygon",
            "Counterclockwise_2D_Arbitrary",
            "Intersection",
            "Affine_Code",
            "Optimize_Integer",
            "Optimize_Float",
            "Print",
            "Float_Real",
            "Ex_Affine_Approximation",
            "Ex_Ineqs",
            "Ex_Inter",
            "Affine_Arithmetic"
        ]
    },
    {
        "session": "Prim_Dijkstra_Simple",
        "title": "Purely Functional, Simple, and Efficient Implementation of Prim and Dijkstra",
        "authors": [
            "Peter Lammich",
            "Tobias Nipkow"
        ],
        "topics": [
            "Computer science/Algorithms/Graph"
        ],
        "date": "2019-06-25",
        "abstract": "\nWe verify purely functional, simple and efficient implementations of\nPrim's and Dijkstra's algorithms. This constitutes the first\nverification of an executable and even efficient version of\nPrim's algorithm. This entry formalizes the second part of our\nITP-2019 proof pearl <em>Purely Functional, Simple and Efficient\nPriority Search Trees and Applications to Prim and Dijkstra</em>.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-29"
            }
        ],
        "dependencies": [
            "Priority_Search_Trees"
        ],
        "theories": [
            "Common",
            "Chapter_Prim",
            "Undirected_Graph",
            "Undirected_Graph_Specs",
            "Prim_Abstract",
            "Undirected_Graph_Impl",
            "Prim_Impl",
            "Chapter_Dijkstra",
            "Directed_Graph",
            "Directed_Graph_Specs",
            "Dijkstra_Abstract",
            "Directed_Graph_Impl",
            "Dijkstra_Impl"
        ]
    },
    {
        "session": "BTree",
        "title": "A Verified Imperative Implementation of B-Trees",
        "authors": [
            "Niels Mündler"
        ],
        "topics": [
            "Computer science/Data structures"
        ],
        "date": "2021-02-24",
        "abstract": "\nIn this work, we use the interactive theorem prover Isabelle/HOL to\nverify an imperative implementation of the classical B-tree data\nstructure invented by Bayer and McCreight [ACM 1970]. The\nimplementation supports set membership and insertion queries with\nefficient binary search for intra-node navigation. This is\naccomplished by first specifying the structure abstractly in the\nfunctional modeling language HOL and proving functional correctness.\nUsing manual refinement, we derive an imperative implementation in\nImperative/HOL. We show the validity of this refinement using the\nseparation logic utilities from the <a\nhref=\"https://www.isa-afp.org/entries/Refine_Imperative_HOL.html\">\nIsabelle Refinement Framework </a> . The code can be exported to\nthe programming languages SML and Scala. We examine the runtime of all\noperations indirectly by reproducing results of the logarithmic\nrelationship between height and the number of nodes.  The results are\ndiscussed in greater detail in the corresponding <a\nhref=\"https://mediatum.ub.tum.de/1596550\">Bachelor's\nThesis</a>.",
        "licence": "BSD",
        "dependencies": [
            "Refine_Imperative_HOL"
        ],
        "theories": [
            "BTree",
            "BTree_Height",
            "BTree_Set",
            "BTree_Split",
            "Array_SBlit",
            "Partially_Filled_Array",
            "Basic_Assn",
            "BTree_Imp",
            "BTree_ImpSet",
            "Imperative_Loops",
            "BTree_ImpSplit"
        ]
    },
    {
        "session": "Flow_Networks",
        "title": "Flow Networks and the Min-Cut-Max-Flow Theorem",
        "authors": [
            "Peter Lammich",
            "S. Reza Sefidgar"
        ],
        "topics": [
            "Mathematics/Graph theory"
        ],
        "date": "2017-06-01",
        "abstract": "\nWe present a formalization of flow networks and the Min-Cut-Max-Flow\ntheorem. Our formal proof closely follows a standard textbook proof,\nand is accessible even without being an expert in Isabelle/HOL, the\ninteractive theorem prover used for the formalization.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2020-01-14"
            },
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2017-06-02"
            }
        ],
        "dependencies": [
            "Sepref_IICF",
            "Program-Conflict-Analysis",
            "CAVA_Automata",
            "DFS_Framework",
            "Refine_Imperative_HOL"
        ],
        "theories": [
            "Fofu_Abs_Base",
            "Fofu_Impl_Base",
            "Refine_Add_Fofu",
            "Graph",
            "Network",
            "Residual_Graph",
            "Augmenting_Flow",
            "Augmenting_Path",
            "Ford_Fulkerson",
            "Graph_Impl",
            "Network_Impl",
            "NetCheck"
        ]
    },
    {
        "session": "Prpu_Maxflow",
        "title": "Formalizing Push-Relabel Algorithms",
        "authors": [
            "Peter Lammich",
            "S. Reza Sefidgar"
        ],
        "topics": [
            "Computer science/Algorithms/Graph",
            "Mathematics/Graph theory"
        ],
        "date": "2017-06-01",
        "abstract": "\nWe present a formalization of push-relabel algorithms for computing\nthe maximum flow in a network. We start with Goldberg's et\nal.~generic push-relabel algorithm, for which we show correctness and\nthe time complexity bound of O(V^2E). We then derive the\nrelabel-to-front and FIFO implementation. Using stepwise refinement\ntechniques, we derive an efficient verified implementation.  Our\nformal proof of the abstract algorithms closely follows a standard\ntextbook proof. It is accessible even without being an expert in\nIsabelle/HOL, the interactive theorem prover used for the\nformalization.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2020-01-14"
            },
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2017-06-02"
            }
        ],
        "dependencies": [
            "Flow_Networks"
        ],
        "theories": [
            "Generic_Push_Relabel",
            "Prpu_Common_Inst",
            "Fifo_Push_Relabel",
            "Graph_Topological_Ordering",
            "Relabel_To_Front",
            "Prpu_Common_Impl",
            "Fifo_Push_Relabel_Impl",
            "Relabel_To_Front_Impl",
            "Generated_Code_Test"
        ]
    },
    {
        "session": "List-Index",
        "title": "List Index",
        "date": "2010-02-20",
        "authors": [
            "Tobias Nipkow"
        ],
        "topics": [
            "Computer science/Data structures"
        ],
        "abstract": "This theory provides functions for finding the index of an element in a list, by predicate and by value.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-24"
            },
            {
                "2011-1": "2011-10-11"
            },
            {
                "2011": "2011-02-11"
            },
            {
                "2009-2": "2010-07-01"
            },
            {
                "2009-1": "2010-02-20"
            }
        ],
        "theories": [
            "List_Index"
        ]
    },
    {
        "session": "Linear_Programming",
        "title": "Linear Programming",
        "authors": [
            "Julian Parsert",
            "Cezary Kaliszyk"
        ],
        "topics": [
            "Mathematics/Algebra"
        ],
        "date": "2019-08-06",
        "abstract": "\nWe use the previous formalization of the general simplex algorithm to\nformulate an algorithm for solving linear programs. We encode the\nlinear programs using only linear constraints. Solving these\nconstraints also solves the original linear program. This algorithm is\nproven to be sound by applying the weak duality theorem which is also\npart of this formalization.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-09-23"
            }
        ],
        "dependencies": [
            "Jordan_Normal_Form",
            "Farkas",
            "Linear_Inequalities"
        ],
        "theories": [
            "More_Jordan_Normal_Forms",
            "Matrix_LinPoly",
            "LP_Preliminaries",
            "Linear_Programming"
        ]
    },
    {
        "session": "Ergodic_Theory",
        "title": "Ergodic Theory",
        "authors": [
            "Sebastien Gouezel"
        ],
        "date": "2015-12-01",
        "topics": [
            "Mathematics/Probability theory"
        ],
        "abstract": "Ergodic theory is the branch of mathematics that studies the behaviour of measure preserving transformations, in finite or infinite measure. It interacts both with probability theory (mainly through measure theory) and with geometry as a lot of interesting examples are from geometric origin. We implement the first definitions and theorems of ergodic theory, including notably Poicaré recurrence theorem for finite measure preserving systems (together with the notion of conservativity in general), induced maps, Kac's theorem, Birkhoff theorem (arguably the most important theorem in ergodic theory), and variations around it such as conservativity of the corresponding skew product, or Atkinson lemma.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            }
        ],
        "theories": [
            "SG_Library_Complement",
            "ME_Library_Complement",
            "Fekete",
            "Asymptotic_Density",
            "Measure_Preserving_Transformations",
            "Recurrence",
            "Invariants",
            "Ergodicity",
            "Shift_Operator",
            "Kingman",
            "Gouezel_Karlsson",
            "Kohlberg_Neyman_Karlsson",
            "Transfer_Operator",
            "Normalizing_Sequences"
        ],
        "contributors": [
            "Manuel Eberl"
        ]
    },
    {
        "session": "Concurrent_Revisions",
        "title": "Formalization of Concurrent Revisions",
        "authors": [
            "Roy Overbeek"
        ],
        "topics": [
            "Computer science/Concurrency"
        ],
        "date": "2018-12-25",
        "abstract": "\nConcurrent revisions is a concurrency control model developed by\nMicrosoft Research. It has many interesting properties that\ndistinguish it from other well-known models such as transactional\nmemory. One of these properties is <em>determinacy</em>:\nprograms written within the model always produce the same outcome,\nindependent of scheduling activity. The concurrent revisions model has\nan operational semantics, with an informal proof of determinacy. This\ndocument contains an Isabelle/HOL formalization of this semantics and\nthe proof of determinacy.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2019-01-03"
            }
        ],
        "theories": [
            "Data",
            "Occurrences",
            "Renaming",
            "Substitution",
            "OperationalSemantics",
            "Executions",
            "Determinacy"
        ]
    },
    {
        "session": "Refine_Monadic",
        "title": "Refinement for Monadic Programs",
        "authors": [
            "Peter Lammich"
        ],
        "topics": [
            "Computer science/Programming languages/Logics"
        ],
        "date": "2012-01-30",
        "abstract": "We provide a framework for program and data refinement in Isabelle/HOL.\nThe framework is based on a nondeterminism-monad with assertions, i.e.,\nthe monad carries a set of results or an assertion failure.\nRecursion is expressed by fixed points. For convenience, we also provide\nwhile and foreach combinators.\n<p>\nThe framework provides tools to automatize canonical tasks, such as\nverification condition generation, finding appropriate data refinement relations,\nand refine an executable program to a form that is accepted by the\nIsabelle/HOL code generator.\n<p>\nThis submission comes with a collection of examples and a user-guide,\nillustrating the usage of the framework.",
        "extra": {
            "Change history": "[2012-04-23] Introduced ordered FOREACH loops<br>\n[2012-06] New features\nREC_rule_arb and RECT_rule_arb allow for generalizing over variables.\nprepare_code_thms - command extracts code equations for recursion combinators.<br>\n[2012-07] New example Nested DFS for emptiness check of Buchi-automata with witness.<br>\nNew feature\nfo_rule method to apply resolution using first-order matching. Useful for arg_conf, fun_cong.<br>\n[2012-08] Adaptation to ICF v2.<br>\n[2012-10-05] Adaptations to include support for Automatic Refinement Framework.<br>\n[2013-09] This entry now depends on Automatic Refinement<br>\n[2014-06] New feature vc_solve method to solve verification conditions.\nMaintenace changes VCG-rules for nfoldli, improved setup for FOREACH-loops.<br>\n[2014-07] Now defining recursion via flat domain. Dropped many single-valued prerequisites.\nChanged notion of data refinement. In single-valued case, this matches the old notion.\nIn non-single valued case, the new notion allows for more convenient rules.\nIn particular, the new definitions allow for projecting away ghost variables as a refinement step.<br>\n[2014-11] New features le-or-fail relation (leof), modular reasoning about loop invariants."
        },
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-24"
            },
            {
                "2011-1": "2012-02-10"
            }
        ],
        "dependencies": [
            "Automatic_Refinement"
        ],
        "theories": [
            "Refine_Chapter",
            "Refine_Mono_Prover",
            "files/refine_mono_prover.ML",
            "Refine_Misc",
            "RefineG_Transfer",
            "RefineG_Domain",
            "RefineG_Recursion",
            "RefineG_Assert",
            "Refine_Basic",
            "Refine_Leof",
            "Refine_Heuristics",
            "Refine_More_Comb",
            "RefineG_While",
            "Refine_While",
            "Refine_Det",
            "Refine_Pfun",
            "Refine_Transfer",
            "Refine_Foreach",
            "Refine_Automation",
            "Autoref_Monadic",
            "Refine_Monadic",
            "Example_Chapter",
            "Breadth_First_Search",
            "WordRefine",
            "Examples"
        ]
    },
    {
        "session": "Free-Groups",
        "title": "Free Groups",
        "authors": [
            "Joachim Breitner"
        ],
        "date": "2010-06-24",
        "topics": [
            "Mathematics/Algebra"
        ],
        "abstract": "\nFree Groups are, in a sense, the most generic kind of group. They\nare defined over a set of generators with no additional relations in between\nthem. They play an important role in the definition of group presentations\nand in other fields. This theory provides the definition of Free Group as\nthe set of fully canceled words in the generators. The universal property is\nproven, as well as some isomorphisms results about Free Groups.",
        "extra": {
            "Change history": "[2011-12-11] Added the Ping Pong Lemma."
        },
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-24"
            },
            {
                "2011-1": "2011-10-11"
            },
            {
                "2011": "2011-02-11"
            },
            {
                "2009-2": "2010-07-01"
            }
        ],
        "dependencies": [
            "Applicative_Lifting"
        ],
        "theories": [
            "Cancelation",
            "Generators",
            "FreeGroups",
            "UnitGroup",
            "C2",
            "Isomorphisms",
            "PingPongLemma"
        ]
    },
    {
        "session": "Integration",
        "title": "Integration theory and random variables",
        "authors": [
            "Stefan Richter"
        ],
        "date": "2004-11-19",
        "topics": [
            "Mathematics/Analysis"
        ],
        "abstract": "Lebesgue-style integration plays a major role in advanced probability. We formalize concepts of elementary measure theory, real-valued random variables as Borel-measurable functions, and a stepwise inductive definition of the integral itself. All proofs are carried out in human readable style using the Isar language.",
        "extra": {
            "Note": "This article is of historical interest only. Lebesgue-style integration and probability theory are now available as part of the Isabelle/HOL distribution (directory Probability)."
        },
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-24"
            },
            {
                "2011-1": "2011-10-11"
            },
            {
                "2011": "2011-02-11"
            },
            {
                "2009-2": "2010-07-01"
            },
            {
                "2009-1": "2009-12-12"
            },
            {
                "2009": "2009-04-29"
            },
            {
                "2008": "2008-06-10"
            },
            {
                "2007": "2007-11-27"
            },
            {
                "2005": "2005-10-14"
            },
            {
                "2004": "2004-11-23"
            },
            {
                "2004": "2004-11-22"
            }
        ],
        "theories": [
            "Sigma_Algebra",
            "MonConv",
            "Measure",
            "RealRandVar",
            "Integral",
            "Failure"
        ]
    },
    {
        "session": "HOL-CSP",
        "title": "HOL-CSP Version 2.0",
        "authors": [
            "Safouan Taha",
            "Lina Ye",
            "Burkhart Wolff"
        ],
        "topics": [
            "Computer science/Concurrency/Process calculi",
            "Computer science/Semantics"
        ],
        "date": "2019-04-26",
        "abstract": "\nThis is a complete formalization of the work of Hoare and Roscoe on\nthe denotational semantics of the Failure/Divergence Model of CSP. It\nfollows essentially the presentation of CSP in Roscoe’s Book ”Theory\nand Practice of Concurrency” [8] and the semantic details in a joint\nPaper of Roscoe and Brooks ”An improved failures model for\ncommunicating processes\".  The present work is based on a prior\nformalization attempt, called HOL-CSP 1.0, done in 1997 by H. Tej and\nB. Wolff with the Isabelle proof technology available at that time.\nThis work revealed minor, but omnipresent foundational errors in key\nconcepts like the process invariant. The present version HOL-CSP\nprofits from substantially improved libraries (notably HOLCF),\nimproved automated proof techniques, and structured proof techniques\nin Isar and is substantially shorter but more complete.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2019-04-29"
            }
        ],
        "theories": [
            "Introduction",
            "Process",
            "Bot",
            "Skip",
            "Stop",
            "Mprefix",
            "Det",
            "Ndet",
            "Seq",
            "Hide",
            "Sync",
            "Mndet",
            "CSP",
            "Assertions",
            "Conclusion",
            "CopyBuffer"
        ]
    },
    {
        "session": "Localization_Ring",
        "title": "The Localization of a Commutative Ring",
        "authors": [
            "Anthony Bordg"
        ],
        "topics": [
            "Mathematics/Algebra"
        ],
        "date": "2018-06-14",
        "abstract": "\nWe formalize the localization of a commutative ring R with respect to\na multiplicative subset (i.e. a submonoid of R seen as a\nmultiplicative monoid). This localization is itself a commutative ring\nand we build the natural homomorphism of rings from R to its\nlocalization.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2018-06-17"
            }
        ],
        "theories": [
            "Localization"
        ]
    },
    {
        "session": "AODV",
        "title": "Loop freedom of the (untimed) AODV routing protocol",
        "authors": [
            "Timothy Bourke",
            "Peter Höfner"
        ],
        "date": "2014-10-23",
        "topics": [
            "Computer science/Concurrency/Process calculi"
        ],
        "abstract": "\n<p>\nThe Ad hoc On-demand Distance Vector (AODV) routing protocol allows\nthe nodes in a Mobile Ad hoc Network (MANET) or a Wireless Mesh\nNetwork (WMN) to know where to forward data packets. Such a protocol\nis ‘loop free’ if it never leads to routing decisions that forward\npackets in circles.\n<p>\nThis development mechanises an existing pen-and-paper proof of loop\nfreedom of AODV. The protocol is modelled in the Algebra of\nWireless Networks (AWN), which is the subject of an earlier paper\nand AFP mechanization. The proof relies on a novel compositional\napproach for lifting invariants to networks of nodes.\n</p><p>\nWe exploit the mechanization to analyse several variants of AODV and\nshow that Isabelle/HOL can re-establish most proof obligations\nautomatically and identify exactly the steps that are no longer valid.\n</p>",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-11-03"
            },
            {
                "2014": "2014-11-01"
            }
        ],
        "dependencies": [
            "AWN"
        ],
        "theories": [
            "Aodv_Basic",
            "Aodv_Data",
            "Aodv_Message",
            "Aodv",
            "Aodv_Predicates",
            "Fresher",
            "Seq_Invariants",
            "Quality_Increases",
            "OAodv",
            "Global_Invariants",
            "Loop_Freedom",
            "Aodv_Loop_Freedom",
            "A_Norreqid",
            "A_Aodv_Data",
            "A_Aodv_Message",
            "A_Aodv",
            "A_Aodv_Predicates",
            "A_Fresher",
            "A_Seq_Invariants",
            "A_Quality_Increases",
            "A_OAodv",
            "A_Global_Invariants",
            "A_Loop_Freedom",
            "A_Aodv_Loop_Freedom",
            "B_Fwdrreps",
            "B_Aodv_Data",
            "B_Aodv_Message",
            "B_Aodv",
            "B_Aodv_Predicates",
            "B_Fresher",
            "B_Seq_Invariants",
            "B_Quality_Increases",
            "B_OAodv",
            "B_Global_Invariants",
            "B_Loop_Freedom",
            "B_Aodv_Loop_Freedom",
            "C_Gtobcast",
            "C_Aodv_Data",
            "C_Aodv_Message",
            "C_Aodv",
            "C_Aodv_Predicates",
            "C_Fresher",
            "C_Seq_Invariants",
            "C_Quality_Increases",
            "C_OAodv",
            "C_Global_Invariants",
            "C_Loop_Freedom",
            "C_Aodv_Loop_Freedom",
            "D_Fwdrreqs",
            "D_Aodv_Data",
            "D_Aodv_Message",
            "D_Aodv",
            "D_Aodv_Predicates",
            "D_Fresher",
            "D_Seq_Invariants",
            "D_Quality_Increases",
            "D_OAodv",
            "D_Global_Invariants",
            "D_Loop_Freedom",
            "D_Aodv_Loop_Freedom",
            "E_All_ABCD",
            "E_Aodv_Data",
            "E_Aodv_Message",
            "E_Aodv",
            "E_Aodv_Predicates",
            "E_Fresher",
            "E_Seq_Invariants",
            "E_Quality_Increases",
            "E_OAodv",
            "E_Global_Invariants",
            "E_Loop_Freedom",
            "E_Aodv_Loop_Freedom",
            "All"
        ]
    },
    {
        "session": "Knuth_Bendix_Order",
        "title": "A Formalization of Knuth–Bendix Orders",
        "authors": [
            "Christian Sternagel",
            "René Thiemann"
        ],
        "topics": [
            "Logic/Rewriting"
        ],
        "date": "2020-05-13",
        "abstract": "\nWe define a generalized version of Knuth&ndash;Bendix orders,\nincluding subterm coefficient functions. For these orders we formalize\nseveral properties such as strong normalization, the subterm property,\nclosure properties under substitutions and contexts, as well as ground\ntotality.",
        "licence": "BSD",
        "dependencies": [
            "First_Order_Terms",
            "Matrix",
            "Polynomial_Factorization"
        ],
        "theories": [
            "Order_Pair",
            "Lexicographic_Extension",
            "Subterm_and_Context",
            "Term_Aux",
            "KBO"
        ]
    },
    {
        "session": "Green",
        "title": "An Isabelle/HOL formalisation of Green's Theorem",
        "authors": [
            "Mohammad Abdulaziz",
            "Lawrence C. Paulson"
        ],
        "topics": [
            "Mathematics/Analysis"
        ],
        "date": "2018-01-11",
        "abstract": "\nWe formalise a statement of Green’s theorem—the first formalisation to\nour knowledge—in Isabelle/HOL. The theorem statement that we formalise\nis enough for most applications, especially in physics and\nengineering. Our formalisation is made possible by a novel proof that\navoids the ubiquitous line integral cancellation argument. This\neliminates the need to formalise orientations and region boundaries\nexplicitly with respect to the outwards-pointing normal vector.\nInstead we appeal to a homological argument about equivalences between\npaths.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2018-01-12"
            }
        ],
        "theories": [
            "General_Utils",
            "Derivs",
            "Integrals",
            "Paths",
            "Green",
            "SymmetricR2Shapes",
            "CircExample",
            "DiamExample"
        ]
    },
    {
        "session": "Abstract_Soundness",
        "title": "Abstract Soundness",
        "authors": [
            "Jasmin Christian Blanchette",
            "Andrei Popescu",
            "Dmitriy Traytel"
        ],
        "topics": [
            "Logic/Proof theory"
        ],
        "date": "2017-02-10",
        "abstract": "\nA formalized coinductive account of the abstract development of\nBrotherston, Gorogiannis, and Petersen [APLAS 2012], in a slightly\nmore general form since we work with arbitrary infinite proofs, which\nmay be acyclic. This work is described in detail in an article by the\nauthors, published in 2017 in the <em>Journal of Automated\nReasoning</em>. The abstract proof can be instantiated for\nvarious formalisms, including first-order logic with inductive\npredicates.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2017-02-13"
            }
        ],
        "dependencies": [
            "Abstract_Completeness"
        ],
        "theories": [
            "Finite_Proof_Soundness",
            "Infinite_Proof_Soundness"
        ]
    },
    {
        "session": "Encodability_Process_Calculi",
        "title": "Analysing and Comparing Encodability Criteria for Process Calculi",
        "authors": [
            "Kirstin Peters",
            "Rob van Glabbeek"
        ],
        "date": "2015-08-10",
        "topics": [
            "Computer science/Concurrency/Process calculi"
        ],
        "abstract": "Encodings or the proof of their absence are the main way to\ncompare process calculi. To analyse the quality of encodings and to rule out\ntrivial or meaningless encodings, they are augmented with quality\ncriteria. There exists a bunch of different criteria and different variants\nof criteria in order to reason in different settings. This leads to\nincomparable results. Moreover it is not always clear whether the criteria\nused to obtain a result in a particular setting do indeed fit to this\nsetting. We show how to formally reason about and compare encodability\ncriteria by mapping them on requirements on a relation between source and\ntarget terms that is induced by the encoding function. In particular we\nanalyse the common criteria full abstraction, operational correspondence,\ndivergence reflection, success sensitiveness, and respect of barbs; e.g. we\nanalyse the exact nature of the simulation relation (coupled simulation\nversus bisimulation) that is induced by different variants of operational\ncorrespondence. This way we reduce the problem of analysing or comparing\nencodability criteria to the better understood problem of comparing\nrelations on processes.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-08-11"
            }
        ],
        "theories": [
            "Relations",
            "ProcessCalculi",
            "SimulationRelations",
            "Encodings",
            "SourceTargetRelation",
            "SuccessSensitiveness",
            "DivergenceReflection",
            "OperationalCorrespondence",
            "FullAbstraction",
            "CombinedCriteria"
        ]
    },
    {
        "session": "Category",
        "title": "Category Theory to Yoneda's Lemma",
        "authors": [
            "Greg O'Keefe"
        ],
        "date": "2005-04-21",
        "topics": [
            "Mathematics/Category theory"
        ],
        "license": "LGPL",
        "abstract": "This development proves Yoneda's lemma and aims to be readable by humans. It only defines what is needed for the lemma: categories, functors and natural transformations. Limits, adjunctions and other important concepts are not included.",
        "extra": {
            "Change history": "[2010-04-23] The definition of the constant <tt>equinumerous</tt> was slightly too weak in the original submission and has been fixed in revision <a href=\"https//foss.heptapod.net/isa-afp/afp-devel/-/commit/3498bb1e4c7ba468db8588eb7184c1849641f7d3\">8c2b5b3c995f</a>."
        },
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-03-02"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-24"
            },
            {
                "2011-1": "2011-10-11"
            },
            {
                "2011": "2011-02-11"
            },
            {
                "2009-2": "2010-06-30"
            },
            {
                "2009-1": "2009-12-12"
            },
            {
                "2009": "2009-04-29"
            },
            {
                "2008": "2008-06-10"
            },
            {
                "2007": "2007-11-27"
            },
            {
                "2005": "2005-10-14"
            },
            {
                "2004": "2005-05-01"
            },
            {
                "2004": "2005-04-21"
            }
        ],
        "theories": [
            "Cat",
            "SetCat",
            "Functors",
            "HomFunctors",
            "NatTrans",
            "Yoneda"
        ]
    },
    {
        "session": "Complx",
        "title": "COMPLX: A Verification Framework for Concurrent Imperative Programs",
        "authors": [
            "Sidney Amani",
            "June Andronick",
            "Maksym Bortin",
            "Corey Lewis",
            "Christine Rizkallah",
            "Joseph Tuong"
        ],
        "date": "2016-11-29",
        "topics": [
            "Computer science/Programming languages/Logics",
            "Computer science/Programming languages/Language definitions"
        ],
        "abstract": "\nWe propose a concurrency reasoning framework for imperative programs,\nbased on the Owicki-Gries (OG) foundational shared-variable\nconcurrency method. Our framework combines the approaches of\nHoare-Parallel, a formalisation of OG in Isabelle/HOL for a simple\nwhile-language, and Simpl, a generic imperative language embedded in\nIsabelle/HOL, allowing formal reasoning on C programs. We define the\nComplx language, extending the syntax and semantics of Simpl with\nsupport for parallel composition and synchronisation. We additionally\ndefine an OG logic, which we prove sound w.r.t. the  semantics, and a\nverification condition generator, both supporting involved low-level\nimperative constructs such as function calls and abrupt termination.\nWe illustrate our framework on an example that features exceptions,\nguards and function calls.  We aim to then target concurrent operating\nsystems, such as the interruptible eChronos embedded operating system\nfor which we already have a model-level OG proof using Hoare-Parallel.",
        "extra": {
            "Change history": "[2017-01-13]\nImprove VCG for nested parallels and sequential sections\n(revision 30739dbc3dcb)"
        },
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-11-29"
            }
        ],
        "dependencies": [
            "Word_Lib"
        ],
        "theories": [
            "Language",
            "SmallStep",
            "OG_Annotations",
            "OG_Hoare",
            "SeqCatch_decomp",
            "OG_Soundness",
            "Cache_Tactics",
            "OG_Tactics",
            "OG_Syntax",
            "Examples",
            "SumArr"
        ]
    },
    {
        "session": "NormByEval",
        "title": "Normalization by Evaluation",
        "authors": [
            "Klaus Aehlig",
            "Tobias Nipkow"
        ],
        "date": "2008-02-18",
        "topics": [
            "Computer science/Programming languages/Compiling"
        ],
        "abstract": "This article formalizes normalization by evaluation as implemented in Isabelle. Lambda calculus plus term rewriting is compiled into a functional program with pattern matching. It is proved that the result of a successful evaluation is a) correct, i.e. equivalent to the input, and b) in normal form.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-24"
            },
            {
                "2011-1": "2011-10-11"
            },
            {
                "2011": "2011-02-11"
            },
            {
                "2009-2": "2010-07-01"
            },
            {
                "2009-1": "2009-12-12"
            },
            {
                "2009": "2009-04-29"
            },
            {
                "2008": "2008-06-10"
            },
            {
                "2007": "2008-02-22"
            },
            {
                "2007": "2008-02-18"
            }
        ],
        "theories": [
            "NBE"
        ]
    },
    {
        "session": "Probabilistic_Noninterference",
        "title": "Probabilistic Noninterference",
        "authors": [
            "Andrei Popescu",
            "Johannes Hölzl"
        ],
        "date": "2014-03-11",
        "topics": [
            "Computer science/Security"
        ],
        "abstract": "We formalize a probabilistic noninterference for a multi-threaded language with uniform scheduling, where probabilistic behaviour comes from both the scheduler and the individual threads. We define notions probabilistic noninterference in two variants: resumption-based and trace-based. For the resumption-based notions, we prove compositionality w.r.t. the language constructs and establish sound type-system-like syntactic criteria. This is a formalization of the mathematical development presented at CPP 2013 and CALCO 2013. It is the probabilistic variant of the Possibilistic Noninterference AFP entry.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2014-03-16"
            }
        ],
        "dependencies": [
            "Coinductive",
            "Markov_Models"
        ],
        "theories": [
            "Interface",
            "Language_Semantics",
            "Resumption_Based",
            "Trace_Based",
            "Compositionality",
            "Syntactic_Criteria",
            "Concrete"
        ]
    },
    {
        "session": "BDD",
        "title": "BDD Normalisation",
        "authors": [
            "Veronika Ortner",
            "Norbert Schirmer"
        ],
        "date": "2008-02-29",
        "topics": [
            "Computer science/Data structures"
        ],
        "abstract": "We present the verification of the normalisation of a binary decision diagram (BDD). The normalisation follows the original algorithm presented by Bryant in 1986 and transforms an ordered BDD in a reduced, ordered and shared BDD. The verification is based on Hoare logics.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-24"
            },
            {
                "2011-1": "2011-10-11"
            },
            {
                "2011": "2011-02-11"
            },
            {
                "2009-2": "2010-06-30"
            },
            {
                "2009-1": "2009-12-12"
            },
            {
                "2009": "2009-04-29"
            },
            {
                "2008": "2008-06-10"
            },
            {
                "2007": "2008-03-07"
            }
        ],
        "dependencies": [
            "Simpl"
        ],
        "theories": [
            "BinDag",
            "General",
            "ProcedureSpecs",
            "EvalProof",
            "LevellistProof",
            "ShareRepProof",
            "ShareReduceRepListProof",
            "RepointProof",
            "NormalizeTotalProof"
        ]
    },
    {
        "session": "Aggregation_Algebras",
        "title": "Aggregation Algebras",
        "authors": [
            "Walter Guttmann"
        ],
        "topics": [
            "Mathematics/Algebra"
        ],
        "date": "2018-09-15",
        "abstract": "\nWe develop algebras for aggregation and minimisation for weight\nmatrices and for edge weights in graphs. We verify the correctness of\nPrim's and Kruskal's minimum spanning tree algorithms based\non these algebras. We also show numerous instances of these algebras\nbased on linearly ordered commutative semigroups.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-09-16"
            }
        ],
        "dependencies": [
            "Stone_Kleene_Relation_Algebras"
        ],
        "theories": [
            "Semigroups_Big",
            "Aggregation_Algebras",
            "Matrix_Aggregation_Algebras",
            "Linear_Aggregation_Algebras"
        ]
    },
    {
        "session": "IMO2019",
        "title": "Selected Problems from the International Mathematical Olympiad 2019",
        "authors": [
            "Manuel Eberl"
        ],
        "topics": [
            "Mathematics/Misc"
        ],
        "date": "2019-08-05",
        "abstract": "\n<p>This entry contains formalisations of the answers to three of\nthe six problem of the International Mathematical Olympiad 2019,\nnamely Q1, Q4, and Q5.</p> <p>The reason why these\nproblems were chosen is that they are particularly amenable to\nformalisation: they can be solved with minimal use of libraries. The\nremaining three concern geometry and graph theory, which, in the\nauthor's opinion, are more difficult to formalise resp. require a\nmore complex library.</p>",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-08-06"
            }
        ],
        "dependencies": [
            "Prime_Distribution_Elementary"
        ],
        "theories": [
            "IMO2019_Q1",
            "IMO2019_Q4",
            "IMO2019_Q5"
        ]
    },
    {
        "session": "Generic_Join",
        "title": "Formalization of Multiway-Join Algorithms",
        "authors": [
            "Thibault Dardinier"
        ],
        "topics": [
            "Computer science/Algorithms"
        ],
        "date": "2019-09-16",
        "abstract": "\nWorst-case optimal multiway-join algorithms are recent seminal\nachievement of the database community. These algorithms compute the\nnatural join of multiple relational databases and improve in the worst\ncase over traditional query plan optimizations of nested binary joins.\nIn 2014, <a\nhref=\"https://doi.org/10.1145/2590989.2590991\">Ngo, Ré,\nand Rudra</a> gave a unified presentation of different multi-way\njoin algorithms. We formalized and proved correct their \"Generic\nJoin\" algorithm and extended it to support negative joins.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-09-18"
            }
        ],
        "dependencies": [
            "MFOTL_Monitor"
        ],
        "theories": [
            "Generic_Join",
            "Generic_Join_Correctness",
            "Examples_Join"
        ]
    },
    {
        "session": "Store_Buffer_Reduction",
        "title": "A Reduction Theorem for Store Buffers",
        "authors": [
            "Ernie Cohen",
            "Norbert Schirmer"
        ],
        "topics": [
            "Computer science/Concurrency"
        ],
        "date": "2019-01-07",
        "abstract": "\nWhen verifying a concurrent program, it is usual to assume that memory\nis sequentially consistent.  However, most modern multiprocessors\ndepend on store buffering for efficiency, and provide native\nsequential consistency only at a substantial performance penalty.  To\nregain sequential consistency, a programmer has to follow an\nappropriate programming discipline. However, na&iuml;ve disciplines,\nsuch as protecting all shared accesses with locks, are not flexible\nenough for building high-performance multiprocessor software.  We\npresent a new discipline for concurrent programming under TSO (total\nstore order, with store buffer forwarding). It does not depend on\nconcurrency primitives, such as locks. Instead, threads use ghost\noperations to acquire and release ownership of memory addresses. A\nthread can write to an address only if no other thread owns it, and\ncan read from an address only if it owns it or it is shared and the\nthread has flushed its store buffer since it last wrote to an address\nit did not own. This discipline covers both coarse-grained concurrency\n(where data is protected by locks) as well as fine-grained concurrency\n(where atomic operations race to memory).  We formalize this\ndiscipline in Isabelle/HOL, and prove that if every execution of a\nprogram in a system without store buffers follows the discipline, then\nevery execution of the program with store buffers is sequentially\nconsistent. Thus, we can show sequential consistency under TSO by\nordinary assertional reasoning about the program, without having to\nconsider store buffers at all.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2019-01-11"
            }
        ],
        "theories": [
            "ReduceStoreBuffer",
            "ReduceStoreBufferSimulation",
            "PIMP",
            "SyntaxTweaks",
            "Abbrevs",
            "Variants",
            "Text",
            "Preliminaries"
        ]
    },
    {
        "session": "Gauss-Jordan-Elim-Fun",
        "title": "Gauss-Jordan Elimination for Matrices Represented as Functions",
        "authors": [
            "Tobias Nipkow"
        ],
        "date": "2011-08-19",
        "topics": [
            "Computer science/Algorithms/Mathematical",
            "Mathematics/Algebra"
        ],
        "abstract": "This theory provides a compact formulation of Gauss-Jordan elimination for matrices represented as functions. Its distinctive feature is succinctness. It is not meant for large computations.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2020-01-14"
            },
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-24"
            },
            {
                "2011-1": "2011-10-11"
            },
            {
                "2011": "2011-08-19"
            }
        ],
        "theories": [
            "Gauss_Jordan_Elim_Fun"
        ]
    },
    {
        "session": "Safe_OCL",
        "title": "Safe OCL",
        "authors": [
            "Denis Nikiforov"
        ],
        "topics": [
            "Computer science/Programming languages/Language definitions"
        ],
        "license": "LGPL",
        "date": "2019-03-09",
        "abstract": "\n<p>The theory is a formalization of the\n<a href=\"https://www.omg.org/spec/OCL/\">OCL</a> type system, its abstract\nsyntax and expression typing rules. The theory does not define a concrete\nsyntax and a semantics. In contrast to\n<a href=\"https://www.isa-afp.org/entries/Featherweight_OCL.html\">Featherweight OCL</a>,\nit is based on a deep embedding approach. The type system is defined from scratch,\nit is not based on the Isabelle HOL type system.</p>\n<p>The Safe OCL distincts nullable and non-nullable types. Also the theory gives a\nformal definition of <a href=\"http://ceur-ws.org/Vol-1512/paper07.pdf\">safe\nnavigation operations</a>. The Safe OCL typing rules are much stricter than rules\ngiven in the OCL specification. It allows one to catch more errors on a type\nchecking phase.</p>\n<p>The type theory presented is four-layered: classes, basic types, generic types,\nerrorable types. We introduce the following new types: non-nullable types (T[1]),\nnullable types (T[?]), OclSuper. OclSuper is a supertype of all other types (basic\ntypes, collections, tuples). This type allows us to define a total supremum function,\nso types form an upper semilattice. It allows us to define rich expression typing\nrules in an elegant manner.</p>\n<p>The Preliminaries Chapter of the theory defines a number of helper lemmas for\ntransitive closures and tuples. It defines also a generic object model independent\nfrom OCL. It allows one to use the theory as a reference for formalization of analogous languages.</p>",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2019-03-14"
            }
        ],
        "theories": [
            "Errorable",
            "Transitive_Closure_Ext",
            "Finite_Map_Ext",
            "Tuple",
            "Object_Model",
            "OCL_Basic_Types",
            "OCL_Types",
            "OCL_Syntax",
            "OCL_Object_Model",
            "OCL_Typing",
            "OCL_Normalization",
            "OCL_Examples"
        ]
    },
    {
        "session": "Jinja",
        "title": "Jinja is not Java",
        "authors": [
            "Gerwin Klein",
            "Tobias Nipkow"
        ],
        "date": "2005-06-01",
        "topics": [
            "Computer science/Programming languages/Language definitions"
        ],
        "abstract": "We introduce Jinja, a Java-like programming language with a formal semantics designed to exhibit core features of the Java language architecture. Jinja is a compromise between realism of the language and tractability and clarity of the formal semantics. The following aspects are formalised: a big and a small step operational semantics for Jinja and a proof of their equivalence; a type system and a definite initialisation analysis; a type safety proof of the small step semantics; a virtual machine (JVM), its operational semantics and its type system; a type safety proof for the JVM; a bytecode verifier, i.e. data flow analyser for the JVM; a correctness proof of the bytecode verifier w.r.t. the type system; a compiler and a proof that it preserves semantics and well-typedness. The emphasis of this work is not on particular language features but on providing a unified model of the source language, the virtual machine and the compiler. The whole development has been carried out in the theorem prover Isabelle/HOL.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-24"
            },
            {
                "2011-1": "2011-10-11"
            },
            {
                "2011": "2011-02-11"
            },
            {
                "2009-2": "2010-07-01"
            },
            {
                "2009-1": "2009-12-12"
            },
            {
                "2009": "2009-04-29"
            },
            {
                "2008": "2008-06-10"
            },
            {
                "2007": "2007-11-27"
            },
            {
                "2005": "2006-08-08"
            },
            {
                "2005": "2005-10-14"
            }
        ],
        "dependencies": [
            "List-Index"
        ],
        "theories": [
            "Auxiliary",
            "Type",
            "Decl",
            "TypeRel",
            "Value",
            "Objects",
            "Exceptions",
            "Expr",
            "State",
            "BigStep",
            "SmallStep",
            "SystemClasses",
            "WellForm",
            "WWellForm",
            "Equivalence",
            "WellType",
            "WellTypeRT",
            "DefAss",
            "Conform",
            "Progress",
            "JWellForm",
            "TypeSafe",
            "Annotate",
            "Examples",
            "execute_Bigstep",
            "execute_WellType",
            "JVMState",
            "JVMInstructions",
            "JVMExecInstr",
            "JVMExceptions",
            "JVMExec",
            "JVMDefensive",
            "JVMListExample",
            "Semilat",
            "Err",
            "Opt",
            "Product",
            "Listn",
            "Semilattices",
            "Typing_Framework",
            "SemilatAlg",
            "Typing_Framework_err",
            "Kildall",
            "LBVSpec",
            "LBVCorrect",
            "LBVComplete",
            "Abstract_BV",
            "SemiType",
            "JVM_SemiType",
            "Effect",
            "EffectMono",
            "BVSpec",
            "TF_JVM",
            "BVExec",
            "LBVJVM",
            "BVConform",
            "BVSpecTypeSafe",
            "BVNoTypeError",
            "BVExample",
            "J1",
            "J1WellForm",
            "PCompiler",
            "Hidden",
            "Compiler1",
            "Correctness1",
            "Compiler2",
            "Correctness2",
            "Compiler",
            "TypeComp",
            "Jinja"
        ]
    },
    {
        "session": "Shivers-CFA",
        "title": "Shivers' Control Flow Analysis",
        "topics": [
            "Computer science/Programming languages/Static analysis"
        ],
        "authors": [
            "Joachim Breitner"
        ],
        "date": "2010-11-16",
        "abstract": "\nIn his dissertation, Olin Shivers introduces a concept of control flow graphs\nfor functional languages, provides an algorithm to statically derive a safe\napproximation of the control flow graph and proves this algorithm correct. In\nthis research project, Shivers' algorithms and proofs are formalized\nin the HOLCF extension of HOL.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-24"
            },
            {
                "2011-1": "2011-10-11"
            },
            {
                "2011": "2011-02-11"
            },
            {
                "2009-2": "2010-11-18"
            },
            {
                "2009-2": "2010-11-17"
            }
        ],
        "theories": [
            "HOLCFUtils",
            "CPSScheme",
            "Eval",
            "Utils",
            "SetMap",
            "AbsCF",
            "ExCF",
            "AbsCFCorrect",
            "ExCFSV",
            "Computability",
            "FixTransform",
            "CPSUtils",
            "MapSets",
            "AbsCFComp"
        ]
    },
    {
        "session": "Monad_Memo_DP",
        "title": "Monadification, Memoization and Dynamic Programming",
        "authors": [
            "Simon Wimmer",
            "Shuwei Hu",
            "Tobias Nipkow"
        ],
        "topics": [
            "Computer science/Programming languages/Transformations",
            "Computer science/Algorithms",
            "Computer science/Functional programming"
        ],
        "date": "2018-05-22",
        "abstract": "\nWe present a lightweight framework for the automatic verified\n(functional or imperative) memoization of recursive functions. Our\ntool can turn a pure Isabelle/HOL function definition into a\nmonadified version in a state monad or the Imperative HOL heap monad,\nand prove a correspondence theorem. We provide a variety of memory\nimplementations for the two types of monads. A number of simple\ntechniques allow us to achieve bottom-up computation and\nspace-efficient memoization. The framework’s utility is demonstrated\non a number of representative dynamic programming problems. A detailed\ndescription of our work can be found in the accompanying paper [2].",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2018-05-23"
            }
        ],
        "dependencies": [
            "Show"
        ],
        "theories": [
            "State_Monad_Ext",
            "Pure_Monad",
            "DP_CRelVS",
            "State_Heap_Misc",
            "Heap_Monad_Ext",
            "State_Heap",
            "DP_CRelVH",
            "Memory",
            "Pair_Memory",
            "Index",
            "Memory_Heap",
            "Transform_Cmd",
            "files/Transform_Misc.ML",
            "files/Transform_Const.ML",
            "files/Transform_Data.ML",
            "files/Transform_Tactic.ML",
            "files/Transform_Term.ML",
            "files/Transform.ML",
            "files/Transform_Parser.ML",
            "Bottom_Up_Computation",
            "Bottom_Up_Computation_Heap",
            "Solve_Cong",
            "Heap_Main",
            "State_Main",
            "Example_Misc",
            "Tracing",
            "Ground_Function",
            "files/Ground_Function.ML",
            "Bellman_Ford",
            "Heap_Default",
            "Knapsack",
            "Counting_Tiles",
            "CYK",
            "Min_Ed_Dist0",
            "OptBST",
            "Longest_Common_Subsequence",
            "All_Examples"
        ]
    },
    {
        "session": "Zeta_3_Irrational",
        "title": "The Irrationality of ζ(3)",
        "authors": [
            "Manuel Eberl"
        ],
        "topics": [
            "Mathematics/Number theory"
        ],
        "date": "2019-12-27",
        "abstract": "\n<p>This article provides a formalisation of Beukers's\nstraightforward analytic proof that ζ(3) is irrational. This was first\nproven by Apéry (which is why this result is also often called\n‘Apéry's Theorem’) using a more algebraic approach. This\nformalisation follows <a\nhref=\"http://people.math.sc.edu/filaseta/gradcourses/Math785/Math785Notes4.pdf\">Filaseta's\npresentation</a> of Beukers's proof.</p>",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-12-28"
            }
        ],
        "dependencies": [
            "Prime_Number_Theorem",
            "E_Transcendental",
            "Prime_Distribution_Elementary"
        ],
        "theories": [
            "Zeta_3_Irrational"
        ]
    },
    {
        "session": "Key_Agreement_Strong_Adversaries",
        "title": "Refining Authenticated Key Agreement with Strong Adversaries",
        "authors": [
            "Joseph Lallemand",
            "Christoph Sprenger"
        ],
        "topics": [
            "Computer science/Security"
        ],
        "license": "LGPL",
        "date": "2017-01-31",
        "abstract": "\nWe develop a family of key agreement protocols that are correct by\nconstruction. Our work substantially extends prior work on developing\nsecurity protocols by refinement. First, we strengthen the adversary\nby allowing him to compromise different resources of protocol\nparticipants, such as their long-term keys or their session keys. This\nenables the systematic development of protocols that ensure strong\nproperties such as perfect forward secrecy. Second, we broaden the\nclass of protocols supported to include those with non-atomic keys and\nequationally defined cryptographic operators. We use these extensions\nto develop key agreement protocols including signed Diffie-Hellman and\nthe core of IKEv1 and SKEME.",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2017-02-03"
            }
        ],
        "theories": [
            "Infra",
            "Refinement",
            "Messages",
            "Message_derivation",
            "IK",
            "Secrecy",
            "AuthenticationN",
            "AuthenticationI",
            "Runs",
            "Channels",
            "Payloads",
            "Implem",
            "Implem_lemmas",
            "Implem_symmetric",
            "Implem_asymmetric",
            "pfslvl1",
            "pfslvl2",
            "pfslvl3",
            "pfslvl3_asymmetric",
            "pfslvl3_symmetric",
            "dhlvl1",
            "dhlvl2",
            "dhlvl3",
            "dhlvl3_asymmetric",
            "dhlvl3_symmetric",
            "sklvl1",
            "sklvl2",
            "sklvl3",
            "sklvl3_asymmetric",
            "sklvl3_symmetric"
        ]
    },
    {
        "session": "Multirelations",
        "title": "Binary Multirelations",
        "authors": [
            "Hitoshi Furusawa",
            "Georg Struth"
        ],
        "date": "2015-06-11",
        "topics": [
            "Mathematics/Algebra"
        ],
        "abstract": "\nBinary multirelations associate elements of a set with its subsets; hence\nthey are binary relations from a set to its power set. Applications include\nalternating automata, models and logics for games, program semantics with\ndual demonic and angelic nondeterministic choices and concurrent dynamic\nlogics. This proof document supports an arXiv article that formalises the\nbasic algebra of multirelations and proposes axiom systems for them,\nranging from weak bi-monoids to weak bi-quantales.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-06-13"
            }
        ],
        "dependencies": [
            "Kleene_Algebra"
        ],
        "theories": [
            "C_Algebras",
            "Multirelations"
        ]
    },
    {
        "session": "Nested_Multisets_Ordinals",
        "title": "Formalization of Nested Multisets, Hereditary Multisets, and Syntactic Ordinals",
        "authors": [
            "Jasmin Christian Blanchette",
            "Mathias Fleury",
            "Dmitriy Traytel"
        ],
        "date": "2016-11-12",
        "topics": [
            "Logic/Rewriting"
        ],
        "abstract": "This Isabelle/HOL formalization introduces a nested multiset datatype and defines Dershowitz and Manna's nested multiset order. The order is proved well founded and linear. By removing one constructor, we transform the nested multisets into hereditary multisets. These are isomorphic to the syntactic ordinals—the ordinals can be recursively expressed in Cantor normal form. Addition, subtraction, multiplication, and linear orders are provided on this type.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            }
        ],
        "dependencies": [
            "Ordinal",
            "List-Index"
        ],
        "theories": [
            "Multiset_More",
            "Signed_Multiset",
            "files/zmultiset_simprocs.ML",
            "Nested_Multiset",
            "Hereditary_Multiset",
            "Signed_Hereditary_Multiset",
            "Syntactic_Ordinal",
            "Signed_Syntactic_Ordinal",
            "Syntactic_Ordinal_Bridge",
            "McCarthy_91",
            "Hydra_Battle",
            "Goodstein_Sequence",
            "Unary_PCF"
        ]
    },
    {
        "session": "LTL_to_GBA",
        "title": "Converting Linear-Time Temporal Logic to Generalized Büchi Automata",
        "authors": [
            "Alexander Schimpf",
            "Peter Lammich"
        ],
        "date": "2014-05-28",
        "topics": [
            "Computer science/Automata and formal languages"
        ],
        "abstract": "\nWe formalize linear-time temporal logic (LTL) and the algorithm by Gerth\net al. to convert LTL formulas to generalized Büchi automata.\nWe also formalize some syntactic rewrite rules that can be applied to\noptimize the LTL formula before conversion.\nMoreover, we integrate the Stuttering Equivalence AFP-Entry by Stefan\nMerz, adapting the lemma that next-free LTL formula cannot distinguish\nbetween stuttering equivalent runs to our setting.\n<p>\nWe use the Isabelle Refinement and Collection framework, as well as the\nAutoref tool, to obtain a refined version of our algorithm, from which\nefficiently executable code can be extracted.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2014-05-29"
            }
        ],
        "dependencies": [
            "CAVA_Automata",
            "LTL",
            "Stuttering_Equivalence"
        ],
        "theories": [
            "LTL_to_GBA",
            "LTL_to_GBA_impl",
            "All_Of_LTL_to_GBA"
        ]
    },
    {
        "session": "Stochastic_Matrices",
        "title": "Stochastic Matrices and the Perron-Frobenius Theorem",
        "authors": [
            "René Thiemann"
        ],
        "topics": [
            "Mathematics/Algebra",
            "Computer science/Automata and formal languages"
        ],
        "date": "2017-11-22",
        "abstract": "\nStochastic matrices are a convenient way to model discrete-time and\nfinite state Markov chains. The Perron&ndash;Frobenius theorem\ntells us something about the existence and uniqueness of non-negative\neigenvectors of a stochastic matrix.  In this entry, we formalize\nstochastic matrices, link the formalization to the existing AFP-entry\non Markov chains, and apply the Perron&ndash;Frobenius theorem to\nprove that stationary distributions always exist, and they are unique\nif the stochastic matrix is irreducible.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-11-23"
            }
        ],
        "dependencies": [
            "Markov_Models",
            "Perron_Frobenius",
            "Jordan_Normal_Form"
        ],
        "theories": [
            "Stochastic_Matrix",
            "Stochastic_Vector_PMF",
            "Stochastic_Matrix_Markov_Models",
            "Eigenspace",
            "Stochastic_Matrix_Perron_Frobenius"
        ]
    },
    {
        "session": "Multi_Party_Computation",
        "title": "Multi-Party Computation",
        "authors": [
            "David Aspinall",
            "David Butler"
        ],
        "topics": [
            "Computer science/Security"
        ],
        "date": "2019-05-09",
        "abstract": "\nWe use CryptHOL to consider Multi-Party Computation (MPC) protocols.\nMPC was first considered by Yao in 1983 and recent advances in\nefficiency and an increased demand mean it is now deployed in the real\nworld. Security is considered using the real/ideal world paradigm. We\nfirst define security in the semi-honest security setting where\nparties are assumed not to deviate from the protocol transcript. In\nthis setting we prove multiple Oblivious Transfer (OT) protocols\nsecure and then show security for the gates of the GMW protocol. We\nthen define malicious security, this is a stronger notion of security\nwhere parties are assumed to be fully corrupted by an adversary. In\nthis setting we again consider OT, as it is a fundamental building\nblock of almost all MPC protocols.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2019-05-10"
            }
        ],
        "dependencies": [
            "Game_Based_Crypto"
        ],
        "theories": [
            "Cyclic_Group_Ext",
            "Number_Theory_Aux",
            "Uniform_Sampling",
            "Semi_Honest_Def",
            "OT_Functionalities",
            "ETP",
            "ETP_OT",
            "ETP_RSA_OT",
            "Noar_Pinkas_OT",
            "OT14",
            "GMW",
            "Secure_Multiplication",
            "DH_Ext",
            "Malicious_Defs",
            "Malicious_OT"
        ]
    },
    {
        "session": "Partial_Order_Reduction",
        "title": "Partial Order Reduction",
        "authors": [
            "Julian Brunner"
        ],
        "topics": [
            "Computer science/Automata and formal languages"
        ],
        "date": "2018-06-05",
        "abstract": "\nThis entry provides a formalization of the abstract theory of ample\nset partial order reduction. The formalization includes transition\nsystems with actions, trace theory, as well as basics on finite,\ninfinite, and lazy sequences. We also provide a basic framework for\nstatic analysis on concurrent systems with respect to the ample set\ncondition.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            }
        ],
        "dependencies": [
            "Transition_Systems_and_Automata",
            "Coinductive",
            "Stuttering_Equivalence"
        ],
        "theories": [
            "List_Prefixes",
            "List_Extensions",
            "Word_Prefixes",
            "Set_Extensions",
            "Basic_Extensions",
            "Relation_Extensions",
            "Transition_System_Extensions",
            "Traces",
            "Transition_System_Traces",
            "Functions",
            "ENat_Extensions",
            "CCPO_Extensions",
            "ESet_Extensions",
            "Coinductive_List_Extensions",
            "LList_Prefixes",
            "Stuttering",
            "Transition_System_Interpreted_Traces",
            "Ample_Abstract",
            "Formula",
            "Ample_Correctness",
            "Ample_Analysis"
        ]
    },
    {
        "session": "CryptoBasedCompositionalProperties",
        "title": "Compositional Properties of Crypto-Based Components",
        "authors": [
            "Maria Spichkova"
        ],
        "date": "2014-01-11",
        "topics": [
            "Computer science/Security"
        ],
        "abstract": "This paper presents an Isabelle/HOL set of theories which allows the specification of crypto-based components and the verification of their composition properties wrt. cryptographic aspects. We introduce a formalisation of the security property of data secrecy, the corresponding definitions and proofs. Please note that here we import the Isabelle/HOL theory ListExtras.thy, presented in the AFP entry FocusStreamsCaseStudies-AFP.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2014-01-14"
            },
            {
                "2013-2": "2014-01-11"
            }
        ],
        "theories": [
            "ListExtras",
            "Secrecy_types",
            "inout",
            "Secrecy",
            "CompLocalSecrets",
            "KnowledgeKeysSecrets"
        ]
    },
    {
        "session": "Hybrid_Multi_Lane_Spatial_Logic",
        "title": "Hybrid Multi-Lane Spatial Logic",
        "authors": [
            "Sven Linker"
        ],
        "topics": [
            "Logic/General logic/Modal logic"
        ],
        "date": "2017-11-06",
        "abstract": "\nWe present a semantic embedding of a spatio-temporal multi-modal\nlogic, specifically defined to reason about motorway traffic, into\nIsabelle/HOL. The semantic model is an abstraction of a motorway,\nemphasising local spatial properties, and parameterised by the types\nof sensors deployed in the vehicles. We use the logic to define\ncontroller constraints to ensure safety, i.e., the absence of\ncollisions on the motorway. After proving safety with a restrictive\ndefinition of sensors, we relax these assumptions and show how to\namend the controller constraints to still guarantee safety.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-11-09"
            }
        ],
        "theories": [
            "NatInt",
            "RealInt",
            "Cars",
            "Traffic",
            "Views",
            "Restriction",
            "Move",
            "Sensors",
            "Length",
            "HMLSL",
            "Perfect_Sensors",
            "HMLSL_Perfect",
            "Safety_Perfect",
            "Regular_Sensors",
            "HMLSL_Regular",
            "Safety_Regular"
        ]
    },
    {
        "session": "Proof_Strategy_Language",
        "title": "Proof Strategy Language",
        "authors": [
            "Yutaka Nagashima"
        ],
        "topics": [
            "Tools"
        ],
        "date": "2016-12-20",
        "abstract": "\nIsabelle includes various automatic tools for finding proofs under\ncertain conditions. However, for each conjecture, knowing which\nautomation to use, and how to tweak its parameters, is currently\nlabour intensive. We have developed a language, PSL, designed to\ncapture high level proof strategies. PSL offloads the construction of\nhuman-readable fast-to-replay proof scripts to automatic search,\nmaking use of search-time information about each conjecture. Our\npreliminary evaluations show that PSL reduces the labour cost of\ninteractive theorem proving. This submission contains the\nimplementation of PSL and an example theory file, Example.thy, showing\nhow to write poof strategies in PSL.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-21"
            }
        ],
        "theories": [
            "Try_Hard",
            "files/Utils.ML",
            "files/Subtool.ML",
            "files/Dynamic_Tactic_Generation.ML",
            "files/Constructor_Class.ML",
            "files/Instantiation.ML",
            "files/Monadic_Prover.ML",
            "files/Parser_Combinator.ML",
            "files/PSL_Parser.ML",
            "files/Isar_Interface.ML",
            "PSL",
            "Example"
        ]
    },
    {
        "session": "CCS",
        "title": "CCS in nominal logic",
        "authors": [
            "Jesper Bengtson"
        ],
        "date": "2012-05-29",
        "topics": [
            "Computer science/Concurrency/Process calculi"
        ],
        "abstract": "We formalise a large portion of CCS as described in Milner's book 'Communication and Concurrency' using the nominal datatype package in Isabelle. Our results include many of the standard theorems of bisimulation equivalence and congruence, for both weak and strong versions. One main goal of this formalisation is to keep the machine-checked proofs as close to their pen-and-paper counterpart as possible.\n<p>\nThis entry is described in detail in <a href=\"http://www.itu.dk/people/jebe/files/thesis.pdf\">Bengtson's thesis</a>.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-06-14"
            }
        ],
        "theories": [
            "Agent",
            "Tau_Chain",
            "Weak_Cong_Semantics",
            "Weak_Semantics",
            "Strong_Sim",
            "Weak_Sim",
            "Weak_Cong_Sim",
            "Strong_Sim_SC",
            "Strong_Bisim",
            "Strong_Sim_Pres",
            "Strong_Bisim_Pres",
            "Struct_Cong",
            "Strong_Bisim_SC",
            "Weak_Bisim",
            "Weak_Cong",
            "Weak_Sim_Pres",
            "Weak_Bisim_Pres",
            "Weak_Cong_Sim_Pres",
            "Weak_Cong_Pres"
        ]
    },
    {
        "session": "Amicable_Numbers",
        "title": "Amicable Numbers",
        "authors": [
            "Angeliki Koutsoukou-Argyraki"
        ],
        "topics": [
            "Mathematics/Number theory"
        ],
        "date": "2020-08-04",
        "abstract": "\nThis is a formalisation of Amicable Numbers, involving some relevant\nmaterial including Euler's sigma function, some relevant\ndefinitions, results and examples as well as rules such as\nTh&#257;bit ibn Qurra's Rule, Euler's Rule, te\nRiele's Rule and Borho's Rule with breeders.",
        "licence": "BSD",
        "dependencies": [
            "Pratt_Certificate",
            "Polynomial_Factorization"
        ],
        "theories": [
            "Amicable_Numbers"
        ]
    },
    {
        "session": "Myhill-Nerode",
        "title": "The Myhill-Nerode Theorem Based on Regular Expressions",
        "authors": [
            "Chunhan Wu",
            "Xingyuan Zhang",
            "Christian Urban"
        ],
        "contributors": [
            "Manuel Eberl"
        ],
        "topics": [
            "Computer science/Automata and formal languages"
        ],
        "date": "2011-08-26",
        "abstract": "There are many proofs of the Myhill-Nerode theorem using automata. In this library we give a proof entirely based on regular expressions, since regularity of languages can be conveniently defined using regular expressions (it is more painful in HOL to define regularity in terms of automata).  We prove the first direction of the Myhill-Nerode theorem by solving equational systems that involve regular expressions.  For the second direction we give two proofs: one using tagging-functions and another using partial derivatives. We also establish various closure properties of regular languages. Most details of the theories are described in our ITP 2011 paper.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-03-02"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-24"
            },
            {
                "2011-1": "2011-10-11"
            }
        ],
        "dependencies": [
            "Abstract-Rewriting",
            "Regular-Sets",
            "Open_Induction",
            "Well_Quasi_Orders"
        ],
        "theories": [
            "Folds",
            "Myhill_1",
            "Myhill_2",
            "Myhill",
            "Closures",
            "Closures2",
            "Non_Regular_Languages"
        ]
    },
    {
        "session": "Generalized_Counting_Sort",
        "title": "An Efficient Generalization of Counting Sort for Large, possibly Infinite Key Ranges",
        "authors": [
            "Pasquale Noce"
        ],
        "topics": [
            "Computer science/Algorithms",
            "Computer science/Functional programming"
        ],
        "date": "2019-12-04",
        "abstract": "\nCounting sort is a well-known algorithm that sorts objects of any kind\nmapped to integer keys, or else to keys in one-to-one correspondence\nwith some subset of the integers (e.g. alphabet letters). However, it\nis suitable for direct use, viz. not just as a subroutine of another\nsorting algorithm (e.g. radix sort), only if the key range is not\nsignificantly larger than the number of the objects to be sorted.\nThis paper describes a tail-recursive generalization of counting sort\nmaking use of a bounded number of counters, suitable for direct use in\ncase of a large, or even infinite key range of any kind, subject to\nthe only constraint of being a subset of an arbitrary linear order.\nAfter performing a pen-and-paper analysis of how such algorithm has to\nbe designed to maximize its efficiency, this paper formalizes the\nresulting generalized counting sort (GCsort) algorithm and then\nformally proves its correctness properties, namely that (a) the\ncounters' number is maximized never exceeding the fixed upper\nbound, (b) objects are conserved, (c) objects get sorted, and (d) the\nalgorithm is stable.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-12-09"
            }
        ],
        "theories": [
            "Algorithm",
            "Conservation",
            "Sorting",
            "Stability"
        ]
    },
    {
        "session": "Stewart_Apollonius",
        "title": "Stewart's Theorem and Apollonius' Theorem",
        "authors": [
            "Lukas Bulwahn"
        ],
        "topics": [
            "Mathematics/Geometry"
        ],
        "date": "2017-07-31",
        "abstract": "\nThis entry formalizes the two geometric theorems, Stewart's and\nApollonius' theorem. Stewart's Theorem relates the length of\na triangle's cevian to the lengths of the triangle's two\nsides. Apollonius' Theorem is a specialisation of Stewart's\ntheorem, restricting the cevian to be the median. The proof applies\nthe law of cosines, some basic geometric facts about triangles and\nthen simply transforms the terms algebraically to yield the\nconjectured relation. The formalization in Isabelle can closely follow\nthe informal proofs described in the Wikipedia articles of those two\ntheorems.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2017-08-01"
            }
        ],
        "dependencies": [
            "Triangle"
        ],
        "theories": [
            "Stewart_Apollonius"
        ]
    },
    {
        "session": "Smooth_Manifolds",
        "title": "Smooth Manifolds",
        "authors": [
            "Fabian Immler",
            "Bohua Zhan"
        ],
        "topics": [
            "Mathematics/Analysis",
            "Mathematics/Topology"
        ],
        "date": "2018-10-22",
        "abstract": "\nWe formalize the definition and basic properties of smooth manifolds\nin Isabelle/HOL. Concepts covered include partition of unity, tangent\nand cotangent spaces, and the fundamental theorem of path integrals.\nWe also examine some concrete manifolds such as spheres and projective\nspaces. The formalization makes extensive use of the analysis and\nlinear algebra libraries in Isabelle/HOL, in particular its\n“types-to-sets” mechanism.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-10-23"
            }
        ],
        "theories": [
            "Analysis_More",
            "Smooth",
            "Bump_Function",
            "Chart",
            "Topological_Manifold",
            "Differentiable_Manifold",
            "Partition_Of_Unity",
            "Tangent_Space",
            "Cotangent_Space",
            "Product_Manifold",
            "Sphere",
            "Projective_Space"
        ]
    },
    {
        "session": "Order_Lattice_Props",
        "title": "Properties of Orderings and Lattices",
        "authors": [
            "Georg Struth"
        ],
        "topics": [
            "Mathematics/Order"
        ],
        "date": "2018-12-11",
        "abstract": "\nThese components add further fundamental order and lattice-theoretic\nconcepts and properties to Isabelle's libraries.  They follow by\nand large the introductory sections of the Compendium of Continuous\nLattices,  covering directed and filtered sets, down-closed and\nup-closed sets, ideals and filters, Galois connections, closure and\nco-closure operators. Some emphasis is on duality and morphisms\nbetween structures, as in the Compendium.  To this end, three ad-hoc\napproaches to duality are compared.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-28"
            },
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-12-19"
            }
        ],
        "theories": [
            "Sup_Lattice",
            "Order_Duality",
            "Order_Lattice_Props",
            "Representations",
            "Galois_Connections",
            "Fixpoint_Fusion",
            "Closure_Operators",
            "Order_Lattice_Props_Loc",
            "Order_Lattice_Props_Wenzel"
        ]
    },
    {
        "session": "Jacobson_Basic_Algebra",
        "title": "A Case Study in Basic Algebra",
        "authors": [
            "Clemens Ballarin"
        ],
        "topics": [
            "Mathematics/Algebra"
        ],
        "date": "2019-08-30",
        "abstract": "\nThe focus of this case study is re-use in abstract algebra.  It\ncontains locale-based formalisations of selected parts of set, group\nand ring theory from Jacobson's <i>Basic Algebra</i>\nleading to the respective fundamental homomorphism theorems.  The\nstudy is not intended as a library base for abstract algebra.  It\nrather explores an approach towards abstract algebra in Isabelle.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-09-01"
            }
        ],
        "theories": [
            "Set_Theory",
            "Group_Theory",
            "Ring_Theory"
        ]
    },
    {
        "session": "Relational_Method",
        "title": "The Relational Method with Message Anonymity for the Verification of Cryptographic Protocols",
        "authors": [
            "Pasquale Noce"
        ],
        "topics": [
            "Computer science/Security"
        ],
        "date": "2020-12-05",
        "abstract": "\nThis paper introduces a new method for the formal verification of\ncryptographic protocols, the relational method, derived from\nPaulson's inductive method by means of some enhancements aimed at\nstreamlining formal definitions and proofs, specially for protocols\nusing public key cryptography. Moreover, this paper proposes a method\nto formalize a further security property, message anonymity, in\naddition to message confidentiality and authenticity.  The relational\nmethod, including message anonymity, is then applied to the\nverification of a sample authentication protocol, comprising Password\nAuthenticated Connection Establishment (PACE) with Chip Authentication\nMapping followed by the explicit verification of an additional\npassword over the PACE secure channel.",
        "licence": "BSD",
        "theories": [
            "Definitions",
            "Authentication",
            "Anonymity",
            "Possibility"
        ]
    },
    {
        "session": "Concurrent_Ref_Alg",
        "title": "Concurrent Refinement Algebra and Rely Quotients",
        "authors": [
            "Julian Fell",
            "Ian J. Hayes",
            "Andrius Velykis"
        ],
        "topics": [
            "Computer science/Concurrency"
        ],
        "date": "2016-12-30",
        "abstract": "\nThe concurrent refinement algebra developed here is designed to\nprovide a foundation for rely/guarantee reasoning about concurrent\nprograms. The algebra builds on a complete lattice of commands by\nproviding sequential composition, parallel composition and a novel\nweak conjunction operator. The weak conjunction operator coincides\nwith the lattice supremum providing its arguments are non-aborting,\nbut aborts if either of its arguments do. Weak conjunction provides an\nabstract version of a guarantee condition as a guarantee process. We\ndistinguish between models that distribute sequential composition over\nnon-deterministic choice from the left (referred to as being\nconjunctive in the refinement calculus literature) and those that\ndon't. Least and greatest fixed points of monotone functions are\nprovided to allow recursion and iteration operators to be added to the\nlanguage. Additional iteration laws are available for conjunctive\nmodels. The rely quotient of processes <i>c</i> and\n<i>i</i> is the process that, if executed in parallel with\n<i>i</i> implements <i>c</i>. It represents an\nabstract version of a rely condition generalised to a process.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2017-01-04"
            }
        ],
        "theories": [
            "Refinement_Lattice",
            "Sequential",
            "Parallel",
            "Conjunction",
            "CRA",
            "Galois_Connections",
            "Iteration",
            "Conjunctive_Sequential",
            "Infimum_Nat",
            "Conjunctive_Iteration",
            "Rely_Quotient"
        ]
    },
    {
        "session": "Dict_Construction",
        "title": "Dictionary Construction",
        "authors": [
            "Lars Hupel"
        ],
        "topics": [
            "Tools"
        ],
        "date": "2017-05-24",
        "abstract": "\nIsabelle's code generator natively supports type classes. For\ntargets that do not have language support for classes and instances,\nit performs the well-known dictionary translation, as described by\nHaftmann and Nipkow. This translation happens outside the logic, i.e.,\nthere is no guarantee that it is correct, besides the pen-and-paper\nproof. This work implements a certified dictionary translation that\nproduces new class-free constants and derives equality theorems.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            }
        ],
        "dependencies": [
            "Automatic_Refinement",
            "Lazy_Case",
            "Show"
        ],
        "theories": [
            "Introduction",
            "Impossibility",
            "Dict_Construction",
            "files/dict_construction_util.ML",
            "files/transfer_termination.ML",
            "files/congruences.ML",
            "files/side_conditions.ML",
            "files/class_graph.ML",
            "files/dict_construction.ML",
            "Termination",
            "Test_Dict_Construction",
            "Test_Side_Conditions",
            "Test_Lazy_Case"
        ]
    },
    {
        "session": "Laws_of_Large_Numbers",
        "title": "The Laws of Large Numbers",
        "authors": [
            "Manuel Eberl"
        ],
        "topics": [
            "Mathematics/Probability theory"
        ],
        "date": "2021-02-10",
        "abstract": "\n<p>The Law of Large Numbers states that, informally, if one\nperforms a random experiment $X$ many times and takes the average of\nthe results, that average will be very close to the expected value\n$E[X]$.</p> <p> More formally, let\n$(X_i)_{i\\in\\mathbb{N}}$ be a sequence of independently identically\ndistributed random variables whose expected value $E[X_1]$ exists.\nDenote the running average of $X_1, \\ldots, X_n$ as $\\overline{X}_n$.\nThen:</p> <ul> <li>The Weak Law of Large Numbers\nstates that $\\overline{X}_{n} \\longrightarrow E[X_1]$ in probability\nfor $n\\to\\infty$, i.e. $\\mathcal{P}(|\\overline{X}_{n} - E[X_1]| >\n\\varepsilon) \\longrightarrow 0$ as $n\\to\\infty$ for any $\\varepsilon\n> 0$.</li> <li>The Strong Law of Large Numbers states\nthat $\\overline{X}_{n} \\longrightarrow E[X_1]$ almost surely for\n$n\\to\\infty$, i.e. $\\mathcal{P}(\\overline{X}_{n} \\longrightarrow\nE[X_1]) = 1$.</li> </ul> <p>In this entry, I\nformally prove the strong law and from it the weak law. The approach\nused for the proof of the strong law is a particularly quick and slick\none based on ergodic theory, which was formalised by Gouëzel in\nanother AFP entry.</p>",
        "licence": "BSD",
        "dependencies": [
            "Ergodic_Theory"
        ],
        "theories": [
            "Laws_of_Large_Numbers",
            "Laws_of_Large_Numbers_Example"
        ]
    },
    {
        "session": "InformationFlowSlicing",
        "title": "Information Flow Noninterference via Slicing",
        "authors": [
            "Daniel Wasserrab"
        ],
        "date": "2010-03-23",
        "topics": [
            "Computer science/Security"
        ],
        "abstract": "\n<p>\nIn this contribution, we show how correctness proofs for <a\nhref=\"Slicing.html\">intra-</a> and <a\nhref=\"HRB-Slicing.html\">interprocedural slicing</a> can be used to prove\nthat slicing is able to guarantee information flow noninterference.\nMoreover, we also illustrate how to lift the control flow graphs of the\nrespective frameworks such that they fulfil the additional assumptions\nneeded in the noninterference proofs. A detailed description of the\nintraprocedural proof and its interplay with the slicing framework can be\nfound in the PLAS'09 paper by Wasserrab et al.\n</p>\n<p>\nThis entry contains the part for intra-procedural slicing. See entry\n<a href=\"InformationFlowSlicing_Inter.html\">InformationFlowSlicing_Inter</a>\nfor the inter-procedural part.\n</p>",
        "extra": {
            "Change history": "[2016-06-10] The original entry <a\nhref=\"InformationFlowSlicing.html\">InformationFlowSlicing</a> contained both\nthe <a href=\"InformationFlowSlicing_Inter.html\">inter-</a> and <a\nhref=\"InformationFlowSlicing.html\">intra-procedural</a> case was split into\ntwo for easier maintenance."
        },
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-24"
            },
            {
                "2011-1": "2011-10-11"
            },
            {
                "2011": "2011-02-11"
            },
            {
                "2009-2": "2010-07-01"
            },
            {
                "2009-1": "2010-03-23"
            }
        ],
        "dependencies": [
            "Slicing"
        ],
        "theories": [
            "NonInterferenceIntra",
            "LiftingIntra",
            "NonInterferenceWhile"
        ]
    },
    {
        "session": "Optics",
        "title": "Optics",
        "authors": [
            "Simon Foster",
            "Frank Zeyda"
        ],
        "topics": [
            "Computer science/Functional programming",
            "Mathematics/Algebra"
        ],
        "date": "2017-05-25",
        "abstract": "\nLenses provide an abstract interface for manipulating data types\nthrough spatially-separated views. They are defined abstractly in\nterms of two functions, <em>get</em>, the return a value\nfrom the source type, and <em>put</em> that updates the\nvalue. We mechanise the underlying theory of lenses, in terms of an\nalgebraic hierarchy of lenses, including well-behaved and very\nwell-behaved lenses, each lens class being characterised by a set of\nlens laws. We also mechanise a lens algebra in Isabelle that enables\ntheir composition and comparison, so as to allow construction of\ncomplex lenses. This is accompanied by a large library of algebraic\nlaws. Moreover we also show how the lens classes can be applied by\ninstantiating them with a number of Isabelle data types.",
        "extra": {
            "Change history": "[2020-03-02]\nAdded partial bijective and symmetric lenses.\nImproved alphabet command generating additional lenses and results.\nSeveral additional lens relations, including observational equivalence.\nAdditional theorems throughout.\nAdaptations for Isabelle 2020.\n(revision 44e2e5c)\n[2021-01-27]\nAddition of new theorems throughout, particularly for prisms.\nNew \"chantype\" command allows the definition of an algebraic datatype with generated prisms.\nNew \"dataspace\" command allows the definition of a local-based state space, including lenses and prisms.\nAddition of various examples for the above.\n(revision 89cf045a)"
        },
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2017-06-01"
            }
        ],
        "theories": [
            "Interp",
            "Two",
            "Lens_Laws",
            "Lens_Algebra",
            "Lens_Order",
            "Lens_Symmetric",
            "Lens_Instances",
            "files/Lens_Lib.ML",
            "files/Lens_Record.ML",
            "files/Lens_Statespace.ML",
            "Lenses",
            "Prisms",
            "files/Prism_Lib.ML",
            "Channel_Type",
            "files/Channel_Type.ML",
            "Dataspace",
            "files/Dataspace.ML",
            "Scenes",
            "Optics",
            "Lens_Record_Example",
            "Lens_State"
        ]
    },
    {
        "session": "Marriage",
        "title": "Hall's Marriage Theorem",
        "authors": [
            "Dongchen Jiang",
            "Tobias Nipkow"
        ],
        "date": "2010-12-17",
        "topics": [
            "Mathematics/Combinatorics"
        ],
        "abstract": "Two proofs of Hall's Marriage Theorem: one due to Halmos and Vaughan, one due to Rado.",
        "extra": {
            "Change history": "[2011-09-09] Added Rado's proof"
        },
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-24"
            },
            {
                "2011-1": "2011-10-11"
            },
            {
                "2011": "2011-02-11"
            },
            {
                "2009-2": "2010-12-17"
            }
        ],
        "theories": [
            "Marriage"
        ]
    },
    {
        "session": "Metalogic_ProofChecker",
        "title": "Isabelle's Metalogic: Formalization and Proof Checker",
        "authors": [
            "Tobias Nipkow",
            "Simon Roßkopf"
        ],
        "topics": [
            "Logic/General logic"
        ],
        "date": "2021-04-27",
        "abstract": "\nIn this entry we formalize Isabelle's metalogic in Isabelle/HOL.\nFurthermore, we define a language of proof terms and an executable\nproof checker and prove its soundness wrt. the metalogic.  The\nformalization is intentionally kept close to the Isabelle\nimplementation(for example using de Brujin indices) to enable easy\nintegration of generated code with the Isabelle system without a\ncomplicated translation layer.  The formalization is described in our\n<a href=\"https://arxiv.org/pdf/2104.12224.pdf\">CADE 28 paper</a>.",
        "licence": "BSD",
        "dependencies": [
            "List-Index"
        ],
        "theories": [
            "Core",
            "Preliminaries",
            "Term",
            "Sorts",
            "SortConstants",
            "Theory",
            "Term_Subst",
            "Name",
            "BetaNorm",
            "BetaNormProof",
            "EtaNorm",
            "EtaNormProof",
            "Logic",
            "EqualityProof",
            "ProofTerm",
            "SortsExe",
            "Instances",
            "TheoryExe",
            "CheckerExe",
            "CodeGen"
        ]
    },
    {
        "session": "Saturation_Framework_Extensions",
        "title": "Extensions to the Comprehensive Framework for Saturation Theorem Proving",
        "authors": [
            "Jasmin Christian Blanchette",
            "Sophie Tourret"
        ],
        "topics": [
            "Logic/General logic/Mechanization of proofs"
        ],
        "date": "2020-08-25",
        "abstract": "\nThis Isabelle/HOL formalization extends the AFP entry\n<em>Saturation_Framework</em> with the following\ncontributions:  <ul> <li>an application of the framework\nto prove Bachmair and Ganzinger's resolution prover RP\nrefutationally complete, which was formalized in a more ad hoc fashion\nby Schlichtkrull et al. in the AFP entry\n<em>Ordered_Resultion_Prover</em>;</li>\n<li>generalizations of various basic concepts formalized by\nSchlichtkrull et al., which were needed to verify RP and could be\nuseful to formalize other calculi, such as superposition;</li>\n<li>alternative proofs of fairness (and hence saturation and\nultimately refutational completeness) for the given clause procedures\nGC and LGC, based on invariance.</li> </ul>",
        "licence": "BSD",
        "dependencies": [
            "Saturation_Framework",
            "Ordered_Resolution_Prover",
            "First_Order_Terms",
            "Well_Quasi_Orders"
        ],
        "theories": [
            "Soundness",
            "Standard_Redundancy_Criterion",
            "Clausal_Calculus",
            "FO_Ordered_Resolution_Prover_Revisited",
            "Given_Clause_Architectures_Revisited"
        ]
    },
    {
        "session": "Sqrt_Babylonian",
        "title": "Computing N-th Roots using the Babylonian Method",
        "authors": [
            "René Thiemann"
        ],
        "date": "2013-01-03",
        "topics": [
            "Mathematics/Analysis"
        ],
        "license": "LGPL",
        "abstract": "\nWe implement the Babylonian method to compute n-th roots of numbers.\nWe provide precise algorithms for naturals, integers and rationals, and\noffer an approximation algorithm for square roots over linear ordered fields. Moreover, there\nare precise algorithms to compute the floor and the ceiling of n-th roots.",
        "extra": {
            "Change history": "[2013-10-16] Added algorithms to compute floor and ceiling of sqrt of integers.\n[2014-07-11] Moved NthRoot_Impl from Real-Impl to this entry."
        },
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2013-01-04"
            }
        ],
        "dependencies": [
            "Cauchy"
        ],
        "theories": [
            "Sqrt_Babylonian_Auxiliary",
            "Log_Impl",
            "NthRoot_Impl",
            "Sqrt_Babylonian"
        ]
    },
    {
        "session": "Discrete_Summation",
        "title": "Discrete Summation",
        "authors": [
            "Florian Haftmann"
        ],
        "contributors": [
            "Amine Chaieb"
        ],
        "date": "2014-04-13",
        "topics": [
            "Mathematics/Combinatorics"
        ],
        "abstract": "These theories introduce basic concepts and proofs about discrete summation: shifts, formal summation, falling factorials and stirling numbers. As proof of concept, a simple summation conversion is provided.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2014-04-13"
            }
        ],
        "theories": [
            "Factorials",
            "Discrete_Summation",
            "Summation_Conversion",
            "Examples"
        ]
    },
    {
        "session": "CYK",
        "title": "A formalisation of the Cocke-Younger-Kasami algorithm",
        "authors": [
            "Maksym Bortin"
        ],
        "date": "2016-04-27",
        "topics": [
            "Computer science/Algorithms",
            "Computer science/Automata and formal languages"
        ],
        "abstract": "\nThe theory provides a formalisation of the Cocke-Younger-Kasami\nalgorithm (CYK for short), an approach to solving the word problem\nfor context-free languages.  CYK decides if a word is in the\nlanguages generated by a context-free grammar in Chomsky normal form.\nThe formalized algorithm is executable.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-04-27"
            }
        ],
        "theories": [
            "CYK"
        ]
    },
    {
        "session": "Valuation",
        "title": "Fundamental Properties of Valuation Theory and Hensel's Lemma",
        "authors": [
            "Hidetsune Kobayashi"
        ],
        "date": "2007-08-08",
        "topics": [
            "Mathematics/Algebra"
        ],
        "abstract": "Convergence with respect to a valuation is discussed as convergence of a Cauchy sequence. Cauchy sequences of polynomials are defined. They are used to formalize Hensel's lemma.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-03-08"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-24"
            },
            {
                "2011-1": "2011-10-11"
            },
            {
                "2011": "2011-02-11"
            },
            {
                "2009-2": "2010-07-01"
            },
            {
                "2009-1": "2009-12-12"
            },
            {
                "2009": "2009-04-30"
            },
            {
                "2009": "2009-04-29"
            },
            {
                "2008": "2008-06-10"
            },
            {
                "2007": "2007-11-27"
            }
        ],
        "dependencies": [
            "Group-Ring-Module"
        ],
        "theories": [
            "Valuation1",
            "Valuation2",
            "Valuation3"
        ]
    },
    {
        "session": "Incompleteness",
        "title": "Gödel's Incompleteness Theorems",
        "authors": [
            "Lawrence C. Paulson"
        ],
        "date": "2013-11-17",
        "topics": [
            "Logic/Proof theory"
        ],
        "abstract": "Gödel's two incompleteness theorems are formalised, following a careful  <a href=\"http://journals.impan.gov.pl/dm/Inf/422-0-1.html\">presentation</a> by Swierczkowski, in the theory of <a href=\"HereditarilyFinite.html\">hereditarily finite sets</a>. This represents the first ever machine-assisted proof of the second incompleteness theorem. Compared with traditional formalisations using Peano arithmetic (see e.g. Boolos), coding is simpler, with no need to formalise the notion\nof multiplication (let alone that of a prime number)\nin the formalised calculus upon which the theorem is based.\nHowever, other technical problems had to be solved in order to complete the argument.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-12-02"
            },
            {
                "2013-1": "2013-11-17"
            }
        ],
        "dependencies": [
            "HereditarilyFinite",
            "Nominal2"
        ],
        "theories": [
            "SyntaxN",
            "Coding",
            "Predicates",
            "Sigma",
            "Coding_Predicates",
            "Pf_Predicates",
            "Functions",
            "Goedel_I",
            "II_Prelims",
            "Pseudo_Coding",
            "Quote",
            "Goedel_II"
        ]
    },
    {
        "session": "Prime_Number_Theorem",
        "title": "The Prime Number Theorem",
        "authors": [
            "Manuel Eberl",
            "Lawrence C. Paulson"
        ],
        "topics": [
            "Mathematics/Number theory"
        ],
        "date": "2018-09-19",
        "abstract": "\n<p>This article provides a short proof of the Prime Number\nTheorem in several equivalent forms, most notably\n&pi;(<em>x</em>) ~ <em>x</em>/ln\n<em>x</em> where &pi;(<em>x</em>) is the\nnumber of primes no larger than <em>x</em>. It also\ndefines other basic number-theoretic functions related to primes like\nChebyshev's functions &thetasym; and &psi; and the\n&ldquo;<em>n</em>-th prime number&rdquo; function\np<sub><em>n</em></sub>. We also show various\nbounds and relationship between these functions are shown. Lastly, we\nderive Mertens' First and Second Theorem, i.&thinsp;e.\n&sum;<sub><em>p</em>&le;<em>x</em></sub>\nln <em>p</em>/<em>p</em> = ln\n<em>x</em> + <em>O</em>(1) and\n&sum;<sub><em>p</em>&le;<em>x</em></sub>\n1/<em>p</em> = ln ln <em>x</em> + M +\n<em>O</em>(1/ln <em>x</em>). We also give\nexplicit bounds for the remainder terms.</p> <p>The proof\nof the Prime Number Theorem builds on a library of Dirichlet series\nand analytic combinatorics. We essentially follow the presentation by\nNewman. The core part of the proof is a Tauberian theorem for\nDirichlet series, which is proven using complex analysis and then used\nto strengthen Mertens' First Theorem to\n&sum;<sub><em>p</em>&le;<em>x</em></sub>\nln <em>p</em>/<em>p</em> = ln\n<em>x</em> + c + <em>o</em>(1).</p>\n<p>A variant of this proof has been formalised before by\nHarrison in HOL Light, and formalisations of Selberg's elementary\nproof exist both by Avigad <em>et al.</em> in Isabelle and\nby Carneiro in Metamath. The advantage of the analytic proof is that,\nwhile it requires more powerful mathematical tools, it is considerably\nshorter and clearer. This article attempts to provide a short and\nclear formalisation of all components of that proof using the full\nrange of mathematical machinery available in Isabelle, staying as\nclose as possible to Newman's simple paper proof.</p>",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-09-20"
            }
        ],
        "dependencies": [
            "Zeta_Function",
            "Stirling_Formula"
        ],
        "theories": [
            "Prime_Number_Theorem_Library",
            "Newman_Ingham_Tauberian",
            "Prime_Counting_Functions",
            "Prime_Number_Theorem",
            "Mertens_Theorems"
        ]
    },
    {
        "session": "Functional_Ordered_Resolution_Prover",
        "title": "A Verified Functional Implementation of Bachmair and Ganzinger's Ordered Resolution Prover",
        "authors": [
            "Anders Schlichtkrull",
            "Jasmin Christian Blanchette",
            "Dmitriy Traytel"
        ],
        "topics": [
            "Logic/General logic/Mechanization of proofs"
        ],
        "date": "2018-11-23",
        "abstract": "\nThis Isabelle/HOL formalization refines the abstract ordered\nresolution prover  presented in Section 4.3 of Bachmair and\nGanzinger's \"Resolution Theorem Proving\" chapter in the\n<i>Handbook of Automated Reasoning</i>. The result is a\nfunctional implementation of a first-order prover.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-11-29"
            }
        ],
        "dependencies": [
            "Ordered_Resolution_Prover",
            "First_Order_Terms",
            "Knuth_Bendix_Order",
            "Lambda_Free_RPOs",
            "Nested_Multisets_Ordinals",
            "Open_Induction",
            "Polynomial_Factorization"
        ],
        "theories": [
            "Weighted_FO_Ordered_Resolution_Prover",
            "Deterministic_FO_Ordered_Resolution_Prover",
            "IsaFoR_Term",
            "Executable_Subsumption",
            "Executable_FO_Ordered_Resolution_Prover"
        ]
    },
    {
        "session": "GraphMarkingIBP",
        "title": "Verification of the Deutsch-Schorr-Waite Graph Marking Algorithm using Data Refinement",
        "authors": [
            "Viorel Preoteasa",
            "Ralph-Johan Back"
        ],
        "date": "2010-05-28",
        "topics": [
            "Computer science/Algorithms/Graph"
        ],
        "abstract": "The verification of the Deutsch-Schorr-Waite graph marking algorithm is used as a benchmark in many formalizations of pointer programs. The main purpose of this mechanization is to show how data refinement of invariant based programs can be used in verifying practical algorithms. The verification starts with an abstract algorithm working on a graph given by a relation <i>next</i> on nodes. Gradually the abstract program is refined into Deutsch-Schorr-Waite graph marking algorithm where only one bit per graph node of additional memory is used for marking.",
        "extra": {
            "Change history": "[2012-01-05] Updated for the new definition of data refinement and the new syntax for demonic and angelic update statements"
        },
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-24"
            },
            {
                "2011-1": "2012-03-15"
            },
            {
                "2011-1": "2011-10-11"
            },
            {
                "2011": "2011-02-11"
            },
            {
                "2009-2": "2010-07-01"
            },
            {
                "2009-1": "2010-05-28"
            }
        ],
        "dependencies": [
            "DataRefinementIBP"
        ],
        "theories": [
            "Graph",
            "SetMark",
            "StackMark",
            "LinkMark",
            "DSWMark"
        ]
    },
    {
        "session": "Cartan_FP",
        "title": "The Cartan Fixed Point Theorems",
        "authors": [
            "Lawrence C. Paulson"
        ],
        "date": "2016-03-08",
        "topics": [
            "Mathematics/Analysis"
        ],
        "abstract": "\nThe Cartan fixed point theorems concern the group of holomorphic\nautomorphisms on a connected open set of C<sup>n</sup>. Ciolli et al.\nhave formalised the one-dimensional case of these theorems in HOL\nLight. This entry contains their proofs, ported to Isabelle/HOL.  Thus\nit addresses the authors' remark that \"it would be important to write\na formal proof in a language that can be read by both humans and\nmachines\".",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-03-09"
            }
        ],
        "theories": [
            "Cartan"
        ]
    },
    {
        "session": "Matrix_Tensor",
        "title": "Tensor Product of Matrices",
        "topics": [
            "Computer science/Data structures",
            "Mathematics/Algebra"
        ],
        "date": "2016-01-18",
        "authors": [
            "T.V.H. Prathamesh"
        ],
        "abstract": "\nIn this work, the Kronecker tensor product of matrices and the proofs of\nsome of its properties are formalized. Properties which have been formalized\ninclude associativity of the tensor product and the mixed-product\nproperty.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2016-01-19"
            },
            {
                "2015": "2016-01-18"
            }
        ],
        "dependencies": [
            "Matrix"
        ],
        "theories": [
            "Matrix_Tensor"
        ]
    },
    {
        "session": "Goedel_HFSet_Semanticless",
        "title": "From Abstract to Concrete G&ouml;del's Incompleteness Theorems&mdash;Part II",
        "authors": [
            "Andrei Popescu",
            "Dmitriy Traytel"
        ],
        "topics": [
            "Logic/Proof theory"
        ],
        "date": "2020-09-16",
        "abstract": "\nWe validate an abstract formulation of G&ouml;del's Second\nIncompleteness Theorem from a <a\nhref=\"https://www.isa-afp.org/entries/Goedel_Incompleteness.html\">separate\nAFP entry</a> by instantiating it to the case of <i>finite\nconsistent extensions of the Hereditarily Finite (HF) Set\ntheory</i>, i.e., consistent FOL theories extending the HF Set\ntheory with a finite set of axioms.  The instantiation draws heavily\non infrastructure previously developed by Larry Paulson in his <a\nhref=\"https://www.isa-afp.org/entries/Incompleteness.html\">direct\nformalisation of the concrete result</a>. It strengthens\nPaulson's formalization of G&ouml;del's Second from that\nentry by <i>not</i> assuming soundness, and in fact not\nrelying on any notion of model or semantic interpretation. The\nstrengthening was obtained by first replacing some of Paulson’s\nsemantic arguments with proofs within his HF calculus, and then\nplugging in some of Paulson's (modified) lemmas to instantiate\nour soundness-free G&ouml;del's Second locale.",
        "licence": "BSD",
        "dependencies": [
            "Goedel_Incompleteness",
            "HereditarilyFinite",
            "Nominal2"
        ],
        "theories": [
            "SyntaxN",
            "Coding",
            "Predicates",
            "Sigma",
            "Coding_Predicates",
            "Pf_Predicates",
            "II_Prelims",
            "Pseudo_Coding",
            "Quote",
            "Functions",
            "Goedel_I",
            "Instance"
        ]
    },
    {
        "session": "ComponentDependencies",
        "title": "Formalisation and Analysis of Component Dependencies",
        "authors": [
            "Maria Spichkova"
        ],
        "date": "2014-04-28",
        "topics": [
            "Computer science/System description languages"
        ],
        "abstract": "This set of theories presents a formalisation in Isabelle/HOL of data dependencies between components. The approach allows to analyse system structure oriented towards efficient checking of system: it aims at elaborating for a concrete system, which parts of the system are necessary to check a given property.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2014-04-29"
            }
        ],
        "theories": [
            "DataDependenciesConcreteValues",
            "DataDependencies",
            "DataDependenciesCaseStudy"
        ]
    },
    {
        "session": "Complex_Geometry",
        "title": "Complex Geometry",
        "authors": [
            "Filip Marić",
            "Danijela Simić"
        ],
        "topics": [
            "Mathematics/Geometry"
        ],
        "date": "2019-12-16",
        "abstract": "\nA formalization of geometry of complex numbers is presented.\nFundamental objects that are investigated are the complex plane\nextended by a single infinite point, its objects (points, lines and\ncircles), and groups of transformations that act on them (e.g.,\ninversions and Möbius transformations). Most objects are defined\nalgebraically, but correspondence with classical geometric definitions\nis shown.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2020-01-17"
            }
        ],
        "theories": [
            "More_Transcendental",
            "Canonical_Angle",
            "More_Complex",
            "Angles",
            "More_Set",
            "Linear_Systems",
            "Quadratic",
            "Matrices",
            "Unitary_Matrices",
            "Unitary11_Matrices",
            "Hermitean_Matrices",
            "Elementary_Complex_Geometry",
            "Homogeneous_Coordinates",
            "Moebius",
            "Circlines",
            "Oriented_Circlines",
            "Circlines_Angle",
            "Unit_Circle_Preserving_Moebius",
            "Riemann_Sphere",
            "Chordal_Metric"
        ]
    },
    {
        "session": "POPLmark-deBruijn",
        "title": "POPLmark Challenge Via de Bruijn Indices",
        "authors": [
            "Stefan Berghofer"
        ],
        "date": "2007-08-02",
        "topics": [
            "Computer science/Programming languages/Lambda calculi"
        ],
        "abstract": "We present a solution to the POPLmark challenge designed by Aydemir et al., which has as a goal the formalization of the meta-theory of System F<sub>&lt;:</sub>. The formalization is carried out in the theorem prover Isabelle/HOL using an encoding based on de Bruijn indices. We start with a relatively simple formalization covering only the basic features of System F<sub>&lt;:</sub>, and explain how it can be extended to also cover records and more advanced binding constructs.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-03-02"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-24"
            },
            {
                "2011-1": "2011-10-11"
            },
            {
                "2011": "2011-02-11"
            },
            {
                "2009-2": "2010-07-01"
            },
            {
                "2009-1": "2009-12-12"
            },
            {
                "2009": "2009-04-29"
            },
            {
                "2008": "2008-06-10"
            },
            {
                "2007": "2007-11-27"
            }
        ],
        "theories": [
            "Basis",
            "POPLmark",
            "POPLmarkRecord",
            "POPLmarkRecordCtxt",
            "Execute"
        ]
    },
    {
        "session": "List_Interleaving",
        "title": "Reasoning about Lists via List Interleaving",
        "authors": [
            "Pasquale Noce"
        ],
        "date": "2015-06-11",
        "topics": [
            "Computer science/Data structures"
        ],
        "abstract": "\n<p>\nAmong the various mathematical tools introduced in his outstanding work on\nCommunicating Sequential Processes, Hoare has defined \"interleaves\" as the\npredicate satisfied by any three lists such that the first list may be\nsplit into sublists alternately extracted from the other two ones, whatever\nis the criterion for extracting an item from either one list or the other\nin each step.\n</p><p>\nThis paper enriches Hoare's definition by identifying such criterion with\nthe truth value of a predicate taking as inputs the head and the tail of\nthe first list. This enhanced \"interleaves\" predicate turns out to permit\nthe proof of equalities between lists without the need of an induction.\nSome rules that allow to infer \"interleaves\" statements without induction,\nparticularly applying to the addition or removal of a prefix to the input\nlists, are also proven. Finally, a stronger version of the predicate, named\n\"Interleaves\", is shown to fulfil further rules applying to the addition or\nremoval of a suffix to the input lists.\n</p>",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-06-13"
            }
        ],
        "theories": [
            "ListInterleaving"
        ]
    },
    {
        "session": "SPARCv8",
        "title": "A formal model for the SPARCv8 ISA and a proof of non-interference for the LEON3 processor",
        "authors": [
            "Zhe Hou",
            "David Sanan",
            "Alwen Tiu",
            "Yang Liu"
        ],
        "date": "2016-10-19",
        "topics": [
            "Computer science/Security",
            "Computer science/Hardware"
        ],
        "abstract": "\nWe formalise the SPARCv8 instruction set architecture (ISA) which is\nused in processors such as LEON3. Our formalisation can be specialised\nto any SPARCv8 CPU, here we use LEON3 as a running example. Our model\ncovers the operational semantics for all the instructions in the\ninteger unit of the SPARCv8 architecture and it supports Isabelle code\nexport, which effectively turns the Isabelle model into a SPARCv8 CPU\nsimulator. We prove the language-based non-interference property for\nthe LEON3 processor.  Our model is based on deterministic monad, which\nis a modified version of the non-deterministic monad from NICTA/l4v.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-10-19"
            }
        ],
        "dependencies": [
            "Word_Lib"
        ],
        "theories": [
            "WordDecl",
            "Sparc_Types",
            "Lib",
            "DetMonad",
            "DetMonadLemmas",
            "RegistersOps",
            "MMU",
            "Sparc_State",
            "Sparc_Instruction",
            "Sparc_Execution",
            "Sparc_Properties",
            "Sparc_Init_State",
            "Sparc_Code_Gen"
        ]
    },
    {
        "session": "Transformer_Semantics",
        "title": "Transformer Semantics",
        "authors": [
            "Georg Struth"
        ],
        "topics": [
            "Mathematics/Algebra",
            "Computer science/Semantics"
        ],
        "date": "2018-12-11",
        "abstract": "\nThese mathematical components formalise predicate transformer\nsemantics for programs, yet currently only for partial correctness and\nin the absence of faults.  A first part for isotone (or monotone),\nSup-preserving and Inf-preserving transformers follows Back and von\nWright's approach, with additional emphasis on the quantalic\nstructure of algebras of transformers.  The second part develops\nSup-preserving and Inf-preserving predicate transformers from the\npowerset monad, via its Kleisli category and Eilenberg-Moore algebras,\nwith emphasis on adjunctions and dualities, as well as isomorphisms\nbetween relations, state transformers and predicate transformers.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-12-19"
            }
        ],
        "dependencies": [
            "Quantales",
            "Order_Lattice_Props"
        ],
        "theories": [
            "Isotone_Transformers",
            "Sup_Inf_Preserving_Transformers",
            "Powerset_Monad",
            "Kleisli_Transformers",
            "Kleisli_Quantaloid",
            "Kleisli_Quantale"
        ]
    },
    {
        "session": "CryptHOL",
        "title": "CryptHOL",
        "authors": [
            "Andreas Lochbihler"
        ],
        "topics": [
            "Computer science/Security/Cryptography",
            "Computer science/Functional programming",
            "Mathematics/Probability theory"
        ],
        "date": "2017-05-05",
        "abstract": "\n<p>CryptHOL provides a framework for formalising cryptographic arguments\nin Isabelle/HOL. It shallowly embeds a probabilistic functional\nprogramming language in higher order logic. The language features\nmonadic sequencing, recursion, random sampling, failures and failure\nhandling, and black-box access to oracles. Oracles are probabilistic\nfunctions which maintain hidden state between different invocations.\nAll operators are defined in the new semantic domain of\ngenerative probabilistic values, a codatatype. We derive proof rules for\nthe operators and establish a connection with the theory of relational\nparametricity. Thus, the resuting proofs are trustworthy and\ncomprehensible, and the framework is extensible and widely applicable.\n</p><p>\nThe framework is used in the accompanying AFP entry \"Game-based\nCryptography in HOL\". There, we show-case our framework by formalizing\ndifferent game-based proofs from the literature. This formalisation\ncontinues the work described in the author's ESOP 2016 paper.</p>",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2017-05-11"
            }
        ],
        "dependencies": [
            "Probabilistic_While",
            "Applicative_Lifting",
            "Coinductive",
            "Landau_Symbols",
            "Monad_Normalisation",
            "Monomorphic_Monad"
        ],
        "theories": [
            "Misc_CryptHOL",
            "Set_Applicative",
            "SPMF_Applicative",
            "List_Bits",
            "Environment_Functor",
            "Partial_Function_Set",
            "Negligible",
            "Resumption",
            "Generat",
            "Generative_Probabilistic_Value",
            "Computational_Model",
            "GPV_Expectation",
            "GPV_Bisim",
            "GPV_Applicative",
            "Cyclic_Group",
            "Cyclic_Group_SPMF",
            "CryptHOL"
        ]
    },
    {
        "session": "SIFPL",
        "title": "Secure information flow and program logics",
        "authors": [
            "Lennart Beringer",
            "Martin Hofmann"
        ],
        "date": "2008-11-10",
        "topics": [
            "Computer science/Programming languages/Logics",
            "Computer science/Security"
        ],
        "abstract": "We present interpretations of type systems for secure information flow in Hoare logic, complementing previous encodings in relational program logics. We first treat the imperative language IMP, extended by a simple procedure call mechanism. For this language we consider base-line non-interference in the style of Volpano et al. and the flow-sensitive type system by Hunt and Sands. In both cases, we show how typing derivations may be used to automatically generate proofs in the program logic that certify the absence of illicit flows. We then add instructions for object creation and manipulation, and derive appropriate proof rules for base-line non-interference. As a consequence of our work, standard verification technology may be used for verifying that a concrete program satisfies the non-interference property.<br><br>The present proof development represents an update of the formalisation underlying our paper [CSF 2007] and is intended to resolve any ambiguities that may be present in the paper.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2013-12-11"
            },
            {
                "2013-1": "2013-11-17"
            },
            {
                "2013": "2013-02-16"
            },
            {
                "2012": "2012-05-24"
            },
            {
                "2011-1": "2011-10-11"
            },
            {
                "2011": "2011-02-11"
            },
            {
                "2009-2": "2010-07-01"
            },
            {
                "2009-1": "2009-12-12"
            },
            {
                "2009": "2009-04-29"
            },
            {
                "2008": "2008-11-13"
            }
        ],
        "theories": [
            "IMP",
            "VDM",
            "VS",
            "ContextVS",
            "Lattice",
            "HuntSands",
            "OBJ",
            "VDM_OBJ",
            "PBIJ",
            "VS_OBJ",
            "ContextOBJ"
        ]
    },
    {
        "session": "pGCL",
        "title": "pGCL for Isabelle",
        "authors": [
            "David Cock"
        ],
        "date": "2014-07-13",
        "topics": [
            "Computer science/Programming languages/Language definitions"
        ],
        "abstract": "\n<p>pGCL is both a programming language and a specification language that\nincorporates both probabilistic and nondeterministic choice, in a unified\nmanner. Program verification is by refinement or annotation (or both), using\neither Hoare triples, or weakest-precondition entailment, in the style of\nGCL.</p>\n<p> This package provides both a shallow embedding of the language\nprimitives, and an annotation and refinement framework. The generated\ndocument includes a brief tutorial.</p>",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-05-27"
            },
            {
                "2014": "2014-08-28"
            },
            {
                "2013-2": "2014-07-13"
            }
        ],
        "theories": [
            "Misc",
            "Expectations",
            "Transformers",
            "Induction",
            "Embedding",
            "Healthiness",
            "Continuity",
            "LoopInduction",
            "Sublinearity",
            "WellDefined",
            "Algebra",
            "StructuredReasoning",
            "Automation",
            "files/pVCG.ML",
            "Determinism",
            "Loops",
            "Termination",
            "pGCL",
            "Primitives",
            "LoopExamples",
            "Monty"
        ]
    },
    {
        "session": "Formal_SSA",
        "title": "Verified Construction of Static Single Assignment Form",
        "authors": [
            "Sebastian Ullrich",
            "Denis Lohner"
        ],
        "date": "2016-02-05",
        "topics": [
            "Computer science/Algorithms",
            "Computer science/Programming languages/Transformations"
        ],
        "abstract": "\n<p>\nWe define a functional variant of the static single assignment (SSA)\nform construction algorithm described by <a\nhref=\"https://doi.org/10.1007/978-3-642-37051-9_6\">Braun et al.</a>,\nwhich combines simplicity and efficiency. The definition is based on a\ngeneral, abstract control flow graph representation using Isabelle locales.\n</p>\n<p>\nWe prove that the algorithm's output is semantically equivalent to the\ninput according to a small-step semantics, and that it is in minimal SSA\nform for the common special case of reducible inputs. We then show the\nsatisfiability of the locale assumptions by giving instantiations for a\nsimple While language.\n</p>\n<p>\nFurthermore, we use a generic instantiation based on typedefs in order\nto extract OCaml code and replace the unverified SSA construction\nalgorithm of the <a href=\"https://doi.org/10.1145/2579080\">CompCertSSA\nproject</a> with it.\n</p>\n<p>\nA more detailed description of the verified SSA construction can be found\nin the paper <a href=\"https://doi.org/10.1145/2892208.2892211\">Verified\nConstruction of Static Single Assignment Form</a>, CC 2016.\n</p>",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2016-02-08"
            }
        ],
        "dependencies": [
            "Collections",
            "Dijkstra_Shortest_Path",
            "Slicing",
            "CAVA_Automata"
        ],
        "theories": [
            "FormalSSA_Misc",
            "Serial_Rel",
            "Mapping_Exts",
            "RBT_Mapping_Exts",
            "Graph_path",
            "SSA_CFG",
            "Minimality",
            "Construct_SSA",
            "Construct_SSA_notriv",
            "SSA_Semantics",
            "While_Combinator_Exts",
            "SSA_CFG_code",
            "Construct_SSA_code",
            "SSA_Transfer_Rules",
            "Construct_SSA_notriv_code",
            "Generic_Interpretation",
            "Generic_Extract",
            "Disjoin_Transform",
            "WhileGraphSSA"
        ]
    },
    {
        "session": "Rewriting_Z",
        "title": "The Z Property",
        "authors": [
            "Bertram Felgenhauer",
            "Julian Nagele",
            "Vincent van Oostrom",
            "Christian Sternagel"
        ],
        "date": "2016-06-30",
        "topics": [
            "Logic/Rewriting"
        ],
        "abstract": "\nWe formalize the Z property introduced by Dehornoy and van Oostrom.\nFirst we show that for any abstract rewrite system, Z implies\nconfluence. Then we give two examples of proofs using Z: confluence of\nlambda-calculus with respect to beta-reduction and confluence of\ncombinatory logic.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-06-30"
            }
        ],
        "dependencies": [
            "Nominal2",
            "Abstract-Rewriting"
        ],
        "theories": [
            "Z",
            "Lambda_Z",
            "CL_Z"
        ]
    },
    {
        "session": "Irrational_Series_Erdos_Straus",
        "title": "Irrationality Criteria for Series by Erdős and Straus",
        "authors": [
            "Angeliki Koutsoukou-Argyraki",
            "Wenda Li"
        ],
        "topics": [
            "Mathematics/Number theory",
            "Mathematics/Analysis"
        ],
        "date": "2020-05-12",
        "abstract": "\nWe formalise certain irrationality criteria for infinite series of the form:\n\\[\\sum_{n=1}^\\infty \\frac{b_n}{\\prod_{i=1}^n a_i} \\]\nwhere $\\{b_n\\}$ is a sequence of integers and $\\{a_n\\}$ a sequence of positive integers\nwith $a_n >1$ for all large n. The results are due to P. Erdős and E. G. Straus\n<a href=\"https://projecteuclid.org/euclid.pjm/1102911140\">[1]</a>.\nIn particular, we formalise Theorem 2.1, Corollary 2.10 and Theorem 3.1.\nThe latter is an application of Theorem 2.1 involving the prime numbers.",
        "licence": "BSD",
        "dependencies": [
            "Prime_Number_Theorem",
            "Prime_Distribution_Elementary"
        ],
        "theories": [
            "Irrational_Series_Erdos_Straus"
        ]
    },
    {
        "session": "Nash_Williams",
        "title": "The Nash-Williams Partition Theorem",
        "authors": [
            "Lawrence C. Paulson"
        ],
        "topics": [
            "Mathematics/Combinatorics"
        ],
        "date": "2020-05-16",
        "abstract": "\nIn 1965, Nash-Williams discovered a generalisation of the infinite\nform of Ramsey's theorem. Where the latter concerns infinite sets\nof n-element sets for some fixed n, the Nash-Williams theorem concerns\ninfinite sets of finite sets (or lists) subject to a “no initial\nsegment” condition. The present formalisation follows a\nmonograph on Ramsey Spaces by Todorčević.",
        "licence": "BSD",
        "theories": [
            "Nash_Extras",
            "Nash_Williams"
        ]
    },
    {
        "session": "Constructor_Funs",
        "title": "Constructor Functions",
        "authors": [
            "Lars Hupel"
        ],
        "topics": [
            "Tools"
        ],
        "date": "2017-04-19",
        "abstract": "\nIsabelle's code generator performs various adaptations for target\nlanguages. Among others, constructor applications have to be fully\nsaturated. That means that for constructor calls occuring as arguments\nto higher-order functions, synthetic lambdas have to be inserted. This\nentry provides tooling to avoid this construction altogether by\nintroducing constructor functions.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2017-04-20"
            }
        ],
        "theories": [
            "Constructor_Funs",
            "files/constructor_funs.ML",
            "Test_Constructor_Funs"
        ]
    },
    {
        "session": "Root_Balanced_Tree",
        "title": "Root-Balanced Tree",
        "authors": [
            "Tobias Nipkow"
        ],
        "date": "2017-08-20",
        "topics": [
            "Computer science/Data structures"
        ],
        "abstract": "\n<p>\nAndersson introduced <em>general balanced trees</em>,\nsearch trees based on the design principle of partial rebuilding:\nperform update operations naively until the tree becomes too\nunbalanced, at which point a whole subtree is rebalanced.  This article\ndefines and analyzes a functional version of general balanced trees,\nwhich we call <em>root-balanced trees</em>. Using a lightweight model\nof execution time, amortized logarithmic complexity is verified in\nthe theorem prover Isabelle.\n</p>\n<p>\nThis is the Isabelle formalization of the material decribed in the APLAS 2017 article\n<a href=\"http://www21.in.tum.de/~nipkow/pubs/aplas17.html\">Verified Root-Balanced Trees</a>\nby the same author, which also presents experimental results that show\ncompetitiveness of root-balanced with AVL and red-black trees.\n</p>",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2017-08-20"
            }
        ],
        "dependencies": [
            "Amortized_Complexity"
        ],
        "theories": [
            "Time_Monad",
            "Root_Balanced_Tree",
            "Root_Balanced_Tree_Tab"
        ]
    },
    {
        "session": "Saturation_Framework",
        "title": "A Comprehensive Framework for Saturation Theorem Proving",
        "authors": [
            "Sophie Tourret"
        ],
        "topics": [
            "Logic/General logic/Mechanization of proofs"
        ],
        "date": "2020-04-09",
        "abstract": "\nThis Isabelle/HOL formalization is the companion of the technical\nreport “A comprehensive framework for saturation theorem proving”,\nitself companion of the eponym IJCAR 2020 paper, written by Uwe\nWaldmann, Sophie Tourret, Simon Robillard and Jasmin Blanchette. It\nverifies a framework for formal refutational completeness proofs of\nabstract provers that implement saturation calculi, such as ordered\nresolution or superposition, and allows to model entire prover\narchitectures in such a way that the static refutational completeness\nof a calculus immediately implies the dynamic  refutational\ncompleteness of a prover implementing the calculus using a variant of\nthe given clause loop.  The technical report “A comprehensive\nframework for saturation theorem proving” is available <a\nhref=\"http://matryoshka.gforge.inria.fr/pubs/satur_report.pdf\">on\nthe Matryoshka website</a>. The names of the Isabelle lemmas and\ntheorems corresponding to the results in the report are indicated in\nthe margin of the report.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2020-04-10"
            }
        ],
        "dependencies": [
            "Ordered_Resolution_Prover",
            "Lambda_Free_RPOs",
            "Well_Quasi_Orders"
        ],
        "theories": [
            "Calculus",
            "Intersection_Calculus",
            "Calculus_Variations",
            "Lifting_to_Non_Ground_Calculi",
            "Labeled_Lifting_to_Non_Ground_Calculi",
            "Given_Clause_Architectures"
        ]
    },
    {
        "session": "Case_Labeling",
        "title": "Generating Cases from Labeled Subgoals",
        "authors": [
            "Lars Noschinski"
        ],
        "date": "2015-07-21",
        "topics": [
            "Tools",
            "Computer science/Programming languages/Misc"
        ],
        "abstract": "\nIsabelle/Isar provides named cases to structure proofs. This article\ncontains an implementation of a proof method <tt>casify</tt>, which can\nbe used to easily extend proof tools with support for named cases. Such\na proof tool must produce labeled subgoals, which are then interpreted\nby <tt>casify</tt>.\n<p>\nAs examples, this work contains verification condition generators\nproducing named cases for three languages: The Hoare language from\n<tt>HOL/Library</tt>, a monadic language for computations with failure\n(inspired by the AutoCorres tool), and a language of conditional\nexpressions. These VCGs are demonstrated by a number of example programs.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-06-11"
            },
            {
                "2018": "2018-08-16"
            },
            {
                "2017": "2017-10-10"
            },
            {
                "2016-1": "2016-12-17"
            },
            {
                "2016": "2016-02-22"
            },
            {
                "2015": "2015-08-17"
            },
            {
                "2015": "2015-07-27"
            },
            {
                "2015": "2015-07-24"
            }
        ],
        "theories": [
            "Case_Labeling",
            "files/print_nested_cases.ML",
            "files/util.ML",
            "files/casify.ML",
            "Labeled_Hoare",
            "files/labeled_hoare_tac.ML",
            "Labeled_Hoare_Examples",
            "Conditionals",
            "Monadic_Language",
            "files/AFP/Case_Labeling/util.ML",
            "Case_Labeling_Examples"
        ]
    },
    {
        "session": "Sigma_Commit_Crypto",
        "title": "Sigma Protocols and Commitment Schemes",
        "authors": [
            "David Butler",
            "Andreas Lochbihler"
        ],
        "topics": [
            "Computer science/Security/Cryptography"
        ],
        "date": "2019-10-07",
        "abstract": "\nWe use CryptHOL to formalise commitment schemes and Sigma-protocols.\nBoth are widely used fundamental two party cryptographic primitives.\nSecurity for commitment schemes is considered using game-based\ndefinitions whereas the security of Sigma-protocols is considered\nusing both the game-based and simulation-based security paradigms. In\nthis work, we first define security for both primitives and then prove\nsecure multiple case studies: the Schnorr, Chaum-Pedersen and\nOkamoto Sigma-protocols as well as a construction that allows for\ncompound (AND and OR statements) Sigma-protocols and the Pedersen and\nRivest commitment schemes. We also prove that commitment schemes can\nbe constructed from Sigma-protocols. We formalise this proof at an\nabstract level, only assuming the existence of a Sigma-protocol;\nconsequently, the instantiations of this result for the concrete\nSigma-protocols we consider come for free.",
        "licence": "BSD",
        "olderReleases": [
            {
                "2019": "2019-10-08"
            }
        ],
        "dependencies": [
            "CryptHOL"
        ],
        "theories": [
            "Commitment_Schemes",
            "Cyclic_Group_Ext",
            "Discrete_Log",
            "Number_Theory_Aux",
            "Uniform_Sampling",
            "Pedersen",
            "Rivest",
            "Sigma_Protocols",
            "Schnorr_Sigma_Commit",
            "Chaum_Pedersen_Sigma_Commit",
            "Okamoto_Sigma_Commit",
            "Xor",
            "Sigma_AND",
            "Sigma_OR"
        ]
    }
]